{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdMyi2D4-6YC"
   },
   "source": [
    "# **Model 2 seq2seq for text Summurization using seq2seq lib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IzJv9UUti_og"
   },
   "source": [
    "### Intro\n",
    "This is a modification to https://github.com/dongjun-Lee/text-summarization-tensorflow \n",
    "I am builging it in a notebook envronment to be able to easily integrate with colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6lSefB0pn9Gq"
   },
   "source": [
    "## Helpers (Googel Drive , Utilits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pNpy7UgCx9m"
   },
   "source": [
    "### Utilits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0VWHFAnoDL4"
   },
   "source": [
    "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "id": "hf4b16KopBx7",
    "outputId": "357927ef-5859-40c2-b8a6-75a9e25b977c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (3.8.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from gensim) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from gensim) (1.17.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from gensim) (1.3.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied: boto>=2.32 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto3 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.10.26)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.26 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.26)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.26->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.26->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already satisfied: wget in /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages (3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/sbu_nlp2019/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sbu_nlp2019/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install wget\n",
    "  \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1tUe8ILummG"
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/38088652/pandas-convert-categories-to-numbers\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "#ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "\n",
    "def get_pos_tags_dict(words):\n",
    "    #sent = nltk.word_tokenize(sent)\n",
    "    #print(sent)\n",
    "    post_tags_for_words = nltk.pos_tag(words)\n",
    "\n",
    "    pos_list ={}\n",
    "    #sent = preprocess(ex)\n",
    "    for word,pos in post_tags_for_words:\n",
    "        pos_list[word] = pos\n",
    "    #print(pos_list)\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(list(pos_list.items()))\n",
    "    df.columns = ['word', 'pos']\n",
    "    df.pos = pd.Categorical(df.pos)\n",
    "    df['code'] = df.pos.cat.codes\n",
    "    #print(df)\n",
    "\n",
    "    pos_list ={}\n",
    "    for index, row in df.iterrows():\n",
    "        pos_list[row['word']] = row['code']\n",
    "#     print(pos_list)\n",
    "    return pos_list , post_tags_for_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lA1tCj2mn_2x"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# default_path = \"drive/My Drive/NLP/project/rnn-lstm/data/\"\n",
    "default_path = \"/home/sbu_nlp2019/AbstractiveTextSummarization/Model2/dataset/\"\n",
    "\n",
    "train_article_path = default_path + \"sumdata/train/train.article.txt\"\n",
    "train_title_path   = default_path + \"sumdata/train/train.title.txt\"\n",
    "valid_article_path = default_path + \"sumdata/train/valid.article.filter.txt\"\n",
    "valid_title_path   = default_path + \"sumdata/train/valid.title.filter.txt\"\n",
    "\n",
    "#valid_article_path = default_path + \"sumdata/DUC2003/input.txt\"\n",
    "#valid_title_path   = default_path + \"sumdata/DUC2003/task1_ref0.txt\"\n",
    "\n",
    "def clean_str(sentence):\n",
    "    sentence = re.sub(\"[#.]+\", \"#\", sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_text_list(data_path, toy):\n",
    "    with open (data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        if not toy:\n",
    "            return [clean_str(x.strip()) for x in f.readlines()][:200000]\n",
    "        else:\n",
    "            return [clean_str(x.strip()) for x in f.readlines()][:50]\n",
    "        \n",
    "def build_dict(step, toy=False):\n",
    "    if step == \"train\":\n",
    "        train_article_list = get_text_list(train_article_path, toy)\n",
    "        train_title_list = get_text_list(train_title_path, toy)\n",
    "\n",
    "        words = list()\n",
    "        for sentence in train_article_list + train_title_list:\n",
    "            for word in word_tokenize(sentence):\n",
    "                words.append(word)\n",
    "\n",
    "        word_counter = collections.Counter(words).most_common()\n",
    "        word_dict = dict()\n",
    "        word_dict[\"<padding>\"] = 0\n",
    "        word_dict[\"<unk>\"] = 1\n",
    "        word_dict[\"<s>\"] = 2\n",
    "        word_dict[\"</s>\"] = 3\n",
    "        for word, _ in word_counter:\n",
    "            word_dict[word] = len(word_dict)\n",
    "\n",
    "        with open(default_path + \"word_dict.pickle\", \"wb\") as f:\n",
    "            pickle.dump(word_dict, f)\n",
    "\n",
    "    elif step == \"valid\":\n",
    "        with open(default_path + \"word_dict.pickle\", \"rb\") as f:\n",
    "            word_dict = pickle.load(f)\n",
    "\n",
    "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    article_max_len = 50\n",
    "    summary_max_len = 15\n",
    "\n",
    "    return word_dict, reversed_dict, article_max_len, summary_max_len\n",
    "\n",
    "\n",
    "def build_dataset(step, word_dict, article_max_len, summary_max_len, toy=False):\n",
    "    if step == \"train\":\n",
    "        article_list = get_text_list(train_article_path, toy)\n",
    "        title_list = get_text_list(train_title_path, toy)\n",
    "    elif step == \"valid\":\n",
    "        article_list = get_text_list(valid_article_path, toy)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    x = [word_tokenize(d) for d in article_list]\n",
    "    x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in x]\n",
    "    x = [d[:article_max_len] for d in x]\n",
    "    x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
    "    \n",
    "    if step == \"valid\":\n",
    "        return x\n",
    "    else:        \n",
    "        y = [word_tokenize(d) for d in title_list]\n",
    "        y = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in y]\n",
    "        y = [d[:(summary_max_len - 1)] for d in y]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
    "\n",
    "\n",
    "def get_init_embedding(word_dict , reversed_dict, embedding_size):\n",
    "    print(\"Loading Lists...\")\n",
    "    train_article_list = get_text_list(train_article_path, False)\n",
    "    train_title_list = get_text_list(train_title_path, False)\n",
    "\n",
    "    print(\"Loading TF-IDF...\")\n",
    "    tf_idf_list = tf_idf_generate(train_article_list+train_title_list)\n",
    "    \n",
    "    print(\"Loading Pos Tags...\")\n",
    "    pos_list , postags_for_named_entity = get_pos_tags_dict(word_dict.keys())\n",
    "\n",
    "    #print(\"Loading Named Entity...\")\n",
    "    #named_entity_recs = named_entity(postags_for_named_entity) \n",
    "    \n",
    "    print(\"Loading Glove vectors...\")\n",
    "\n",
    "    with open( default_path + \"glove/model_glove_300.pkl\", 'rb') as handle:\n",
    "        word_vectors = pickle.load(handle)     \n",
    "    \n",
    "    used_words = 0\n",
    "    word_vec_list = list()\n",
    "    for _, word in sorted(reversed_dict.items()):\n",
    "        try:\n",
    "            word_vec = word_vectors.word_vec(word)\n",
    "            if word in tf_idf_list:\n",
    "                v= tf_idf_list[word]\n",
    "                rich_feature_array = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array)\n",
    "            else:\n",
    "                v=0\n",
    "                rich_feature_array = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array)\n",
    "\n",
    "            if word in pos_list:\n",
    "                v=pos_list[word]\n",
    "                rich_feature_array_2 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array_2)\n",
    "            else:\n",
    "                v=0\n",
    "                rich_feature_array_2 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "                word_vec = np.append(word_vec, rich_feature_array_2) \n",
    "\n",
    "            #if word in named_entity_recs:\n",
    "            #  v=named_entity_recs[word]\n",
    "            #  rich_feature_array_3 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "            #  word_vec = np.append(word_vec, rich_feature_array_3)\n",
    "            #else:\n",
    "            #  v=0\n",
    "            #  rich_feature_array_3 = np.array([v,v,v,v,v,v,v,v,v,v])\n",
    "            #  word_vec = np.append(word_vec, rich_feature_array_3)  \n",
    "          \n",
    "            used_words += 1\n",
    "        except KeyError:\n",
    "            word_vec = np.zeros([embedding_size], dtype=np.float32) #to generate for <padding> and <unk>\n",
    "        \n",
    "        \n",
    "        word_vec_list.append(np.array(word_vec))\n",
    "\n",
    "    print(\"words found in glove percentage = \" + str((used_words/len(word_vec_list))*100) )\n",
    "          \n",
    "    # Assign random vector to <s>, </s> token\n",
    "    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n",
    "    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n",
    "\n",
    "    return np.array(word_vec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZoqnXswL_FzR"
   },
   "outputs": [],
   "source": [
    "# _____TF-IDF libraries_____\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# _____helper Libraries_____\n",
    "import pickle  # would be used for saving temp files\n",
    "import csv     # used for accessing the dataset\n",
    "import timeit  # to measure time of training\n",
    "import random  # used to get a random number\n",
    "\n",
    "\n",
    "def tf_idf_generate(sentences):\n",
    "    #https://stackoverflow.com/questions/30976120/find-the-tf-idf-score-of-specific-words-in-documents-using-sklearn\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "    # our corpus\n",
    "    data = sentences\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    # convert text data into term-frequency matrix\n",
    "    data = cv.fit_transform(data)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "    # convert term-frequency matrix into tf-idf\n",
    "    tfidf_matrix = tfidf_transformer.fit_transform(data)\n",
    "\n",
    "    # create dictionary to find a tfidf word each word\n",
    "    word2tfidf = dict(zip(cv.get_feature_names(), tfidf_transformer.idf_))\n",
    "\n",
    "    #i = 0\n",
    "    #for word, score in word2tfidf.items():\n",
    "    #    print(word, score)\n",
    "    #    if (i == 10):\n",
    "    #      break\n",
    "    #    i+=1  \n",
    "  \n",
    "    return word2tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0pP9BSA-h10"
   },
   "outputs": [],
   "source": [
    "# https://nlpforhackers.io/named-entity-extraction/\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "\n",
    "# sentence = \"Mark and John are working at Google.\"\n",
    "\n",
    "\n",
    "# print (ne_chunk(pos_tag(word_dict.keys())[:5]))\n",
    "# names = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "# names = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "def named_entity(post_tags_for_words):\n",
    "    names = ne_chunk(post_tags_for_words)\n",
    "    names_dict = {}\n",
    "    for n in names:\n",
    "        if (len(n) == 1):\n",
    "            named_entity = str(n).split(' ')[0][1:]\n",
    "            word = str(n).split(' ')[1].split('/')[0]\n",
    "            names_dict[word] = named_entity\n",
    "    print(names_dict)\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.DataFrame(list(names_dict.items()))\n",
    "    df.columns = ['word', 'pos']\n",
    "    df.pos = pd.Categorical(df.pos)\n",
    "    df['code'] = df.pos.cat.codes\n",
    "    # print(df)\n",
    "\n",
    "    names_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        names_dict[row['word']] = row['code']\n",
    "    print(names_dict)\n",
    "    return names_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ShEw2241Ecsn",
    "outputId": "1899e04d-e7e4-4bda-bfa0-640552191d13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Loading training dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Building dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", False)\n",
    "print(\"Loading training dataset...\")\n",
    "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "YIbIcmz8N8Nc",
    "outputId": "a489e0eb-4e46-4bba-87ed-ed53e8eae5be"
   },
   "outputs": [],
   "source": [
    "pos_list , postags_for_named_entity = get_pos_tags_dict(word_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "vGAAv_HiOCGK",
    "outputId": "d84ec773-39ab-4503-9c23-1677b8744912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Lists...\n",
      "Loading TF-IDF...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.007402027976367"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading Lists...\")\n",
    "train_article_list = get_text_list(train_article_path, False)\n",
    "train_title_list = get_text_list(train_title_path, False)\n",
    "\n",
    "print(\"Loading TF-IDF...\")\n",
    "tf_idf_list = tf_idf_generate(train_article_list+train_title_list)\n",
    "tf_idf_list[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "B1UVvbwwOs08",
    "outputId": "4c49e99d-8c76-431a-8646-d3cbb25b881c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "4XW03jfHP2pS",
    "outputId": "0f9811d3-6a57-4055-88af-0bf6b3ff8595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iranian-american': 'GPE', 'israeli-jordanian': 'GPE', 'iraq-jordan': 'GPE', 'british-russian': 'GPE', 'saudi-jordanian': 'GPE', 'french-algerian': 'GPE', 'iraqi-syrian': 'GPE', 'greek-bulgarian': 'GPE', 'british-australian': 'GPE', 'iraqi-jordanian': 'GPE', 'russo-belgian': 'GPE', 'greek-macedonian': 'GPE'}\n",
      "{'iranian-american': 0, 'israeli-jordanian': 0, 'iraq-jordan': 0, 'british-russian': 0, 'saudi-jordanian': 0, 'french-algerian': 0, 'iraqi-syrian': 0, 'greek-bulgarian': 0, 'british-australian': 0, 'iraqi-jordanian': 0, 'russo-belgian': 0, 'greek-macedonian': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'iranian-american': 0,\n",
       " 'israeli-jordanian': 0,\n",
       " 'iraq-jordan': 0,\n",
       " 'british-russian': 0,\n",
       " 'saudi-jordanian': 0,\n",
       " 'french-algerian': 0,\n",
       " 'iraqi-syrian': 0,\n",
       " 'greek-bulgarian': 0,\n",
       " 'british-australian': 0,\n",
       " 'iraqi-jordanian': 0,\n",
       " 'russo-belgian': 0,\n",
       " 'greek-macedonian': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entity_recs = named_entity(postags_for_named_entity) \n",
    "named_entity_recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZxFTlhgDsfUF"
   },
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0dZvXIVXsiEt"
   },
   "source": [
    "### POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "c5YAAeXrsuwe",
    "outputId": "d98036d2-d668-4d7c-f0fe-0ff76eddb295"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sbu_nlp2019/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKBuNALo-idI"
   },
   "source": [
    "### Named Entity Reognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "svj4Nx7X-wSn",
    "outputId": "a4f0ed6d-20b9-4b71-b66d-a3962222d7ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/sbu_nlp2019/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /home/sbu_nlp2019/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qI3AJND0mb0r"
   },
   "source": [
    "## Prepare Data (unzip , discover operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1B9ZgJqmsni"
   },
   "source": [
    "\n",
    "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/prep_data.py\n",
    "\n",
    "1.   Word Embedding : Used [Glove pre-trained vectors](https://nlp.stanford.edu/projects/glove/ ) to initialize word embedding.  \n",
    "2.   Dataset :  Dataset is available at [harvardnlp/sent-summary](https://github.com/harvardnlp/sent-summary). Locate the summary.tar.gz file in project root directory.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BIf0duZCk3aw"
   },
   "source": [
    "#### unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2dpVeOqnJAh"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import os\n",
    "import tarfile\n",
    "import gzip\n",
    "import zipfile\n",
    "import argparse\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--glove\", action=\"store_true\")\n",
    "#args = parser.parse_args()\n",
    "\n",
    "# Extract data file\n",
    "#with tarfile.open(default_path + \"sumdata/train/summary.tar.gz\", \"r:gz\") as tar:\n",
    "#    tar.extractall()\n",
    "\n",
    "with gzip.open(default_path + \"sumdata/train/train.article.txt.gz\", \"rb\") as gz:\n",
    "    with open(default_path + \"sumdata/train/train.article.txt\", \"wb\") as out:\n",
    "        out.write(gz.read())\n",
    "\n",
    "with gzip.open(default_path + \"sumdata/train/train.title.txt.gz\", \"rb\") as gz:\n",
    "    with open(default_path + \"sumdata/train/train.title.txt\", \"wb\") as out:\n",
    "        out.write(gz.read())\n",
    "\n",
    "        \n",
    "#if args.glove:\n",
    "#    glove_dir = \"glove\"\n",
    "#    glove_url = \"https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\"\n",
    "#\n",
    "#    if not os.path.exists(glove_dir):\n",
    "#        os.mkdir(glove_dir)\n",
    "#\n",
    "#    # Download glove vector\n",
    "#    wget.download(glove_url, out=glove_dir)\n",
    "#\n",
    "#    # Extract glove file\n",
    "#    with zipfile.ZipFile(os.path.join(\"glove\", \"glove.42B.300d.zip\"), \"r\") as z:\n",
    "#        z.extractall(glove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "sIpdo8iI0-JN",
    "outputId": "a4a69110-3b4d-48be-c1c5-235bc4d0a426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Pre-Processing.ipynb  Model2\t\t       README.md\n",
      "Dataset\t\t\t   Model_Backup\t\t       Summarization.ipynb\n",
      "Embeddings\t\t   Model_Backup_AmazonReviews  Summarization.py\n",
      "LICENSE\t\t\t   Processed_Data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyFi7ETY7NwM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "glove_dir = \"glove\"\n",
    "glove_url = \"https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\"\n",
    "#\n",
    "if not os.path.exists(glove_dir):\n",
    "    os.mkdir(glove_dir)\n",
    "\n",
    "# Download glove vector\n",
    "wget.download(glove_url, out=glove_dir)\n",
    "\n",
    "# Extract glove file\n",
    "with zipfile.ZipFile(os.path.join(glove_dir, \"glove.42B.300d.zip\"), \"r\") as z:\n",
    "    z.extractall(glove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvKMw9lAGVif"
   },
   "source": [
    "#### Discover Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nOBeSOwZ_djh"
   },
   "outputs": [],
   "source": [
    "train_article_list = get_text_list(train_article_path, False)\n",
    "train_title_list = get_text_list(train_title_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XppjlyoXBZZz"
   },
   "outputs": [],
   "source": [
    "test = tf_idf_generate(train_article_list + train_title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "R1VhgbIHD0qJ",
    "outputId": "04328190-7b58-441c-9fab-b8f159603305"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.007402027976367"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "RJ-kQgWhFkWP",
    "outputId": "42ff340e-af3b-4dd4-8ce0-dd4da049a1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9076\n",
      "7242\n",
      "4205\n"
     ]
    }
   ],
   "source": [
    "print(word_dict[\"apple\"])\n",
    "print(word_dict[\"cat\"])\n",
    "print(word_dict[\"dog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8THQucjtF7BW",
    "outputId": "a157c9d4-fffa-40b4-875c-d0cee9b00f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\n",
      "diving\n",
      "stands\n"
     ]
    }
   ],
   "source": [
    "print(reversed_dict[9076])\n",
    "print(reversed_dict[7243])\n",
    "print(reversed_dict[4206])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "tg_QrwxVGKeo",
    "outputId": "9af02873-cd31-464c-acfa-b0274027cf98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_max_len : 50\n",
      "summary_max_len : 15\n"
     ]
    }
   ],
   "source": [
    "print(\"article_max_len : \" + str(article_max_len))\n",
    "print(\"summary_max_len : \" + str(summary_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "colab_type": "code",
    "id": "Rzg8fisCGZq2",
    "outputId": "65ad2ef0-26f7-4047-a80a-01c446d14d97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[175, 12, 957, 2385, 723, 16404, 27, 10, 254, 4, 147, 70, 48, 4, 147, 16, 49, 8, 5, 624, 369, 249, 7, 3484, 5682, 61, 6, 660, 481, 28, 491, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "australia\n",
      "'s\n",
      "current\n",
      "account\n",
      "deficit\n",
      "shrunk\n",
      "by\n",
      "a\n",
      "record\n",
      "#\n",
      "billion\n",
      "dollars\n",
      "-lrb-\n",
      "#\n",
      "billion\n",
      "us\n",
      "-rrb-\n",
      "in\n",
      "the\n",
      "june\n",
      "quarter\n",
      "due\n",
      "to\n",
      "soaring\n",
      "commodity\n",
      "prices\n",
      ",\n",
      "figures\n",
      "released\n",
      "monday\n",
      "showed\n",
      "#\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n",
      "<padding>\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "for num in train_x[0] :\n",
    "    print(reversed_dict[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "yUcUihAGG2ge",
    "outputId": "36798597-cbd2-48d3-9534-16738ff35f2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[156, 957, 2385, 723, 7485, 897]\n",
      "australian\n",
      "current\n",
      "account\n",
      "deficit\n",
      "narrows\n",
      "sharply\n"
     ]
    }
   ],
   "source": [
    "print(train_y[0])\n",
    "for num in train_y[0] :\n",
    "  print(reversed_dict[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "n-fGxQPSJ242",
    "outputId": "6be3f927-8f7f-439e-e914-9d893a4eec9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Lists...\n",
      "Loading TF-IDF...\n",
      "Loading Pos Tags...\n",
      "Loading Glove vectors...\n",
      "words found in glove percentage = 91.75771029889796\n"
     ]
    }
   ],
   "source": [
    "test_embedding = get_init_embedding(word_dict , reversed_dict, 320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r4k6ornzH5XV",
    "outputId": "781dfa13-788b-4f36-b12b-dc13be93bb5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(reversed_dict[2000])\n",
    "len(test_embedding[30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Nrpqz0cNzDXT",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<padding>': 12,\n",
       "  '<unk>': 17,\n",
       "  '<s>': 17,\n",
       "  '</s>': 17,\n",
       "  '#': 0,\n",
       "  'the': 8,\n",
       "  ',': 3,\n",
       "  'to': 27,\n",
       "  'in': 11,\n",
       "  'of': 11,\n",
       "  'a': 8,\n",
       "  'on': 11,\n",
       "  \"'s\": 20,\n",
       "  'and': 6,\n",
       "  'said': 29,\n",
       "  'for': 11,\n",
       "  'us': 21,\n",
       "  'with': 11,\n",
       "  'as': 23,\n",
       "  'at': 11,\n",
       "  'tuesday': 16,\n",
       "  'thursday': 11,\n",
       "  'an': 8,\n",
       "  'wednesday': 16,\n",
       "  'after': 11,\n",
       "  '``': 38,\n",
       "  'from': 11,\n",
       "  'by': 11,\n",
       "  'monday': 16,\n",
       "  'friday': 12,\n",
       "  '<': 17,\n",
       "  'unk': 16,\n",
       "  '>': 16,\n",
       "  'that': 34,\n",
       "  'percent': 16,\n",
       "  'was': 29,\n",
       "  'president': 16,\n",
       "  'new': 12,\n",
       "  'his': 22,\n",
       "  'over': 16,\n",
       "  'has': 33,\n",
       "  'its': 22,\n",
       "  'here': 23,\n",
       "  'minister': 16,\n",
       "  'it': 21,\n",
       "  'against': 11,\n",
       "  'two': 7,\n",
       "  'government': 16,\n",
       "  '-lrb-': 17,\n",
       "  '-rrb-': 17,\n",
       "  'will': 15,\n",
       "  'sunday': 28,\n",
       "  'were': 29,\n",
       "  'up': 23,\n",
       "  'world': 16,\n",
       "  'is': 33,\n",
       "  'police': 12,\n",
       "  'saturday': 23,\n",
       "  'first': 23,\n",
       "  'iraq': 28,\n",
       "  'their': 22,\n",
       "  'prices': 19,\n",
       "  'china': 32,\n",
       "  'be': 28,\n",
       "  'officials': 19,\n",
       "  'south': 23,\n",
       "  'have': 32,\n",
       "  'he': 21,\n",
       "  'killed': 31,\n",
       "  'un': 12,\n",
       "  'dollars': 19,\n",
       "  'year': 16,\n",
       "  '-': 5,\n",
       "  'united': 12,\n",
       "  'people': 19,\n",
       "  'state': 16,\n",
       "  'million': 7,\n",
       "  'shares': 19,\n",
       "  'talks': 19,\n",
       "  'french': 12,\n",
       "  'official': 12,\n",
       "  'into': 11,\n",
       "  'war': 16,\n",
       "  'country': 16,\n",
       "  'out': 11,\n",
       "  'foreign': 12,\n",
       "  'more': 13,\n",
       "  'european': 12,\n",
       "  'reported': 29,\n",
       "  'british': 12,\n",
       "  'prime': 12,\n",
       "  'three': 7,\n",
       "  'bank': 16,\n",
       "  'who': 35,\n",
       "  'group': 16,\n",
       "  'leader': 16,\n",
       "  'former': 12,\n",
       "  'which': 34,\n",
       "  'military': 12,\n",
       "  'week': 16,\n",
       "  'had': 29,\n",
       "  'one': 7,\n",
       "  'but': 6,\n",
       "  'security': 16,\n",
       "  'states': 19,\n",
       "  'share': 16,\n",
       "  'court': 16,\n",
       "  'close': 12,\n",
       "  'dealers': 19,\n",
       "  'oil': 16,\n",
       "  'international': 12,\n",
       "  'israeli': 16,\n",
       "  'higher': 24,\n",
       "  'this': 8,\n",
       "  'down': 26,\n",
       "  'would': 15,\n",
       "  'been': 31,\n",
       "  'not': 23,\n",
       "  'next': 12,\n",
       "  'trade': 16,\n",
       "  'hong': 16,\n",
       "  'kong': 28,\n",
       "  'lower': 13,\n",
       "  'says': 33,\n",
       "  'russian': 12,\n",
       "  'dollar': 16,\n",
       "  'announced': 29,\n",
       "  'india': 12,\n",
       "  'day': 16,\n",
       "  'palestinian': 12,\n",
       "  'last': 12,\n",
       "  'closed': 29,\n",
       "  'they': 21,\n",
       "  'stocks': 19,\n",
       "  'german': 29,\n",
       "  'peace': 16,\n",
       "  'than': 11,\n",
       "  'israel': 12,\n",
       "  'france': 16,\n",
       "  'years': 19,\n",
       "  'are': 32,\n",
       "  'indian': 12,\n",
       "  'chief': 12,\n",
       "  'off': 26,\n",
       "  'top': 12,\n",
       "  'opposition': 16,\n",
       "  'party': 16,\n",
       "  'billion': 7,\n",
       "  'visit': 16,\n",
       "  'eu': 6,\n",
       "  'cup': 12,\n",
       "  'second': 16,\n",
       "  'between': 11,\n",
       "  'news': 16,\n",
       "  'chinese': 12,\n",
       "  'pakistan': 16,\n",
       "  'australian': 12,\n",
       "  'troops': 19,\n",
       "  'when': 37,\n",
       "  'four': 7,\n",
       "  'economic': 12,\n",
       "  'city': 16,\n",
       "  'market': 16,\n",
       "  'russia': 16,\n",
       "  'set': 31,\n",
       "  'attack': 16,\n",
       "  'agency': 16,\n",
       "  'end': 32,\n",
       "  'national': 12,\n",
       "  'union': 16,\n",
       "  'month': 16,\n",
       "  'early': 12,\n",
       "  'iraqi': 16,\n",
       "  'five': 7,\n",
       "  'about': 11,\n",
       "  'australia': 16,\n",
       "  'britain': 16,\n",
       "  'southern': 12,\n",
       "  'dead': 12,\n",
       "  'african': 12,\n",
       "  'army': 16,\n",
       "  'iran': 12,\n",
       "  'central': 12,\n",
       "  'ahead': 23,\n",
       "  'amid': 11,\n",
       "  'leaders': 19,\n",
       "  'back': 23,\n",
       "  'north': 23,\n",
       "  'following': 30,\n",
       "  'korea': 16,\n",
       "  'west': 14,\n",
       "  'death': 16,\n",
       "  'called': 31,\n",
       "  'street': 16,\n",
       "  'during': 11,\n",
       "  'nuclear': 12,\n",
       "  'wall': 16,\n",
       "  'open': 12,\n",
       "  'forces': 19,\n",
       "  'rights': 19,\n",
       "  'stock': 16,\n",
       "  'arrested': 31,\n",
       "  'could': 15,\n",
       "  'northern': 28,\n",
       "  'bush': 23,\n",
       "  'six': 7,\n",
       "  'general': 12,\n",
       "  'crisis': 16,\n",
       "  'taiwan': 16,\n",
       "  'elections': 19,\n",
       "  'east': 23,\n",
       "  'political': 12,\n",
       "  'major': 12,\n",
       "  'africa': 16,\n",
       "  'under': 11,\n",
       "  'attacks': 19,\n",
       "  'deal': 32,\n",
       "  'no': 8,\n",
       "  'parliament': 16,\n",
       "  'her': 22,\n",
       "  'defense': 16,\n",
       "  'spokesman': 16,\n",
       "  'meeting': 16,\n",
       "  'authorities': 19,\n",
       "  'company': 16,\n",
       "  'rebels': 33,\n",
       "  'ministry': 16,\n",
       "  'least': 14,\n",
       "  'injured': 12,\n",
       "  'key': 12,\n",
       "  'despite': 11,\n",
       "  'germany': 12,\n",
       "  'election': 16,\n",
       "  'england': 16,\n",
       "  'secretary': 16,\n",
       "  'some': 8,\n",
       "  'bid': 16,\n",
       "  'asian': 12,\n",
       "  'time': 16,\n",
       "  'months': 19,\n",
       "  'strike': 16,\n",
       "  'force': 16,\n",
       "  'air': 16,\n",
       "  'take': 28,\n",
       "  'trading': 16,\n",
       "  'third': 12,\n",
       "  'bomb': 16,\n",
       "  'may': 15,\n",
       "  'global': 12,\n",
       "  'due': 12,\n",
       "  'before': 11,\n",
       "  'nations': 19,\n",
       "  'house': 16,\n",
       "  'late': 12,\n",
       "  'record': 16,\n",
       "  'meet': 16,\n",
       "  'home': 16,\n",
       "  'capital': 16,\n",
       "  'report': 16,\n",
       "  'giant': 16,\n",
       "  'presidential': 12,\n",
       "  'win': 16,\n",
       "  'investors': 19,\n",
       "  'profit': 32,\n",
       "  'test': 16,\n",
       "  'trial': 16,\n",
       "  'aid': 16,\n",
       "  'plans': 19,\n",
       "  'held': 29,\n",
       "  'protest': 14,\n",
       "  'afghanistan': 12,\n",
       "  'economy': 16,\n",
       "  'tokyo': 12,\n",
       "  'italian': 12,\n",
       "  'head': 16,\n",
       "  'gaza': 16,\n",
       "  'plan': 16,\n",
       "  'sri': 33,\n",
       "  'since': 11,\n",
       "  'final': 12,\n",
       "  'korean': 16,\n",
       "  'left': 29,\n",
       "  'man': 16,\n",
       "  'markets': 19,\n",
       "  'turkey': 32,\n",
       "  'council': 12,\n",
       "  'reports': 19,\n",
       "  'calls': 19,\n",
       "  'zealand': 32,\n",
       "  'financial': 12,\n",
       "  'thai': 16,\n",
       "  'accused': 29,\n",
       "  'media': 19,\n",
       "  'team': 16,\n",
       "  'countries': 19,\n",
       "  'all': 8,\n",
       "  'sources': 19,\n",
       "  'help': 32,\n",
       "  'near': 11,\n",
       "  'vote': 16,\n",
       "  'london': 16,\n",
       "  'office': 16,\n",
       "  'islamic': 16,\n",
       "  'rose': 29,\n",
       "  'power': 16,\n",
       "  'philippine': 16,\n",
       "  'soldiers': 19,\n",
       "  'expected': 29,\n",
       "  'warned': 29,\n",
       "  'europe': 16,\n",
       "  'being': 30,\n",
       "  'american': 12,\n",
       "  \"'\": 2,\n",
       "  'days': 19,\n",
       "  'indonesian': 12,\n",
       "  'local': 12,\n",
       "  'support': 16,\n",
       "  'press': 16,\n",
       "  'summit': 16,\n",
       "  'or': 6,\n",
       "  'other': 12,\n",
       "  'region': 16,\n",
       "  'rate': 16,\n",
       "  'gains': 19,\n",
       "  'saying': 30,\n",
       "  'league': 16,\n",
       "  'saudi': 16,\n",
       "  'thousands': 19,\n",
       "  'human': 12,\n",
       "  'george': 32,\n",
       "  'gold': 12,\n",
       "  'high': 12,\n",
       "  'turkish': 16,\n",
       "  'suspected': 29,\n",
       "  'border': 16,\n",
       "  'hit': 16,\n",
       "  'philippines': 19,\n",
       "  'town': 16,\n",
       "  'seven': 7,\n",
       "  'him': 21,\n",
       "  'charges': 19,\n",
       "  'fire': 32,\n",
       "  'if': 11,\n",
       "  'republic': 12,\n",
       "  'part': 16,\n",
       "  'senior': 12,\n",
       "  'euros': 16,\n",
       "  'fell': 29,\n",
       "  'yen': 23,\n",
       "  'won': 31,\n",
       "  'violence': 16,\n",
       "  'died': 29,\n",
       "  'return': 16,\n",
       "  'while': 11,\n",
       "  'lead': 12,\n",
       "  'tour': 16,\n",
       "  'baghdad': 16,\n",
       "  'killing': 30,\n",
       "  'another': 8,\n",
       "  'opened': 31,\n",
       "  'pm': 16,\n",
       "  'lanka': 16,\n",
       "  'indonesia': 16,\n",
       "  'including': 30,\n",
       "  'clinton': 19,\n",
       "  'rise': 16,\n",
       "  'victory': 16,\n",
       "  'weekend': 16,\n",
       "  'most': 25,\n",
       "  'quarter': 16,\n",
       "  'points': 19,\n",
       "  'paris': 12,\n",
       "  'car': 16,\n",
       "  'italy': 19,\n",
       "  'urged': 29,\n",
       "  'public': 12,\n",
       "  'ruling': 16,\n",
       "  'still': 23,\n",
       "  'main': 12,\n",
       "  'them': 21,\n",
       "  'eastern': 12,\n",
       "  'told': 29,\n",
       "  'made': 31,\n",
       "  'afghan': 12,\n",
       "  'interest': 16,\n",
       "  'spanish': 12,\n",
       "  'bosnian': 12,\n",
       "  'malaysia': 16,\n",
       "  'nato': 12,\n",
       "  'malaysian': 12,\n",
       "  'workers': 19,\n",
       "  'members': 19,\n",
       "  'firm': 12,\n",
       "  'w': 32,\n",
       "  '--': 5,\n",
       "  'wounded': 29,\n",
       "  'women': 19,\n",
       "  'palestinians': 19,\n",
       "  'leading': 30,\n",
       "  'obama': 16,\n",
       "  'found': 29,\n",
       "  'start': 23,\n",
       "  'rebel': 12,\n",
       "  'western': 12,\n",
       "  'where': 37,\n",
       "  'through': 11,\n",
       "  'white': 12,\n",
       "  'commission': 16,\n",
       "  'hold': 28,\n",
       "  'make': 12,\n",
       "  'bill': 16,\n",
       "  'according': 30,\n",
       "  'island': 16,\n",
       "  'growth': 16,\n",
       "  'zimbabwe': 17,\n",
       "  'john': 16,\n",
       "  'arab': 16,\n",
       "  'agreement': 16,\n",
       "  'bosnia': 16,\n",
       "  'ministers': 19,\n",
       "  'fears': 19,\n",
       "  'sales': 19,\n",
       "  'eight': 7,\n",
       "  'decision': 16,\n",
       "  'spain': 16,\n",
       "  'arrived': 29,\n",
       "  'coast': 12,\n",
       "  'hopes': 19,\n",
       "  'around': 11,\n",
       "  'net': 12,\n",
       "  'cut': 16,\n",
       "  'strong': 12,\n",
       "  'took': 29,\n",
       "  'round': 16,\n",
       "  'earlier': 24,\n",
       "  'euro': 12,\n",
       "  'release': 16,\n",
       "  'action': 16,\n",
       "  'egypt': 33,\n",
       "  'health': 16,\n",
       "  'th': 19,\n",
       "  'rally': 23,\n",
       "  'shot': 12,\n",
       "  'face': 16,\n",
       "  'militants': 19,\n",
       "  'case': 16,\n",
       "  'murder': 16,\n",
       "  'move': 16,\n",
       "  'middle': 16,\n",
       "  'asia': 16,\n",
       "  'arafat': 11,\n",
       "  'washington': 16,\n",
       "  'lebanon': 16,\n",
       "  'vietnam': 12,\n",
       "  'airport': 16,\n",
       "  'fresh': 12,\n",
       "  'campaign': 16,\n",
       "  'fight': 12,\n",
       "  'september': 16,\n",
       "  'men': 19,\n",
       "  'alleged': 29,\n",
       "  'there': 9,\n",
       "  'exchange': 16,\n",
       "  'japanese': 12,\n",
       "  'data': 19,\n",
       "  'morning': 16,\n",
       "  'largest': 14,\n",
       "  'kashmir': 16,\n",
       "  'urges': 19,\n",
       "  'afp': 32,\n",
       "  'football': 16,\n",
       "  'ban': 16,\n",
       "  'law': 16,\n",
       "  'moscow': 29,\n",
       "  'march': 12,\n",
       "  'series': 16,\n",
       "  'mission': 16,\n",
       "  'discuss': 16,\n",
       "  'signed': 29,\n",
       "  'biggest': 14,\n",
       "  'suicide': 16,\n",
       "  'released': 31,\n",
       "  'gas': 16,\n",
       "  'muslim': 16,\n",
       "  'fall': 16,\n",
       "  'agreed': 29,\n",
       "  'embassy': 12,\n",
       "  'overnight': 12,\n",
       "  'match': 16,\n",
       "  'warns': 33,\n",
       "  'canada': 16,\n",
       "  'showed': 29,\n",
       "  'thailand': 12,\n",
       "  'coach': 16,\n",
       "  'york': 16,\n",
       "  'nine': 7,\n",
       "  'concerns': 19,\n",
       "  'call': 28,\n",
       "  'federal': 12,\n",
       "  'several': 12,\n",
       "  'because': 11,\n",
       "  'plane': 16,\n",
       "  'ties': 19,\n",
       "  'club': 32,\n",
       "  'province': 16,\n",
       "  'finance': 16,\n",
       "  'charged': 31,\n",
       "  'opening': 30,\n",
       "  'iranian': 12,\n",
       "  'probe': 16,\n",
       "  'launched': 31,\n",
       "  'missing': 30,\n",
       "  'ago': 11,\n",
       "  'number': 16,\n",
       "  'pakistani': 12,\n",
       "  'coalition': 16,\n",
       "  'any': 8,\n",
       "  'protests': 19,\n",
       "  'television': 16,\n",
       "  'nigeria': 19,\n",
       "  'began': 29,\n",
       "  'prison': 16,\n",
       "  'business': 16,\n",
       "  'season': 16,\n",
       "  'cricket': 16,\n",
       "  'program': 16,\n",
       "  'bangladesh': 16,\n",
       "  'conference': 16,\n",
       "  'children': 19,\n",
       "  'premier': 32,\n",
       "  'greek': 12,\n",
       "  'hospital': 16,\n",
       "  'half': 16,\n",
       "  'fighting': 30,\n",
       "  'title': 12,\n",
       "  'across': 11,\n",
       "  'pressure': 16,\n",
       "  'regional': 12,\n",
       "  'work': 16,\n",
       "  'hundreds': 19,\n",
       "  'possible': 12,\n",
       "  'star': 16,\n",
       "  'rival': 16,\n",
       "  'further': 24,\n",
       "  'department': 16,\n",
       "  'free': 12,\n",
       "  'put': 29,\n",
       "  'boost': 28,\n",
       "  'energy': 16,\n",
       "  'toll': 11,\n",
       "  'bombing': 30,\n",
       "  'efforts': 19,\n",
       "  'industry': 16,\n",
       "  'demand': 16,\n",
       "  'canadian': 12,\n",
       "  'nigerian': 12,\n",
       "  'blast': 16,\n",
       "  'envoy': 12,\n",
       "  'statement': 16,\n",
       "  'deadly': 23,\n",
       "  'swiss': 12,\n",
       "  'controversial': 12,\n",
       "  'center': 16,\n",
       "  'gulf': 16,\n",
       "  'saddam': 12,\n",
       "  'sign': 16,\n",
       "  'woman': 16,\n",
       "  'go': 32,\n",
       "  'wins': 33,\n",
       "  'democratic': 12,\n",
       "  'ireland': 16,\n",
       "  'jail': 16,\n",
       "  'crash': 16,\n",
       "  'profits': 19,\n",
       "  'body': 16,\n",
       "  'arrest': 12,\n",
       "  'labor': 16,\n",
       "  'budget': 16,\n",
       "  'champions': 19,\n",
       "  'nearly': 23,\n",
       "  'claims': 19,\n",
       "  'victims': 19,\n",
       "  'continued': 29,\n",
       "  'congress': 12,\n",
       "  'way': 16,\n",
       "  'cooperation': 16,\n",
       "  'food': 16,\n",
       "  'recent': 12,\n",
       "  'us-led': 12,\n",
       "  'losses': 19,\n",
       "  'index': 16,\n",
       "  'latest': 14,\n",
       "  'polls': 19,\n",
       "  'taking': 30,\n",
       "  'again': 23,\n",
       "  'japan': 12,\n",
       "  'november': 12,\n",
       "  'crimes': 19,\n",
       "  'massive': 12,\n",
       "  'king': 16,\n",
       "  'newspaper': 16,\n",
       "  'october': 17,\n",
       "  'armed': 29,\n",
       "  'nation': 16,\n",
       "  'only': 23,\n",
       "  'refugees': 33,\n",
       "  'aimed': 31,\n",
       "  'led': 29,\n",
       "  'jailed': 12,\n",
       "  'pay': 16,\n",
       "  'beijing': 16,\n",
       "  'row': 16,\n",
       "  'should': 15,\n",
       "  'results': 19,\n",
       "  'egyptian': 12,\n",
       "  'launch': 12,\n",
       "  'mark': 16,\n",
       "  'she': 21,\n",
       "  'arms': 33,\n",
       "  'joint': 12,\n",
       "  'rates': 19,\n",
       "  'drug': 16,\n",
       "  'control': 16,\n",
       "  'terrorism': 16,\n",
       "  'june': 16,\n",
       "  'ukraine': 12,\n",
       "  'dutch': 16,\n",
       "  'corruption': 16,\n",
       "  'ordered': 29,\n",
       "  'chechnya': 12,\n",
       "  'just': 23,\n",
       "  'service': 16,\n",
       "  'weeks': 19,\n",
       "  'others': 19,\n",
       "  'radio': 32,\n",
       "  'policy': 16,\n",
       "  'rescue': 16,\n",
       "  'champion': 16,\n",
       "  'sector': 16,\n",
       "  'investment': 16,\n",
       "  'heavy': 12,\n",
       "  'threat': 16,\n",
       "  'race': 16,\n",
       "  'failed': 29,\n",
       "  'leave': 12,\n",
       "  'serb': 12,\n",
       "  'battle': 16,\n",
       "  'conflict': 16,\n",
       "  'banks': 19,\n",
       "  'jewish': 12,\n",
       "  'met': 29,\n",
       "  'de': 10,\n",
       "  'wants': 33,\n",
       "  'future': 12,\n",
       "  'hussein': 12,\n",
       "  'loss': 16,\n",
       "  'join': 16,\n",
       "  'without': 11,\n",
       "  'groups': 19,\n",
       "  'strip': 16,\n",
       "  'figures': 19,\n",
       "  'life': 16,\n",
       "  'confirmed': 29,\n",
       "  'slightly': 23,\n",
       "  'sudan': 31,\n",
       "  'can': 15,\n",
       "  'hours': 19,\n",
       "  'board': 16,\n",
       "  'olympic': 11,\n",
       "  'illegal': 12,\n",
       "  'earnings': 19,\n",
       "  'ended': 29,\n",
       "  'planned': 31,\n",
       "  'sanctions': 19,\n",
       "  'stage': 16,\n",
       "  'jordan': 12,\n",
       "  'step': 16,\n",
       "  'price': 16,\n",
       "  'terror': 16,\n",
       "  'shanghai': 29,\n",
       "  'communist': 12,\n",
       "  'analysts': 19,\n",
       "  'cabinet': 16,\n",
       "  'sentenced': 29,\n",
       "  'clashes': 19,\n",
       "  'corp': 32,\n",
       "  '-year-old': 23,\n",
       "  'detained': 31,\n",
       "  'deputy': 16,\n",
       "  'red': 29,\n",
       "  'what': 35,\n",
       "  'soldier': 13,\n",
       "  'process': 16,\n",
       "  'emergency': 16,\n",
       "  'airlines': 19,\n",
       "  'january': 12,\n",
       "  'warning': 16,\n",
       "  'among': 11,\n",
       "  'fourth': 12,\n",
       "  'give': 32,\n",
       "  'play': 28,\n",
       "  'family': 16,\n",
       "  'hamas': 19,\n",
       "  'swedish': 12,\n",
       "  'poll': 16,\n",
       "  'get': 32,\n",
       "  'special': 12,\n",
       "  'ii': 16,\n",
       "  'contract': 16,\n",
       "  'ready': 12,\n",
       "  'camp': 16,\n",
       "  'companies': 19,\n",
       "  'named': 31,\n",
       "  'church': 12,\n",
       "  'space': 16,\n",
       "  'member': 16,\n",
       "  'recovery': 16,\n",
       "  'opens': 33,\n",
       "  'past': 12,\n",
       "  'fund': 16,\n",
       "  'previous': 12,\n",
       "  'rejected': 31,\n",
       "  'use': 16,\n",
       "  'deficit': 16,\n",
       "  'denied': 29,\n",
       "  'resolution': 16,\n",
       "  'operation': 16,\n",
       "  'barack': 16,\n",
       "  'jobs': 19,\n",
       "  'sea': 16,\n",
       "  'published': 31,\n",
       "  'ceasefire': 19,\n",
       "  'say': 32,\n",
       "  'taliban': 12,\n",
       "  'outside': 12,\n",
       "  'terrorist': 12,\n",
       "  'run': 16,\n",
       "  'congo': 16,\n",
       "  'services': 19,\n",
       "  'committee': 16,\n",
       "  'order': 16,\n",
       "  'ambassador': 16,\n",
       "  'july': 23,\n",
       "  'dispute': 12,\n",
       "  'development': 16,\n",
       "  'claimed': 29,\n",
       "  'michael': 16,\n",
       "  'prisoners': 19,\n",
       "  'keep': 32,\n",
       "  'civil': 12,\n",
       "  'ground': 16,\n",
       "  'side': 23,\n",
       "  'english': 12,\n",
       "  'session': 16,\n",
       "  'accord': 16,\n",
       "  'post': 16,\n",
       "  'role': 16,\n",
       "  'parliamentary': 12,\n",
       "  'jakarta': 16,\n",
       "  'production': 16,\n",
       "  'system': 16,\n",
       "  'scandal': 16,\n",
       "  'mixed': 12,\n",
       "  'parties': 19,\n",
       "  'offer': 32,\n",
       "  'arabia': 11,\n",
       "  'yasser': 13,\n",
       "  'games': 19,\n",
       "  'show': 32,\n",
       "  'separatist': 12,\n",
       "  'chairman': 16,\n",
       "  'diplomatic': 12,\n",
       "  'irish': 16,\n",
       "  'until': 11,\n",
       "  'film': 16,\n",
       "  'annual': 12,\n",
       "  'pct': 16,\n",
       "  'inflation': 16,\n",
       "  'takes': 33,\n",
       "  'relations': 19,\n",
       "  'vowed': 29,\n",
       "  'earthquake': 16,\n",
       "  'went': 29,\n",
       "  'clash': 12,\n",
       "  'weapons': 19,\n",
       "  'territory': 12,\n",
       "  'increase': 16,\n",
       "  'rugby': 16,\n",
       "  'judge': 16,\n",
       "  'building': 30,\n",
       "  'greece': 12,\n",
       "  'kuwait': 16,\n",
       "  'supreme': 16,\n",
       "  'syria': 16,\n",
       "  'suspended': 29,\n",
       "  'condemned': 12,\n",
       "  'legal': 12,\n",
       "  'become': 16,\n",
       "  'train': 16,\n",
       "  'justice': 12,\n",
       "  'daily': 12,\n",
       "  'stop': 28,\n",
       "  'airline': 16,\n",
       "  'organization': 16,\n",
       "  'association': 16,\n",
       "  'seeks': 33,\n",
       "  'forced': 31,\n",
       "  'al-qaeda': 12,\n",
       "  'whether': 11,\n",
       "  'travel': 16,\n",
       "  'suspect': 32,\n",
       "  'mexico': 12,\n",
       "  'place': 16,\n",
       "  'grand': 12,\n",
       "  'drop': 16,\n",
       "  'championship': 16,\n",
       "  'tax': 16,\n",
       "  'troubled': 12,\n",
       "  'lawmakers': 19,\n",
       "  'moslem': 32,\n",
       "  'seek': 12,\n",
       "  'debt': 16,\n",
       "  'blair': 16,\n",
       "  'militant': 12,\n",
       "  'chechen': 16,\n",
       "  'station': 16,\n",
       "  'myanmar': 16,\n",
       "  'chirac': 16,\n",
       "  'december': 28,\n",
       "  's': 16,\n",
       "  'long': 23,\n",
       "  'later': 23,\n",
       "  'coup': 12,\n",
       "  'industrial': 12,\n",
       "  'sent': 16,\n",
       "  'officers': 19,\n",
       "  'buying': 30,\n",
       "  'pope': 16,\n",
       "  'reform': 16,\n",
       "  'profit-taking': 16,\n",
       "  'kill': 16,\n",
       "  'continue': 28,\n",
       "  'send': 16,\n",
       "  'plant': 16,\n",
       "  'nepal': 11,\n",
       "  'both': 8,\n",
       "  'movement': 16,\n",
       "  'huge': 12,\n",
       "  'falls': 33,\n",
       "  'change': 16,\n",
       "  'anniversary': 12,\n",
       "  'rising': 30,\n",
       "  'private': 12,\n",
       "  'line': 16,\n",
       "  'denies': 19,\n",
       "  'kenya': 32,\n",
       "  'gets': 33,\n",
       "  'mass': 16,\n",
       "  'must': 15,\n",
       "  'now': 23,\n",
       "  'away': 23,\n",
       "  'likely': 12,\n",
       "  'road': 16,\n",
       "  'arrives': 33,\n",
       "  'same': 12,\n",
       "  'measures': 19,\n",
       "  'fraud': 16,\n",
       "  'experts': 19,\n",
       "  'interior': 12,\n",
       "  'august': 16,\n",
       "  'paul': 16,\n",
       "  'students': 19,\n",
       "  'putin': 32,\n",
       "  'trying': 30,\n",
       "  'hits': 19,\n",
       "  'bus': 12,\n",
       "  'base': 16,\n",
       "  'sees': 19,\n",
       "  'faces': 33,\n",
       "  'currency': 16,\n",
       "  'behind': 11,\n",
       "  'operations': 19,\n",
       "  'big': 12,\n",
       "  'cuts': 19,\n",
       "  'signs': 19,\n",
       "  'domestic': 12,\n",
       "  'firms': 19,\n",
       "  'low': 12,\n",
       "  'money': 16,\n",
       "  'reserve': 16,\n",
       "  'staff': 16,\n",
       "  'players': 19,\n",
       "  'explosion': 32,\n",
       "  'asked': 31,\n",
       "  'refugee': 12,\n",
       "  'orders': 19,\n",
       "  'given': 31,\n",
       "  'sharply': 23,\n",
       "  'maker': 16,\n",
       "  'begin': 16,\n",
       "  'banking': 30,\n",
       "  'beat': 16,\n",
       "  'rules': 19,\n",
       "  'governor': 32,\n",
       "  'stake': 16,\n",
       "  'source': 16,\n",
       "  'pacific': 16,\n",
       "  'czech': 16,\n",
       "  'sharon': 16,\n",
       "  'activists': 19,\n",
       "  'seeking': 30,\n",
       "  'consumer': 16,\n",
       "  'growing': 30,\n",
       "  'expressed': 29,\n",
       "  'buy': 16,\n",
       "  'concern': 16,\n",
       "  'civilians': 19,\n",
       "  'gave': 29,\n",
       "  'level': 12,\n",
       "  'medical': 12,\n",
       "  'squad': 16,\n",
       "  'fired': 29,\n",
       "  'resume': 12,\n",
       "  'many': 12,\n",
       "  'championships': 19,\n",
       "  'georgia': 32,\n",
       "  'rises': 33,\n",
       "  'aircraft': 16,\n",
       "  'night': 16,\n",
       "  'america': 11,\n",
       "  'reach': 16,\n",
       "  'lebanese': 12,\n",
       "  'executive': 16,\n",
       "  'almost': 23,\n",
       "  'land': 32,\n",
       "  'credit': 16,\n",
       "  'stay': 16,\n",
       "  'debate': 16,\n",
       "  'seen': 31,\n",
       "  'lost': 31,\n",
       "  'term': 16,\n",
       "  'issue': 16,\n",
       "  'threatened': 29,\n",
       "  'internet': 12,\n",
       "  'black': 12,\n",
       "  'prize': 16,\n",
       "  'cyprus': 16,\n",
       "  'brazil': 12,\n",
       "  'officer': 16,\n",
       "  'break': 16,\n",
       "  'trip': 16,\n",
       "  'confidence': 16,\n",
       "  'commander': 16,\n",
       "  'manager': 16,\n",
       "  'allow': 28,\n",
       "  'christmas': 16,\n",
       "  'brokers': 19,\n",
       "  'current': 12,\n",
       "  'ivory': 12,\n",
       "  'hurricane': 16,\n",
       "  'holding': 30,\n",
       "  'flu': 16,\n",
       "  'raid': 16,\n",
       "  'reached': 29,\n",
       "  'negotiations': 19,\n",
       "  'taken': 31,\n",
       "  'banned': 29,\n",
       "  'positive': 12,\n",
       "  'port': 16,\n",
       "  'kills': 19,\n",
       "  'auto': 16,\n",
       "  'dies': 19,\n",
       "  'seized': 29,\n",
       "  'polish': 12,\n",
       "  'regime': 12,\n",
       "  'area': 16,\n",
       "  'kurdish': 12,\n",
       "  'towards': 19,\n",
       "  'ruled': 29,\n",
       "  'disaster': 16,\n",
       "  'son': 16,\n",
       "  'build': 28,\n",
       "  'issues': 19,\n",
       "  'croatia': 28,\n",
       "  'project': 16,\n",
       "  'used': 31,\n",
       "  'flight': 16,\n",
       "  'treaty': 16,\n",
       "  'vice': 16,\n",
       "  'rule': 16,\n",
       "  'sweden': 29,\n",
       "  'whose': 36,\n",
       "  'sell': 16,\n",
       "  'child': 16,\n",
       "  'prince': 16,\n",
       "  'crashed': 29,\n",
       "  'guerrillas': 19,\n",
       "  'vows': 19,\n",
       "  'referendum': 32,\n",
       "  'job': 16,\n",
       "  ...},\n",
       " [('<padding>', 'JJ'),\n",
       "  ('<unk>', 'NNP'),\n",
       "  ('<s>', 'NNP'),\n",
       "  ('</s>', 'NNP'),\n",
       "  ('#', '#'),\n",
       "  ('the', 'DT'),\n",
       "  (',', ','),\n",
       "  ('to', 'TO'),\n",
       "  ('in', 'IN'),\n",
       "  ('of', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('on', 'IN'),\n",
       "  (\"'s\", 'POS'),\n",
       "  ('and', 'CC'),\n",
       "  ('said', 'VBD'),\n",
       "  ('for', 'IN'),\n",
       "  ('us', 'PRP'),\n",
       "  ('with', 'IN'),\n",
       "  ('as', 'RB'),\n",
       "  ('at', 'IN'),\n",
       "  ('tuesday', 'NN'),\n",
       "  ('thursday', 'IN'),\n",
       "  ('an', 'DT'),\n",
       "  ('wednesday', 'NN'),\n",
       "  ('after', 'IN'),\n",
       "  ('``', '``'),\n",
       "  ('from', 'IN'),\n",
       "  ('by', 'IN'),\n",
       "  ('monday', 'NN'),\n",
       "  ('friday', 'JJ'),\n",
       "  ('<', 'NNP'),\n",
       "  ('unk', 'NN'),\n",
       "  ('>', 'NN'),\n",
       "  ('that', 'WDT'),\n",
       "  ('percent', 'NN'),\n",
       "  ('was', 'VBD'),\n",
       "  ('president', 'NN'),\n",
       "  ('new', 'JJ'),\n",
       "  ('his', 'PRP$'),\n",
       "  ('over', 'NN'),\n",
       "  ('has', 'VBZ'),\n",
       "  ('its', 'PRP$'),\n",
       "  ('here', 'RB'),\n",
       "  ('minister', 'NN'),\n",
       "  ('it', 'PRP'),\n",
       "  ('against', 'IN'),\n",
       "  ('two', 'CD'),\n",
       "  ('government', 'NN'),\n",
       "  ('-lrb-', 'NNP'),\n",
       "  ('-rrb-', 'NNP'),\n",
       "  ('will', 'MD'),\n",
       "  ('sunday', 'VB'),\n",
       "  ('were', 'VBD'),\n",
       "  ('up', 'RB'),\n",
       "  ('world', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('police', 'JJ'),\n",
       "  ('saturday', 'RB'),\n",
       "  ('first', 'RB'),\n",
       "  ('iraq', 'VB'),\n",
       "  ('their', 'PRP$'),\n",
       "  ('prices', 'NNS'),\n",
       "  ('china', 'VBP'),\n",
       "  ('be', 'VB'),\n",
       "  ('officials', 'NNS'),\n",
       "  ('south', 'RB'),\n",
       "  ('have', 'VBP'),\n",
       "  ('he', 'PRP'),\n",
       "  ('killed', 'VBN'),\n",
       "  ('un', 'JJ'),\n",
       "  ('dollars', 'NNS'),\n",
       "  ('year', 'NN'),\n",
       "  ('-', ':'),\n",
       "  ('united', 'JJ'),\n",
       "  ('people', 'NNS'),\n",
       "  ('state', 'NN'),\n",
       "  ('million', 'CD'),\n",
       "  ('shares', 'NNS'),\n",
       "  ('talks', 'NNS'),\n",
       "  ('french', 'JJ'),\n",
       "  ('official', 'JJ'),\n",
       "  ('into', 'IN'),\n",
       "  ('war', 'NN'),\n",
       "  ('country', 'NN'),\n",
       "  ('out', 'IN'),\n",
       "  ('foreign', 'JJ'),\n",
       "  ('more', 'JJR'),\n",
       "  ('european', 'JJ'),\n",
       "  ('reported', 'VBD'),\n",
       "  ('british', 'JJ'),\n",
       "  ('prime', 'JJ'),\n",
       "  ('three', 'CD'),\n",
       "  ('bank', 'NN'),\n",
       "  ('who', 'WP'),\n",
       "  ('group', 'NN'),\n",
       "  ('leader', 'NN'),\n",
       "  ('former', 'JJ'),\n",
       "  ('which', 'WDT'),\n",
       "  ('military', 'JJ'),\n",
       "  ('week', 'NN'),\n",
       "  ('had', 'VBD'),\n",
       "  ('one', 'CD'),\n",
       "  ('but', 'CC'),\n",
       "  ('security', 'NN'),\n",
       "  ('states', 'NNS'),\n",
       "  ('share', 'NN'),\n",
       "  ('court', 'NN'),\n",
       "  ('close', 'JJ'),\n",
       "  ('dealers', 'NNS'),\n",
       "  ('oil', 'NN'),\n",
       "  ('international', 'JJ'),\n",
       "  ('israeli', 'NN'),\n",
       "  ('higher', 'RBR'),\n",
       "  ('this', 'DT'),\n",
       "  ('down', 'RP'),\n",
       "  ('would', 'MD'),\n",
       "  ('been', 'VBN'),\n",
       "  ('not', 'RB'),\n",
       "  ('next', 'JJ'),\n",
       "  ('trade', 'NN'),\n",
       "  ('hong', 'NN'),\n",
       "  ('kong', 'VB'),\n",
       "  ('lower', 'JJR'),\n",
       "  ('says', 'VBZ'),\n",
       "  ('russian', 'JJ'),\n",
       "  ('dollar', 'NN'),\n",
       "  ('announced', 'VBD'),\n",
       "  ('india', 'JJ'),\n",
       "  ('day', 'NN'),\n",
       "  ('palestinian', 'JJ'),\n",
       "  ('last', 'JJ'),\n",
       "  ('closed', 'VBD'),\n",
       "  ('they', 'PRP'),\n",
       "  ('stocks', 'NNS'),\n",
       "  ('german', 'VBD'),\n",
       "  ('peace', 'NN'),\n",
       "  ('than', 'IN'),\n",
       "  ('israel', 'JJ'),\n",
       "  ('france', 'NN'),\n",
       "  ('years', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('indian', 'JJ'),\n",
       "  ('chief', 'JJ'),\n",
       "  ('off', 'RP'),\n",
       "  ('top', 'JJ'),\n",
       "  ('opposition', 'NN'),\n",
       "  ('party', 'NN'),\n",
       "  ('billion', 'CD'),\n",
       "  ('visit', 'NN'),\n",
       "  ('eu', 'CC'),\n",
       "  ('cup', 'JJ'),\n",
       "  ('second', 'NN'),\n",
       "  ('between', 'IN'),\n",
       "  ('news', 'NN'),\n",
       "  ('chinese', 'JJ'),\n",
       "  ('pakistan', 'NN'),\n",
       "  ('australian', 'JJ'),\n",
       "  ('troops', 'NNS'),\n",
       "  ('when', 'WRB'),\n",
       "  ('four', 'CD'),\n",
       "  ('economic', 'JJ'),\n",
       "  ('city', 'NN'),\n",
       "  ('market', 'NN'),\n",
       "  ('russia', 'NN'),\n",
       "  ('set', 'VBN'),\n",
       "  ('attack', 'NN'),\n",
       "  ('agency', 'NN'),\n",
       "  ('end', 'VBP'),\n",
       "  ('national', 'JJ'),\n",
       "  ('union', 'NN'),\n",
       "  ('month', 'NN'),\n",
       "  ('early', 'JJ'),\n",
       "  ('iraqi', 'NN'),\n",
       "  ('five', 'CD'),\n",
       "  ('about', 'IN'),\n",
       "  ('australia', 'NN'),\n",
       "  ('britain', 'NN'),\n",
       "  ('southern', 'JJ'),\n",
       "  ('dead', 'JJ'),\n",
       "  ('african', 'JJ'),\n",
       "  ('army', 'NN'),\n",
       "  ('iran', 'JJ'),\n",
       "  ('central', 'JJ'),\n",
       "  ('ahead', 'RB'),\n",
       "  ('amid', 'IN'),\n",
       "  ('leaders', 'NNS'),\n",
       "  ('back', 'RB'),\n",
       "  ('north', 'RB'),\n",
       "  ('following', 'VBG'),\n",
       "  ('korea', 'NN'),\n",
       "  ('west', 'JJS'),\n",
       "  ('death', 'NN'),\n",
       "  ('called', 'VBN'),\n",
       "  ('street', 'NN'),\n",
       "  ('during', 'IN'),\n",
       "  ('nuclear', 'JJ'),\n",
       "  ('wall', 'NN'),\n",
       "  ('open', 'JJ'),\n",
       "  ('forces', 'NNS'),\n",
       "  ('rights', 'NNS'),\n",
       "  ('stock', 'NN'),\n",
       "  ('arrested', 'VBN'),\n",
       "  ('could', 'MD'),\n",
       "  ('northern', 'VB'),\n",
       "  ('bush', 'RB'),\n",
       "  ('six', 'CD'),\n",
       "  ('general', 'JJ'),\n",
       "  ('crisis', 'NN'),\n",
       "  ('taiwan', 'NN'),\n",
       "  ('elections', 'NNS'),\n",
       "  ('east', 'RB'),\n",
       "  ('political', 'JJ'),\n",
       "  ('major', 'JJ'),\n",
       "  ('africa', 'NN'),\n",
       "  ('under', 'IN'),\n",
       "  ('attacks', 'NNS'),\n",
       "  ('deal', 'VBP'),\n",
       "  ('no', 'DT'),\n",
       "  ('parliament', 'NN'),\n",
       "  ('her', 'PRP$'),\n",
       "  ('defense', 'NN'),\n",
       "  ('spokesman', 'NN'),\n",
       "  ('meeting', 'NN'),\n",
       "  ('authorities', 'NNS'),\n",
       "  ('company', 'NN'),\n",
       "  ('rebels', 'VBZ'),\n",
       "  ('ministry', 'NN'),\n",
       "  ('least', 'JJS'),\n",
       "  ('injured', 'JJ'),\n",
       "  ('key', 'JJ'),\n",
       "  ('despite', 'IN'),\n",
       "  ('germany', 'JJ'),\n",
       "  ('election', 'NN'),\n",
       "  ('england', 'NN'),\n",
       "  ('secretary', 'NN'),\n",
       "  ('some', 'DT'),\n",
       "  ('bid', 'NN'),\n",
       "  ('asian', 'JJ'),\n",
       "  ('time', 'NN'),\n",
       "  ('months', 'NNS'),\n",
       "  ('strike', 'NN'),\n",
       "  ('force', 'NN'),\n",
       "  ('air', 'NN'),\n",
       "  ('take', 'VB'),\n",
       "  ('trading', 'NN'),\n",
       "  ('third', 'JJ'),\n",
       "  ('bomb', 'NN'),\n",
       "  ('may', 'MD'),\n",
       "  ('global', 'JJ'),\n",
       "  ('due', 'JJ'),\n",
       "  ('before', 'IN'),\n",
       "  ('nations', 'NNS'),\n",
       "  ('house', 'NN'),\n",
       "  ('late', 'JJ'),\n",
       "  ('record', 'NN'),\n",
       "  ('meet', 'NN'),\n",
       "  ('home', 'NN'),\n",
       "  ('capital', 'NN'),\n",
       "  ('report', 'NN'),\n",
       "  ('giant', 'NN'),\n",
       "  ('presidential', 'JJ'),\n",
       "  ('win', 'NN'),\n",
       "  ('investors', 'NNS'),\n",
       "  ('profit', 'VBP'),\n",
       "  ('test', 'NN'),\n",
       "  ('trial', 'NN'),\n",
       "  ('aid', 'NN'),\n",
       "  ('plans', 'NNS'),\n",
       "  ('held', 'VBD'),\n",
       "  ('protest', 'JJS'),\n",
       "  ('afghanistan', 'JJ'),\n",
       "  ('economy', 'NN'),\n",
       "  ('tokyo', 'JJ'),\n",
       "  ('italian', 'JJ'),\n",
       "  ('head', 'NN'),\n",
       "  ('gaza', 'NN'),\n",
       "  ('plan', 'NN'),\n",
       "  ('sri', 'VBZ'),\n",
       "  ('since', 'IN'),\n",
       "  ('final', 'JJ'),\n",
       "  ('korean', 'NN'),\n",
       "  ('left', 'VBD'),\n",
       "  ('man', 'NN'),\n",
       "  ('markets', 'NNS'),\n",
       "  ('turkey', 'VBP'),\n",
       "  ('council', 'JJ'),\n",
       "  ('reports', 'NNS'),\n",
       "  ('calls', 'NNS'),\n",
       "  ('zealand', 'VBP'),\n",
       "  ('financial', 'JJ'),\n",
       "  ('thai', 'NN'),\n",
       "  ('accused', 'VBD'),\n",
       "  ('media', 'NNS'),\n",
       "  ('team', 'NN'),\n",
       "  ('countries', 'NNS'),\n",
       "  ('all', 'DT'),\n",
       "  ('sources', 'NNS'),\n",
       "  ('help', 'VBP'),\n",
       "  ('near', 'IN'),\n",
       "  ('vote', 'NN'),\n",
       "  ('london', 'NN'),\n",
       "  ('office', 'NN'),\n",
       "  ('islamic', 'NN'),\n",
       "  ('rose', 'VBD'),\n",
       "  ('power', 'NN'),\n",
       "  ('philippine', 'NN'),\n",
       "  ('soldiers', 'NNS'),\n",
       "  ('expected', 'VBD'),\n",
       "  ('warned', 'VBD'),\n",
       "  ('europe', 'NN'),\n",
       "  ('being', 'VBG'),\n",
       "  ('american', 'JJ'),\n",
       "  (\"'\", \"''\"),\n",
       "  ('days', 'NNS'),\n",
       "  ('indonesian', 'JJ'),\n",
       "  ('local', 'JJ'),\n",
       "  ('support', 'NN'),\n",
       "  ('press', 'NN'),\n",
       "  ('summit', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('other', 'JJ'),\n",
       "  ('region', 'NN'),\n",
       "  ('rate', 'NN'),\n",
       "  ('gains', 'NNS'),\n",
       "  ('saying', 'VBG'),\n",
       "  ('league', 'NN'),\n",
       "  ('saudi', 'NN'),\n",
       "  ('thousands', 'NNS'),\n",
       "  ('human', 'JJ'),\n",
       "  ('george', 'VBP'),\n",
       "  ('gold', 'JJ'),\n",
       "  ('high', 'JJ'),\n",
       "  ('turkish', 'NN'),\n",
       "  ('suspected', 'VBD'),\n",
       "  ('border', 'NN'),\n",
       "  ('hit', 'NN'),\n",
       "  ('philippines', 'NNS'),\n",
       "  ('town', 'NN'),\n",
       "  ('seven', 'CD'),\n",
       "  ('him', 'PRP'),\n",
       "  ('charges', 'NNS'),\n",
       "  ('fire', 'VBP'),\n",
       "  ('if', 'IN'),\n",
       "  ('republic', 'JJ'),\n",
       "  ('part', 'NN'),\n",
       "  ('senior', 'JJ'),\n",
       "  ('euros', 'NN'),\n",
       "  ('fell', 'VBD'),\n",
       "  ('yen', 'RB'),\n",
       "  ('won', 'VBN'),\n",
       "  ('violence', 'NN'),\n",
       "  ('died', 'VBD'),\n",
       "  ('return', 'NN'),\n",
       "  ('while', 'IN'),\n",
       "  ('lead', 'JJ'),\n",
       "  ('tour', 'NN'),\n",
       "  ('baghdad', 'NN'),\n",
       "  ('killing', 'VBG'),\n",
       "  ('another', 'DT'),\n",
       "  ('opened', 'VBN'),\n",
       "  ('pm', 'NN'),\n",
       "  ('lanka', 'NN'),\n",
       "  ('indonesia', 'NN'),\n",
       "  ('including', 'VBG'),\n",
       "  ('clinton', 'NNS'),\n",
       "  ('rise', 'NN'),\n",
       "  ('victory', 'NN'),\n",
       "  ('weekend', 'NN'),\n",
       "  ('most', 'RBS'),\n",
       "  ('quarter', 'NN'),\n",
       "  ('points', 'NNS'),\n",
       "  ('paris', 'JJ'),\n",
       "  ('car', 'NN'),\n",
       "  ('italy', 'NNS'),\n",
       "  ('urged', 'VBD'),\n",
       "  ('public', 'JJ'),\n",
       "  ('ruling', 'NN'),\n",
       "  ('still', 'RB'),\n",
       "  ('main', 'JJ'),\n",
       "  ('them', 'PRP'),\n",
       "  ('eastern', 'JJ'),\n",
       "  ('told', 'VBD'),\n",
       "  ('made', 'VBN'),\n",
       "  ('afghan', 'JJ'),\n",
       "  ('interest', 'NN'),\n",
       "  ('spanish', 'JJ'),\n",
       "  ('bosnian', 'JJ'),\n",
       "  ('malaysia', 'NN'),\n",
       "  ('nato', 'JJ'),\n",
       "  ('malaysian', 'JJ'),\n",
       "  ('workers', 'NNS'),\n",
       "  ('members', 'NNS'),\n",
       "  ('firm', 'JJ'),\n",
       "  ('w', 'VBP'),\n",
       "  ('--', ':'),\n",
       "  ('wounded', 'VBD'),\n",
       "  ('women', 'NNS'),\n",
       "  ('palestinians', 'NNS'),\n",
       "  ('leading', 'VBG'),\n",
       "  ('obama', 'NN'),\n",
       "  ('found', 'VBD'),\n",
       "  ('start', 'RB'),\n",
       "  ('rebel', 'JJ'),\n",
       "  ('western', 'JJ'),\n",
       "  ('where', 'WRB'),\n",
       "  ('through', 'IN'),\n",
       "  ('white', 'JJ'),\n",
       "  ('commission', 'NN'),\n",
       "  ('hold', 'VB'),\n",
       "  ('make', 'JJ'),\n",
       "  ('bill', 'NN'),\n",
       "  ('according', 'VBG'),\n",
       "  ('island', 'NN'),\n",
       "  ('growth', 'NN'),\n",
       "  ('zimbabwe', 'NNP'),\n",
       "  ('john', 'NN'),\n",
       "  ('arab', 'NN'),\n",
       "  ('agreement', 'NN'),\n",
       "  ('bosnia', 'NN'),\n",
       "  ('ministers', 'NNS'),\n",
       "  ('fears', 'NNS'),\n",
       "  ('sales', 'NNS'),\n",
       "  ('eight', 'CD'),\n",
       "  ('decision', 'NN'),\n",
       "  ('spain', 'NN'),\n",
       "  ('arrived', 'VBD'),\n",
       "  ('coast', 'JJ'),\n",
       "  ('hopes', 'NNS'),\n",
       "  ('around', 'IN'),\n",
       "  ('net', 'JJ'),\n",
       "  ('cut', 'NN'),\n",
       "  ('strong', 'JJ'),\n",
       "  ('took', 'VBD'),\n",
       "  ('round', 'NN'),\n",
       "  ('earlier', 'RBR'),\n",
       "  ('euro', 'JJ'),\n",
       "  ('release', 'NN'),\n",
       "  ('action', 'NN'),\n",
       "  ('egypt', 'VBZ'),\n",
       "  ('health', 'NN'),\n",
       "  ('th', 'NNS'),\n",
       "  ('rally', 'RB'),\n",
       "  ('shot', 'JJ'),\n",
       "  ('face', 'NN'),\n",
       "  ('militants', 'NNS'),\n",
       "  ('case', 'NN'),\n",
       "  ('murder', 'NN'),\n",
       "  ('move', 'NN'),\n",
       "  ('middle', 'NN'),\n",
       "  ('asia', 'NN'),\n",
       "  ('arafat', 'IN'),\n",
       "  ('washington', 'NN'),\n",
       "  ('lebanon', 'NN'),\n",
       "  ('vietnam', 'JJ'),\n",
       "  ('airport', 'NN'),\n",
       "  ('fresh', 'JJ'),\n",
       "  ('campaign', 'NN'),\n",
       "  ('fight', 'JJ'),\n",
       "  ('september', 'NN'),\n",
       "  ('men', 'NNS'),\n",
       "  ('alleged', 'VBD'),\n",
       "  ('there', 'EX'),\n",
       "  ('exchange', 'NN'),\n",
       "  ('japanese', 'JJ'),\n",
       "  ('data', 'NNS'),\n",
       "  ('morning', 'NN'),\n",
       "  ('largest', 'JJS'),\n",
       "  ('kashmir', 'NN'),\n",
       "  ('urges', 'NNS'),\n",
       "  ('afp', 'VBP'),\n",
       "  ('football', 'NN'),\n",
       "  ('ban', 'NN'),\n",
       "  ('law', 'NN'),\n",
       "  ('moscow', 'VBD'),\n",
       "  ('march', 'JJ'),\n",
       "  ('series', 'NN'),\n",
       "  ('mission', 'NN'),\n",
       "  ('discuss', 'NN'),\n",
       "  ('signed', 'VBD'),\n",
       "  ('biggest', 'JJS'),\n",
       "  ('suicide', 'NN'),\n",
       "  ('released', 'VBN'),\n",
       "  ('gas', 'NN'),\n",
       "  ('muslim', 'NN'),\n",
       "  ('fall', 'NN'),\n",
       "  ('agreed', 'VBD'),\n",
       "  ('embassy', 'JJ'),\n",
       "  ('overnight', 'JJ'),\n",
       "  ('match', 'NN'),\n",
       "  ('warns', 'VBZ'),\n",
       "  ('canada', 'NN'),\n",
       "  ('showed', 'VBD'),\n",
       "  ('thailand', 'JJ'),\n",
       "  ('coach', 'NN'),\n",
       "  ('york', 'NN'),\n",
       "  ('nine', 'CD'),\n",
       "  ('concerns', 'NNS'),\n",
       "  ('call', 'VB'),\n",
       "  ('federal', 'JJ'),\n",
       "  ('several', 'JJ'),\n",
       "  ('because', 'IN'),\n",
       "  ('plane', 'NN'),\n",
       "  ('ties', 'NNS'),\n",
       "  ('club', 'VBP'),\n",
       "  ('province', 'NN'),\n",
       "  ('finance', 'NN'),\n",
       "  ('charged', 'VBN'),\n",
       "  ('opening', 'VBG'),\n",
       "  ('iranian', 'JJ'),\n",
       "  ('probe', 'NN'),\n",
       "  ('launched', 'VBN'),\n",
       "  ('missing', 'VBG'),\n",
       "  ('ago', 'IN'),\n",
       "  ('number', 'NN'),\n",
       "  ('pakistani', 'JJ'),\n",
       "  ('coalition', 'NN'),\n",
       "  ('any', 'DT'),\n",
       "  ('protests', 'NNS'),\n",
       "  ('television', 'NN'),\n",
       "  ('nigeria', 'NNS'),\n",
       "  ('began', 'VBD'),\n",
       "  ('prison', 'NN'),\n",
       "  ('business', 'NN'),\n",
       "  ('season', 'NN'),\n",
       "  ('cricket', 'NN'),\n",
       "  ('program', 'NN'),\n",
       "  ('bangladesh', 'NN'),\n",
       "  ('conference', 'NN'),\n",
       "  ('children', 'NNS'),\n",
       "  ('premier', 'VBP'),\n",
       "  ('greek', 'JJ'),\n",
       "  ('hospital', 'NN'),\n",
       "  ('half', 'NN'),\n",
       "  ('fighting', 'VBG'),\n",
       "  ('title', 'JJ'),\n",
       "  ('across', 'IN'),\n",
       "  ('pressure', 'NN'),\n",
       "  ('regional', 'JJ'),\n",
       "  ('work', 'NN'),\n",
       "  ('hundreds', 'NNS'),\n",
       "  ('possible', 'JJ'),\n",
       "  ('star', 'NN'),\n",
       "  ('rival', 'NN'),\n",
       "  ('further', 'RBR'),\n",
       "  ('department', 'NN'),\n",
       "  ('free', 'JJ'),\n",
       "  ('put', 'VBD'),\n",
       "  ('boost', 'VB'),\n",
       "  ('energy', 'NN'),\n",
       "  ('toll', 'IN'),\n",
       "  ('bombing', 'VBG'),\n",
       "  ('efforts', 'NNS'),\n",
       "  ('industry', 'NN'),\n",
       "  ('demand', 'NN'),\n",
       "  ('canadian', 'JJ'),\n",
       "  ('nigerian', 'JJ'),\n",
       "  ('blast', 'NN'),\n",
       "  ('envoy', 'JJ'),\n",
       "  ('statement', 'NN'),\n",
       "  ('deadly', 'RB'),\n",
       "  ('swiss', 'JJ'),\n",
       "  ('controversial', 'JJ'),\n",
       "  ('center', 'NN'),\n",
       "  ('gulf', 'NN'),\n",
       "  ('saddam', 'JJ'),\n",
       "  ('sign', 'NN'),\n",
       "  ('woman', 'NN'),\n",
       "  ('go', 'VBP'),\n",
       "  ('wins', 'VBZ'),\n",
       "  ('democratic', 'JJ'),\n",
       "  ('ireland', 'NN'),\n",
       "  ('jail', 'NN'),\n",
       "  ('crash', 'NN'),\n",
       "  ('profits', 'NNS'),\n",
       "  ('body', 'NN'),\n",
       "  ('arrest', 'JJ'),\n",
       "  ('labor', 'NN'),\n",
       "  ('budget', 'NN'),\n",
       "  ('champions', 'NNS'),\n",
       "  ('nearly', 'RB'),\n",
       "  ('claims', 'NNS'),\n",
       "  ('victims', 'NNS'),\n",
       "  ('continued', 'VBD'),\n",
       "  ('congress', 'JJ'),\n",
       "  ('way', 'NN'),\n",
       "  ('cooperation', 'NN'),\n",
       "  ('food', 'NN'),\n",
       "  ('recent', 'JJ'),\n",
       "  ('us-led', 'JJ'),\n",
       "  ('losses', 'NNS'),\n",
       "  ('index', 'NN'),\n",
       "  ('latest', 'JJS'),\n",
       "  ('polls', 'NNS'),\n",
       "  ('taking', 'VBG'),\n",
       "  ('again', 'RB'),\n",
       "  ('japan', 'JJ'),\n",
       "  ('november', 'JJ'),\n",
       "  ('crimes', 'NNS'),\n",
       "  ('massive', 'JJ'),\n",
       "  ('king', 'NN'),\n",
       "  ('newspaper', 'NN'),\n",
       "  ('october', 'NNP'),\n",
       "  ('armed', 'VBD'),\n",
       "  ('nation', 'NN'),\n",
       "  ('only', 'RB'),\n",
       "  ('refugees', 'VBZ'),\n",
       "  ('aimed', 'VBN'),\n",
       "  ('led', 'VBD'),\n",
       "  ('jailed', 'JJ'),\n",
       "  ('pay', 'NN'),\n",
       "  ('beijing', 'NN'),\n",
       "  ('row', 'NN'),\n",
       "  ('should', 'MD'),\n",
       "  ('results', 'NNS'),\n",
       "  ('egyptian', 'JJ'),\n",
       "  ('launch', 'JJ'),\n",
       "  ('mark', 'NN'),\n",
       "  ('she', 'PRP'),\n",
       "  ('arms', 'VBZ'),\n",
       "  ('joint', 'JJ'),\n",
       "  ('rates', 'NNS'),\n",
       "  ('drug', 'NN'),\n",
       "  ('control', 'NN'),\n",
       "  ('terrorism', 'NN'),\n",
       "  ('june', 'NN'),\n",
       "  ('ukraine', 'JJ'),\n",
       "  ('dutch', 'NN'),\n",
       "  ('corruption', 'NN'),\n",
       "  ('ordered', 'VBD'),\n",
       "  ('chechnya', 'JJ'),\n",
       "  ('just', 'RB'),\n",
       "  ('service', 'NN'),\n",
       "  ('weeks', 'NNS'),\n",
       "  ('others', 'NNS'),\n",
       "  ('radio', 'VBP'),\n",
       "  ('policy', 'NN'),\n",
       "  ('rescue', 'NN'),\n",
       "  ('champion', 'NN'),\n",
       "  ('sector', 'NN'),\n",
       "  ('investment', 'NN'),\n",
       "  ('heavy', 'JJ'),\n",
       "  ('threat', 'NN'),\n",
       "  ('race', 'NN'),\n",
       "  ('failed', 'VBD'),\n",
       "  ('leave', 'JJ'),\n",
       "  ('serb', 'JJ'),\n",
       "  ('battle', 'NN'),\n",
       "  ('conflict', 'NN'),\n",
       "  ('banks', 'NNS'),\n",
       "  ('jewish', 'JJ'),\n",
       "  ('met', 'VBD'),\n",
       "  ('de', 'FW'),\n",
       "  ('wants', 'VBZ'),\n",
       "  ('future', 'JJ'),\n",
       "  ('hussein', 'JJ'),\n",
       "  ('loss', 'NN'),\n",
       "  ('join', 'NN'),\n",
       "  ('without', 'IN'),\n",
       "  ('groups', 'NNS'),\n",
       "  ('strip', 'NN'),\n",
       "  ('figures', 'NNS'),\n",
       "  ('life', 'NN'),\n",
       "  ('confirmed', 'VBD'),\n",
       "  ('slightly', 'RB'),\n",
       "  ('sudan', 'VBN'),\n",
       "  ('can', 'MD'),\n",
       "  ('hours', 'NNS'),\n",
       "  ('board', 'NN'),\n",
       "  ('olympic', 'IN'),\n",
       "  ('illegal', 'JJ'),\n",
       "  ('earnings', 'NNS'),\n",
       "  ('ended', 'VBD'),\n",
       "  ('planned', 'VBN'),\n",
       "  ('sanctions', 'NNS'),\n",
       "  ('stage', 'NN'),\n",
       "  ('jordan', 'JJ'),\n",
       "  ('step', 'NN'),\n",
       "  ('price', 'NN'),\n",
       "  ('terror', 'NN'),\n",
       "  ('shanghai', 'VBD'),\n",
       "  ('communist', 'JJ'),\n",
       "  ('analysts', 'NNS'),\n",
       "  ('cabinet', 'NN'),\n",
       "  ('sentenced', 'VBD'),\n",
       "  ('clashes', 'NNS'),\n",
       "  ('corp', 'VBP'),\n",
       "  ('-year-old', 'RB'),\n",
       "  ('detained', 'VBN'),\n",
       "  ('deputy', 'NN'),\n",
       "  ('red', 'VBD'),\n",
       "  ('what', 'WP'),\n",
       "  ('soldier', 'JJR'),\n",
       "  ('process', 'NN'),\n",
       "  ('emergency', 'NN'),\n",
       "  ('airlines', 'NNS'),\n",
       "  ('january', 'JJ'),\n",
       "  ('warning', 'NN'),\n",
       "  ('among', 'IN'),\n",
       "  ('fourth', 'JJ'),\n",
       "  ('give', 'VBP'),\n",
       "  ('play', 'VB'),\n",
       "  ('family', 'NN'),\n",
       "  ('hamas', 'NNS'),\n",
       "  ('swedish', 'JJ'),\n",
       "  ('poll', 'NN'),\n",
       "  ('get', 'VBP'),\n",
       "  ('special', 'JJ'),\n",
       "  ('ii', 'NN'),\n",
       "  ('contract', 'NN'),\n",
       "  ('ready', 'JJ'),\n",
       "  ('camp', 'NN'),\n",
       "  ('companies', 'NNS'),\n",
       "  ('named', 'VBN'),\n",
       "  ('church', 'JJ'),\n",
       "  ('space', 'NN'),\n",
       "  ('member', 'NN'),\n",
       "  ('recovery', 'NN'),\n",
       "  ('opens', 'VBZ'),\n",
       "  ('past', 'JJ'),\n",
       "  ('fund', 'NN'),\n",
       "  ('previous', 'JJ'),\n",
       "  ('rejected', 'VBN'),\n",
       "  ('use', 'NN'),\n",
       "  ('deficit', 'NN'),\n",
       "  ('denied', 'VBD'),\n",
       "  ('resolution', 'NN'),\n",
       "  ('operation', 'NN'),\n",
       "  ('barack', 'NN'),\n",
       "  ('jobs', 'NNS'),\n",
       "  ('sea', 'NN'),\n",
       "  ('published', 'VBN'),\n",
       "  ('ceasefire', 'NNS'),\n",
       "  ('say', 'VBP'),\n",
       "  ('taliban', 'JJ'),\n",
       "  ('outside', 'JJ'),\n",
       "  ('terrorist', 'JJ'),\n",
       "  ('run', 'NN'),\n",
       "  ('congo', 'NN'),\n",
       "  ('services', 'NNS'),\n",
       "  ('committee', 'NN'),\n",
       "  ('order', 'NN'),\n",
       "  ('ambassador', 'NN'),\n",
       "  ('july', 'RB'),\n",
       "  ('dispute', 'JJ'),\n",
       "  ('development', 'NN'),\n",
       "  ('claimed', 'VBD'),\n",
       "  ('michael', 'NN'),\n",
       "  ('prisoners', 'NNS'),\n",
       "  ('keep', 'VBP'),\n",
       "  ('civil', 'JJ'),\n",
       "  ('ground', 'NN'),\n",
       "  ('side', 'RB'),\n",
       "  ('english', 'JJ'),\n",
       "  ('session', 'NN'),\n",
       "  ('accord', 'NN'),\n",
       "  ('post', 'NN'),\n",
       "  ('role', 'NN'),\n",
       "  ('parliamentary', 'JJ'),\n",
       "  ('jakarta', 'NN'),\n",
       "  ('production', 'NN'),\n",
       "  ('system', 'NN'),\n",
       "  ('scandal', 'NN'),\n",
       "  ('mixed', 'JJ'),\n",
       "  ('parties', 'NNS'),\n",
       "  ('offer', 'VBP'),\n",
       "  ('arabia', 'IN'),\n",
       "  ('yasser', 'JJR'),\n",
       "  ('games', 'NNS'),\n",
       "  ('show', 'VBP'),\n",
       "  ('separatist', 'JJ'),\n",
       "  ('chairman', 'NN'),\n",
       "  ('diplomatic', 'JJ'),\n",
       "  ('irish', 'NN'),\n",
       "  ('until', 'IN'),\n",
       "  ('film', 'NN'),\n",
       "  ('annual', 'JJ'),\n",
       "  ('pct', 'NN'),\n",
       "  ('inflation', 'NN'),\n",
       "  ('takes', 'VBZ'),\n",
       "  ('relations', 'NNS'),\n",
       "  ('vowed', 'VBD'),\n",
       "  ('earthquake', 'NN'),\n",
       "  ('went', 'VBD'),\n",
       "  ('clash', 'JJ'),\n",
       "  ('weapons', 'NNS'),\n",
       "  ('territory', 'JJ'),\n",
       "  ('increase', 'NN'),\n",
       "  ('rugby', 'NN'),\n",
       "  ('judge', 'NN'),\n",
       "  ('building', 'VBG'),\n",
       "  ('greece', 'JJ'),\n",
       "  ('kuwait', 'NN'),\n",
       "  ('supreme', 'NN'),\n",
       "  ('syria', 'NN'),\n",
       "  ('suspended', 'VBD'),\n",
       "  ('condemned', 'JJ'),\n",
       "  ('legal', 'JJ'),\n",
       "  ('become', 'NN'),\n",
       "  ('train', 'NN'),\n",
       "  ('justice', 'JJ'),\n",
       "  ('daily', 'JJ'),\n",
       "  ('stop', 'VB'),\n",
       "  ('airline', 'NN'),\n",
       "  ('organization', 'NN'),\n",
       "  ('association', 'NN'),\n",
       "  ('seeks', 'VBZ'),\n",
       "  ('forced', 'VBN'),\n",
       "  ('al-qaeda', 'JJ'),\n",
       "  ('whether', 'IN'),\n",
       "  ('travel', 'NN'),\n",
       "  ('suspect', 'VBP'),\n",
       "  ('mexico', 'JJ'),\n",
       "  ('place', 'NN'),\n",
       "  ('grand', 'JJ'),\n",
       "  ('drop', 'NN'),\n",
       "  ('championship', 'NN'),\n",
       "  ('tax', 'NN'),\n",
       "  ('troubled', 'JJ'),\n",
       "  ('lawmakers', 'NNS'),\n",
       "  ('moslem', 'VBP'),\n",
       "  ('seek', 'JJ'),\n",
       "  ('debt', 'NN'),\n",
       "  ('blair', 'NN'),\n",
       "  ('militant', 'JJ'),\n",
       "  ('chechen', 'NN'),\n",
       "  ('station', 'NN'),\n",
       "  ('myanmar', 'NN'),\n",
       "  ('chirac', 'NN'),\n",
       "  ('december', 'VB'),\n",
       "  ('s', 'NN'),\n",
       "  ('long', 'RB'),\n",
       "  ('later', 'RB'),\n",
       "  ('coup', 'JJ'),\n",
       "  ('industrial', 'JJ'),\n",
       "  ('sent', 'NN'),\n",
       "  ('officers', 'NNS'),\n",
       "  ('buying', 'VBG'),\n",
       "  ('pope', 'NN'),\n",
       "  ('reform', 'NN'),\n",
       "  ('profit-taking', 'NN'),\n",
       "  ('kill', 'NN'),\n",
       "  ('continue', 'VB'),\n",
       "  ('send', 'NN'),\n",
       "  ('plant', 'NN'),\n",
       "  ('nepal', 'IN'),\n",
       "  ('both', 'DT'),\n",
       "  ('movement', 'NN'),\n",
       "  ('huge', 'JJ'),\n",
       "  ('falls', 'VBZ'),\n",
       "  ('change', 'NN'),\n",
       "  ('anniversary', 'JJ'),\n",
       "  ('rising', 'VBG'),\n",
       "  ('private', 'JJ'),\n",
       "  ('line', 'NN'),\n",
       "  ('denies', 'NNS'),\n",
       "  ('kenya', 'VBP'),\n",
       "  ('gets', 'VBZ'),\n",
       "  ('mass', 'NN'),\n",
       "  ('must', 'MD'),\n",
       "  ('now', 'RB'),\n",
       "  ('away', 'RB'),\n",
       "  ('likely', 'JJ'),\n",
       "  ('road', 'NN'),\n",
       "  ('arrives', 'VBZ'),\n",
       "  ('same', 'JJ'),\n",
       "  ('measures', 'NNS'),\n",
       "  ('fraud', 'NN'),\n",
       "  ('experts', 'NNS'),\n",
       "  ('interior', 'JJ'),\n",
       "  ('august', 'NN'),\n",
       "  ('paul', 'NN'),\n",
       "  ('students', 'NNS'),\n",
       "  ('putin', 'VBP'),\n",
       "  ('trying', 'VBG'),\n",
       "  ('hits', 'NNS'),\n",
       "  ('bus', 'JJ'),\n",
       "  ('base', 'NN'),\n",
       "  ('sees', 'NNS'),\n",
       "  ('faces', 'VBZ'),\n",
       "  ('currency', 'NN'),\n",
       "  ('behind', 'IN'),\n",
       "  ('operations', 'NNS'),\n",
       "  ('big', 'JJ'),\n",
       "  ('cuts', 'NNS'),\n",
       "  ('signs', 'NNS'),\n",
       "  ('domestic', 'JJ'),\n",
       "  ('firms', 'NNS'),\n",
       "  ('low', 'JJ'),\n",
       "  ('money', 'NN'),\n",
       "  ('reserve', 'NN'),\n",
       "  ('staff', 'NN'),\n",
       "  ('players', 'NNS'),\n",
       "  ('explosion', 'VBP'),\n",
       "  ('asked', 'VBN'),\n",
       "  ('refugee', 'JJ'),\n",
       "  ('orders', 'NNS'),\n",
       "  ('given', 'VBN'),\n",
       "  ('sharply', 'RB'),\n",
       "  ('maker', 'NN'),\n",
       "  ('begin', 'NN'),\n",
       "  ('banking', 'VBG'),\n",
       "  ('beat', 'NN'),\n",
       "  ('rules', 'NNS'),\n",
       "  ('governor', 'VBP'),\n",
       "  ('stake', 'NN'),\n",
       "  ('source', 'NN'),\n",
       "  ('pacific', 'NN'),\n",
       "  ('czech', 'NN'),\n",
       "  ('sharon', 'NN'),\n",
       "  ('activists', 'NNS'),\n",
       "  ('seeking', 'VBG'),\n",
       "  ('consumer', 'NN'),\n",
       "  ('growing', 'VBG'),\n",
       "  ('expressed', 'VBD'),\n",
       "  ('buy', 'NN'),\n",
       "  ('concern', 'NN'),\n",
       "  ('civilians', 'NNS'),\n",
       "  ('gave', 'VBD'),\n",
       "  ('level', 'JJ'),\n",
       "  ('medical', 'JJ'),\n",
       "  ('squad', 'NN'),\n",
       "  ('fired', 'VBD'),\n",
       "  ('resume', 'JJ'),\n",
       "  ('many', 'JJ'),\n",
       "  ('championships', 'NNS'),\n",
       "  ('georgia', 'VBP'),\n",
       "  ('rises', 'VBZ'),\n",
       "  ('aircraft', 'NN'),\n",
       "  ('night', 'NN'),\n",
       "  ('america', 'IN'),\n",
       "  ('reach', 'NN'),\n",
       "  ('lebanese', 'JJ'),\n",
       "  ('executive', 'NN'),\n",
       "  ('almost', 'RB'),\n",
       "  ('land', 'VBP'),\n",
       "  ('credit', 'NN'),\n",
       "  ('stay', 'NN'),\n",
       "  ('debate', 'NN'),\n",
       "  ('seen', 'VBN'),\n",
       "  ('lost', 'VBN'),\n",
       "  ('term', 'NN'),\n",
       "  ('issue', 'NN'),\n",
       "  ('threatened', 'VBD'),\n",
       "  ('internet', 'JJ'),\n",
       "  ('black', 'JJ'),\n",
       "  ('prize', 'NN'),\n",
       "  ('cyprus', 'NN'),\n",
       "  ('brazil', 'JJ'),\n",
       "  ('officer', 'NN'),\n",
       "  ('break', 'NN'),\n",
       "  ('trip', 'NN'),\n",
       "  ('confidence', 'NN'),\n",
       "  ('commander', 'NN'),\n",
       "  ('manager', 'NN'),\n",
       "  ('allow', 'VB'),\n",
       "  ('christmas', 'NN'),\n",
       "  ('brokers', 'NNS'),\n",
       "  ('current', 'JJ'),\n",
       "  ('ivory', 'JJ'),\n",
       "  ('hurricane', 'NN'),\n",
       "  ('holding', 'VBG'),\n",
       "  ('flu', 'NN'),\n",
       "  ('raid', 'NN'),\n",
       "  ('reached', 'VBD'),\n",
       "  ('negotiations', 'NNS'),\n",
       "  ('taken', 'VBN'),\n",
       "  ('banned', 'VBD'),\n",
       "  ('positive', 'JJ'),\n",
       "  ('port', 'NN'),\n",
       "  ('kills', 'NNS'),\n",
       "  ('auto', 'NN'),\n",
       "  ('dies', 'NNS'),\n",
       "  ('seized', 'VBD'),\n",
       "  ('polish', 'JJ'),\n",
       "  ('regime', 'JJ'),\n",
       "  ('area', 'NN'),\n",
       "  ('kurdish', 'JJ'),\n",
       "  ('towards', 'NNS'),\n",
       "  ('ruled', 'VBD'),\n",
       "  ('disaster', 'NN'),\n",
       "  ('son', 'NN'),\n",
       "  ('build', 'VB'),\n",
       "  ('issues', 'NNS'),\n",
       "  ('croatia', 'VB'),\n",
       "  ('project', 'NN'),\n",
       "  ('used', 'VBN'),\n",
       "  ('flight', 'NN'),\n",
       "  ('treaty', 'NN'),\n",
       "  ('vice', 'NN'),\n",
       "  ('rule', 'NN'),\n",
       "  ('sweden', 'VBD'),\n",
       "  ('whose', 'WP$'),\n",
       "  ('sell', 'NN'),\n",
       "  ('child', 'NN'),\n",
       "  ('prince', 'NN'),\n",
       "  ('crashed', 'VBD'),\n",
       "  ('guerrillas', 'NNS'),\n",
       "  ('vows', 'NNS'),\n",
       "  ('referendum', 'VBP'),\n",
       "  ('job', 'NN'),\n",
       "  ...])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list = get_pos_tags_dict(word_dict.keys())\n",
    "pos_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_r1e_f2nCKK"
   },
   "source": [
    "## Model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MABp2aWMoNKh"
   },
   "source": [
    "Encoder-Decoder model with attention mechanism.\n",
    "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/model.py\n",
    "\n",
    "1.   Encoder : Used LSTM cell with stack_bidirectional_dynamic_rnn.\n",
    "2.   Decoder : Used LSTM BasicDecoder for training, and BeamSearchDecoder for inference.\n",
    "3.   Attention Mechanism : Used BahdanauAttention with weight normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKHwTTZg5eJa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "#from utils import get_init_embedding\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n",
    "        self.vocabulary_size = len(reversed_dict)\n",
    "        self.embedding_size = args.embedding_size\n",
    "        self.num_hidden = args.num_hidden\n",
    "        self.num_layers = args.num_layers\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.beam_width = args.beam_width\n",
    "        if not forward_only:\n",
    "            self.keep_prob = args.keep_prob\n",
    "        else:\n",
    "            self.keep_prob = 1.0\n",
    "        self.cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "        with tf.variable_scope(\"decoder/projection\"):\n",
    "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
    "\n",
    "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
    "        self.X_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            if not forward_only and args.glove: #training\n",
    "                #init_embeddings = tf.constant(get_init_embedding(word_dict ,reversed_dict, self.embedding_size), dtype=tf.float32)\n",
    "                init_embeddings = tf.constant(test_embedding, dtype=tf.float32)\n",
    "            else: #testing\n",
    "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
    "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
    "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
    "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
    "\n",
    "        with tf.name_scope(\"encoder\"):\n",
    "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            # TODO: Dropout\n",
    "            fw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in fw_cells]\n",
    "            bw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in bw_cells]\n",
    "\n",
    "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
    "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
    "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
    "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
    "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
    "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
    "            decoder_cell = self.cell(self.num_hidden * 2)\n",
    "\n",
    "            if not forward_only: #trainig\n",
    "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
    "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
    "                self.decoder_output = outputs.rnn_output\n",
    "                self.logits = tf.transpose(\n",
    "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
    "                self.logits_reshape = tf.concat(\n",
    "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
    "            else: #testing\n",
    "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
    "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
    "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
    "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
    "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
    "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                    cell=decoder_cell,\n",
    "                    embedding=self.embeddings,\n",
    "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
    "                    end_token=tf.constant(3),\n",
    "                    initial_state=initial_state,\n",
    "                    beam_width=self.beam_width,\n",
    "                    output_layer=self.projection_layer\n",
    "                )\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
    "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if not forward_only: #training\n",
    "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
    "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
    "                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))\n",
    "                # TODO: Regularization\n",
    "                params = tf.trainable_variables()\n",
    "                gradients = tf.gradients(self.loss, params)\n",
    "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_xON6b4nS8x"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ak3tIhETnaIJ"
   },
   "source": [
    "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/train.py\n",
    "\n",
    "We used sumdata/train/train.article.txt and sumdata/train/train.title.txt for training data. To train the model, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vq9ZA0hFnUZJ",
    "outputId": "65296e28-3025-4a7b-a2ec-6227dcffe53b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing from previous trained model: /home/sbu_nlp2019/AbstractiveTextSummarization/Model2/dataset/saved_model_2/model.ckpt-25000 ...\n",
      "INFO:tensorflow:Restoring parameters from /home/sbu_nlp2019/AbstractiveTextSummarization/Model2/dataset/saved_model_2/model.ckpt-25000\n",
      "\n",
      "Iteration starts.\n",
      "Number of batches per epoch : 3125\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.perf_counter()\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "#from model import Model\n",
    "#from utils import build_dict, build_dataset, batch_iter\n",
    "\n",
    "# Uncomment next 2 lines to suppress error and Tensorflow info verbosity. Or change logging levels\n",
    "# tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#def add_arguments(parser):\n",
    "#    parser.add_argument(\"--num_hidden\", type=int, default=150, help=\"Network size.\")\n",
    "#    parser.add_argument(\"--num_layers\", type=int, default=2, help=\"Network depth.\")\n",
    "#    parser.add_argument(\"--beam_width\", type=int, default=10, help=\"Beam width for beam search decoder.\")\n",
    "#    parser.add_argument(\"--glove\", action=\"store_true\", help=\"Use glove as initial word embedding.\")\n",
    "#    parser.add_argument(\"--embedding_size\", type=int, default=300, help=\"Word embedding size.\")\n",
    "#\n",
    "#    parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"Learning rate.\")\n",
    "#    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size.\")\n",
    "#    parser.add_argument(\"--num_epochs\", type=int, default=10, help=\"Number of epochs.\")\n",
    "#    parser.add_argument(\"--keep_prob\", type=float, default=0.8, help=\"Dropout keep prob.\")\n",
    "#\n",
    "#    parser.add_argument(\"--toy\", action=\"store_true\", help=\"Use only 50K samples of data\")\n",
    "#\n",
    "#    parser.add_argument(\"--with_model\", action=\"store_true\", help=\"Continue from previously saved model\")\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "  \n",
    "args.num_hidden=150\n",
    "args.num_layers=2\n",
    "args.beam_width=10\n",
    "args.glove=\"store_true\"\n",
    "args.embedding_size=320\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=64\n",
    "args.num_epochs=10\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=False #\"store_true\"\n",
    "\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#add_arguments(parser)\n",
    "#args = parser.parse_args()\n",
    "#with open(\"args.pickle\", \"wb\") as f:\n",
    "#    pickle.dump(args, f)\n",
    "\n",
    "if not os.path.exists(default_path + \"saved_model_2\"):\n",
    "    os.mkdir(default_path + \"saved_model_2\")\n",
    "else:\n",
    "    #if args.with_model:\n",
    "        old_model_checkpoint_path = open(default_path + 'saved_model_2/checkpoint', 'r')\n",
    "#         old_model_checkpoint_path = \"\".join([default_path + \"saved_model_2/\",old_model_checkpoint_path.read().splitlines()[0].split('\"')[1] ])\n",
    "#  abs Change made by paras\n",
    "        old_model_checkpoint_path = old_model_checkpoint_path.read().splitlines()[0].split('\"')[1]\n",
    "\n",
    "\n",
    "#print(\"Building dictionary...\")\n",
    "#word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", args.toy)\n",
    "#print(\"Loading training dataset...\")\n",
    "#train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, args.toy)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    if 'old_model_checkpoint_path' in globals():\n",
    "        print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\n",
    "        saver.restore(sess, old_model_checkpoint_path )\n",
    "\n",
    "    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n",
    "    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1\n",
    "\n",
    "    print(\"\\nIteration starts.\")\n",
    "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
    "    for batch_x, batch_y in batches:\n",
    "        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
    "        batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), batch_y))\n",
    "        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
    "        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], batch_y))\n",
    "\n",
    "        batch_decoder_input = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
    "        batch_decoder_output = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
    "\n",
    "        train_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "            model.decoder_input: batch_decoder_input,\n",
    "            model.decoder_len: batch_decoder_len,\n",
    "            model.decoder_target: batch_decoder_output\n",
    "        }\n",
    "\n",
    "        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(\"step {0}: loss = {1}\".format(step, loss))\n",
    "\n",
    "        if step % num_batches_per_epoch == 0:\n",
    "            hours, rem = divmod(time.perf_counter() - start, 3600)\n",
    "            minutes, seconds = divmod(rem, 60)\n",
    "            saver.save(sess, default_path + \"saved_model_2/model.ckpt\", global_step=step)\n",
    "            print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
    "            \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lkWJfa_CYjPo"
   },
   "source": [
    "last training was \n",
    "```\n",
    "step 47000: loss = 8.033841133117676\n",
    "step 48000: loss = 9.481734275817871\n",
    "step 49000: loss = 7.188093662261963\n",
    "step 50000: loss = 14.354914665222168\n",
    "Epoch 16: Model is saved. Elapsed: 02:19:23.39\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbbtwLe7njGS"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCBIdwESnlhZ"
   },
   "source": [
    "https://github.com/dongjun-Lee/text-summarization-tensorflow/blob/master/test.py\n",
    "\n",
    "Generate summary of each article in sumdata/train/valid.article.filter.txt by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "1IUJ0dpon1Hi",
    "outputId": "091d9f83-9db1-4594-f1ed-8398a26535bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary...\n",
      "Loading validation dataset...\n",
      "Loading article and reference...\n",
      "Loading saved model...\n",
      "INFO:tensorflow:Restoring parameters from drive/My Drive/NLP/project/rnn-lstm/data/saved_model_2/model.ckpt-62500\n",
      "Writing summaries to 'result2.txt'...\n",
      "Summaries have been generated\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "#from model import Model\n",
    "#from utils import build_dict, build_dataset, batch_iter\n",
    "\n",
    "\n",
    "#with open(\"args.pickle\", \"rb\") as f:\n",
    "#    args = pickle.load(f)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class args:\n",
    "    pass\n",
    "  \n",
    "args.num_hidden=150\n",
    "args.num_layers=2\n",
    "args.beam_width=10\n",
    "args.glove=\"store_true\"\n",
    "args.embedding_size=320\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=64\n",
    "args.num_epochs=10\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=True\n",
    "\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
    "print(\"Loading validation dataset...\")\n",
    "valid_x = build_dataset(\"valid\", word_dict, article_max_len, summary_max_len, args.toy)\n",
    "valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
    "print(\"Loading article and reference...\")\n",
    "article = get_text_list(valid_article_path, args.toy)\n",
    "reference = get_text_list(valid_title_path, args.toy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Loading saved model...\")\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(default_path + \"saved_model_2/\")\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
    "\n",
    "    print(\"Writing summaries to 'result2.txt'...\")\n",
    "    for batch_x, _ in batches:\n",
    "        batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "\n",
    "        valid_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "        }\n",
    "\n",
    "        prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
    "        prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "        summary_array = []\n",
    "        with open(default_path + \"result2.txt\", \"wb\") as f:\n",
    "            for line in prediction_output:\n",
    "                summary = list()\n",
    "                for word in line:\n",
    "                    if word == \"</s>\":\n",
    "                        break\n",
    "                    if word not in summary:\n",
    "                        summary.append(word)\n",
    "                summary_array.append(\" \".join(summary))\n",
    "                #print(\" \".join(summary), file=f)\n",
    "\n",
    "    print('Summaries have been generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZl_ZCQpRMRt"
   },
   "outputs": [],
   "source": [
    "summary_array = []\n",
    "with open(default_path + \"result2.txt\", \"wb\") as f:\n",
    "    for line in prediction_output:\n",
    "        summary = list()\n",
    "        for word in line:\n",
    "            if word == \"</s>\":\n",
    "                break\n",
    "            if word not in summary:\n",
    "                summary.append(word)\n",
    "        summary_array.append(\" \".join(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "fELp35RzRd1w",
    "outputId": "f78cafcc-8a1b-4320-8a64-149ab2a38aad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['world champion ryder retires from skating',\n",
       " 'us businesses storm endorses tightening of illegal immigrants']"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_array[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-Kj9e5M9lBr"
   },
   "source": [
    "## Evaluate & write output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svL3XWnXaTsW"
   },
   "source": [
    "for comparing (good resources)\n",
    "\n",
    "1.  [thunlp]( https://github.com/thunlp/TensorFlow-Summarization)     works with duc2003\n",
    "2.  [textsum](https://github.com/tensorflow/models/tree/master/research/textsum )   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "Tyi0SXZL0GnH",
    "outputId": "65e956b9-9a4d-4e8e-d3a0-bd71bb9bd84c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumeval\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/87/bfc0f9397b9421305863edfdd2dbea637e47204976cb5473535c856338f4/sumeval-0.2.2.tar.gz (80kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 2.6MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: plac>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from sumeval) (0.9.6)\n",
      "Collecting sacrebleu>=1.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/0e/e5/93d252182f7cbd4b59bb3ec5797e2ce33cfd6f5aadaf327db170cf4b7887/sacrebleu-1.4.2-py3-none-any.whl\n",
      "Collecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/60/ec/836a494dbaa72541f691ec4e66f29fdc8db9bcc7f49e1c2d457ba13ced42/portalocker-1.5.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.3.2->sumeval) (3.6.6)\n",
      "Building wheels for collected packages: sumeval\n",
      "  Building wheel for sumeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sumeval: filename=sumeval-0.2.2-cp36-none-any.whl size=54535 sha256=e1d0767f326cf4dcd35008707bcbd72ca01e709b2ef51069b2ce5c36b3cb23d7\n",
      "  Stored in directory: /root/.cache/pip/wheels/7b/6f/57/19ceecab21445c88f3c565735fa1887b4cd18d340c972eb445\n",
      "Successfully built sumeval\n",
      "Installing collected packages: portalocker, sacrebleu, sumeval\n",
      "Successfully installed portalocker-1.5.1 sacrebleu-1.4.2 sumeval-0.2.2\n",
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!pip install sumeval\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "x6nERCu29oPS",
    "outputId": "6057d7e7-1e09-4f08-d051-f23af726e2fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.stocks=(nsubj)=>close\n",
      "<BasicElement: stocks-[nsubj]->close>\n",
      "a.percent=(dobj)=>close\n",
      "<BasicElement: percent-[dobj]->close>\n",
      "b.fresh=(amod)=>record\n",
      "a.shares=(nsubj)=>close\n",
      "<BasicElement: shares-[nsubj]->close>\n",
      "a.percent=(dobj)=>close\n",
      "<BasicElement: percent-[dobj]->close>\n",
      "ROUGE-1: 0.5454545454545454\n",
      "ROUGE-2: 0.2222222222222222\n",
      "ROUGE-L: 0.5454545454545454\n",
      "ROUGE-BE: 0.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5454545454545454,\n",
       " 0.2222222222222222,\n",
       " 0.5454545454545454,\n",
       " 0.4,\n",
       " 7.386099955930605)"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/chakki-works/sumeval\n",
    "#https://github.com/Tian312/awesome-text-summarization\n",
    "\n",
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "from sumeval.metrics.bleu import BLEUCalculator\n",
    "\n",
    "def eval_rouges(refrence_summary,model_summary):\n",
    "    refrence_summary = \"tokyo shares close up #.## percent\"\n",
    "    model_summary = \"tokyo stocks close up # percent to fresh record high\"\n",
    "\n",
    "    rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
    "\n",
    "    rouge_1 = rouge.rouge_n(\n",
    "                summary=model_summary,\n",
    "                references=refrence_summary,\n",
    "                n=1)\n",
    "\n",
    "    rouge_2 = rouge.rouge_n(\n",
    "                summary=model_summary,\n",
    "                references=[refrence_summary],\n",
    "                n=2)\n",
    "    \n",
    "    rouge_l = rouge.rouge_l(\n",
    "                summary=model_summary,\n",
    "                references=[refrence_summary])\n",
    "    \n",
    "    # You need spaCy to calculate ROUGE-BE\n",
    "    \n",
    "    rouge_be = rouge.rouge_be(\n",
    "                summary=model_summary,\n",
    "                references=[refrence_summary])\n",
    "\n",
    "    bleu = BLEUCalculator()\n",
    "    bleu_score = bleu.bleu( summary=model_summary,\n",
    "                        references=[refrence_summary])\n",
    "\n",
    "    print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}, ROUGE-BE: {}\".format(\n",
    "        rouge_1, rouge_2, rouge_l, rouge_be\n",
    "    ).replace(\", \", \"\\n\"))\n",
    "    \n",
    "    return rouge_1, rouge_2,rouge_l,rouge_be,bleu_score\n",
    "eval_rouges('a','b')\n",
    "#rouge_1, rouge_2,rouge_l,rouge_be = eval_rouges( \"tokyo shares close up #.## percent\",  \"tokyo stocks close up # percent to fresh record high\")\n",
    "\n",
    "#print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}, ROUGE-BE: {}\".format(     rouge_1, rouge_2, rouge_l, rouge_be).replace(\", \", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EyKcd5ffKEBJ",
    "outputId": "723bd299-b702-4dd2-b0a2-d52d1cda73d9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3976ad977b69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msumm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary_array\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'example'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0marticle_element\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mSubElement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'article'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_array' is not defined"
     ]
    }
   ],
   "source": [
    "#https://pymotw.com/2/xml/etree/ElementTree/create.html\n",
    "\n",
    "bleu_arr = []\n",
    "rouge_1_arr  = []\n",
    "rouge_2_arr  = []\n",
    "rouge_L_arr  = []\n",
    "rouge_be_arr = []\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "from xml.dom import minidom\n",
    "from functools import reduce\n",
    "\n",
    "def prettify(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \")\n",
    "  \n",
    "from xml.etree.ElementTree import Element, SubElement, Comment\n",
    "\n",
    "top = Element('ZakSum')\n",
    "\n",
    "comment = Comment('Generated by Amr Zaki')\n",
    "top.append(comment)\n",
    "\n",
    "i=0\n",
    "for summ in summary_array:\n",
    "    example = SubElement(top, 'example')\n",
    "    article_element   = SubElement(example, 'article')\n",
    "    article_element.text = article[i]\n",
    "\n",
    "    reference_element = SubElement(example, 'reference')\n",
    "    reference_element.text = reference[i]\n",
    "\n",
    "    summary_element   = SubElement(example, 'summary')\n",
    "    summary_element.text = summ\n",
    "\n",
    "    rouge_1, rouge_2,rouge_L,rouge_be,bleu_score = eval_rouges(reference[i],summ )\n",
    "\n",
    "    eval_element = SubElement(example, 'eval')\n",
    "    bleu_score_element = SubElement(eval_element,'BLEU', {'score':str(bleu_score)})\n",
    "    ROUGE_1_element  = SubElement(eval_element, 'ROUGE_1' , {'score':str(rouge_1)})\n",
    "    ROUGE_2_element  = SubElement(eval_element, 'ROUGE_2' , {'score':str(rouge_2)})\n",
    "    ROUGE_L_element  = SubElement(eval_element, 'ROUGE_l' , {'score':str(rouge_L)})\n",
    "    ROUGE_be_element  = SubElement(eval_element,'ROUGE_be', {'score':str(rouge_be)})\n",
    "\n",
    "    bleu_arr.append(bleu_score) \n",
    "    rouge_1_arr.append(rouge_1) \n",
    "    rouge_2_arr.append(rouge_2) \n",
    "    rouge_L_arr.append(rouge_L) \n",
    "    rouge_be_arr.append(rouge_be) \n",
    "\n",
    "    i+=1\n",
    "\n",
    "top.set('bleu', str(reduce(lambda x, y: x + y,  bleu_arr) / len(bleu_arr)))\n",
    "top.set('rouge_1', str(reduce(lambda x, y: x + y,  rouge_1_arr) / len(rouge_1_arr)))\n",
    "top.set('rouge_2', str(reduce(lambda x, y: x + y,  rouge_2_arr) / len(rouge_2_arr)))\n",
    "top.set('rouge_L', str(reduce(lambda x, y: x + y,  rouge_L_arr) / len(rouge_L_arr)))\n",
    "top.set('rouge_be', str(reduce(lambda x, y: x + y, rouge_be_arr) / len(rouge_be_arr)))\n",
    "\n",
    "with open(default_path + \"result_featurerich_15_11_2018_5_28pm.xml\", \"w+\") as f:\n",
    "    print(prettify(top), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ihl49sb7bIRv",
    "outputId": "40e1f2f6-fe6b-4c23-a141-824eb83f79c2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-020fd579f37a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_array' is not defined"
     ]
    }
   ],
   "source": [
    "len(summary_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FvnBdHIpb-wo"
   },
   "outputs": [],
   "source": [
    "sentence = \"The Lion Air flight JT 610 had been carrying 189 people, including three children, when it disappeared from radar during a short flight from Jakarta to Pangkal Pinang on the Indonesian island of Bangka, according to Basarnas, Indonesia's national search and rescue agency.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text-Summarization-2-features-paper(tf-idf , pos tags)",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
