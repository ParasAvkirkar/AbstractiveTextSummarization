{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-processed Dataset\n",
    "\n",
    "The Data is preprocessed in [Data_Pre-Processing.ipynb](https://github.com/JRC1995/Abstractive-Summarization/blob/master/Data_Pre-Processing.ipynb)\n",
    "\n",
    "Dataset source: https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('Processed_Data/WikiHowAll_Processed.json') as file:\n",
    "\n",
    "    for json_data in file:\n",
    "        saved_data = json.loads(json_data)\n",
    "\n",
    "        vocab2idx = saved_data[\"vocab\"]\n",
    "        embd = saved_data[\"embd\"]\n",
    "        train_batches_text = saved_data[\"train_batches_text\"]\n",
    "        test_batches_text = saved_data[\"test_batches_text\"]\n",
    "        val_batches_text = saved_data[\"val_batches_text\"]\n",
    "        train_batches_summary = saved_data[\"train_batches_summary\"]\n",
    "        test_batches_summary = saved_data[\"test_batches_summary\"]\n",
    "        val_batches_summary = saved_data[\"val_batches_summary\"]\n",
    "        train_batches_true_text_len = saved_data[\"train_batches_true_text_len\"]\n",
    "        val_batches_true_text_len = saved_data[\"val_batches_true_text_len\"]\n",
    "        test_batches_true_text_len = saved_data[\"test_batches_true_text_len\"]\n",
    "        train_batches_true_summary_len = saved_data[\"train_batches_true_summary_len\"]\n",
    "        val_batches_true_summary_len = saved_data[\"val_batches_true_summary_len\"]\n",
    "        test_batches_true_summary_len = saved_data[\"test_batches_true_summary_len\"]\n",
    "\n",
    "        break\n",
    "        \n",
    "idx2vocab = {v:k for k,v in vocab2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "max_summary_len = 301 # should be summary_max_len as used in data_preprocessing with +1 (+1 for <EOS>) \n",
    "D = 5 # D determines local attention window size\n",
    "window_len = 2*D+1\n",
    "l2=1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "embd_dim = len(embd[0])\n",
    "\n",
    "tf_text = tf.placeholder(tf.int32, [None, None])\n",
    "tf_embd = tf.placeholder(tf.float32, [len(vocab2idx),embd_dim])\n",
    "tf_true_summary_len = tf.placeholder(tf.int32, [None])\n",
    "tf_summary = tf.placeholder(tf.int32,[None, None])\n",
    "tf_train = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed vectorized text\n",
    "\n",
    "Dropout used for regularization \n",
    "(https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py:132: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-4e134df0cdbc>:2: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "embd_text = tf.nn.embedding_lookup(tf_embd, tf_text)\n",
    "embd_text = tf.layers.dropout(embd_text,rate=0.3,training=tf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM function\n",
    "\n",
    "More info: \n",
    "<br>\n",
    "https://dl.acm.org/citation.cfm?id=1246450, \n",
    "<br>\n",
    "https://www.bioinf.jku.at/publications/older/2604.pdf,\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Long_short-term_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(x,hidden_state,cell,input_dim,hidden_size,scope):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        w = tf.get_variable(\"w\", shape=[4,input_dim,hidden_size],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        u = tf.get_variable(\"u\", shape=[4,hidden_size,hidden_size],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        b = tf.get_variable(\"bias\", shape=[4,1,hidden_size],\n",
    "                    dtype=tf.float32,\n",
    "                    trainable=True,\n",
    "                    initializer=tf.zeros_initializer())\n",
    "        \n",
    "    input_gate = tf.nn.sigmoid( tf.matmul(x,w[0]) + tf.matmul(hidden_state,u[0]) + b[0])\n",
    "    forget_gate = tf.nn.sigmoid( tf.matmul(x,w[1]) + tf.matmul(hidden_state,u[1]) + b[1])\n",
    "    output_gate = tf.nn.sigmoid( tf.matmul(x,w[2]) + tf.matmul(hidden_state,u[2]) + b[2])\n",
    "    cell_ = tf.nn.tanh( tf.matmul(x,w[3]) + tf.matmul(hidden_state,u[3]) + b[3])\n",
    "    cell = forget_gate*cell + input_gate*cell_\n",
    "    hidden_state = output_gate*tf.tanh(cell)\n",
    "    \n",
    "    return hidden_state, cell\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Directional LSTM Encoder\n",
    "\n",
    "(https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf)\n",
    "\n",
    "More Info: https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n",
    "\n",
    "Bi-directional LSTM encoder has a forward encoder and a backward encoder. The forward encoder encodes a text sequence from start to end, and the backward encoder encodes the text sequence from end to start.\n",
    "The final output is a combination (in this case, a concatenation) of the forward encoded text and the backward encoded text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = tf.shape(embd_text)[1] #text sequence length\n",
    "N = tf.shape(embd_text)[0] #batch_size\n",
    "\n",
    "i=0\n",
    "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "hidden_forward=tf.TensorArray(size=S, dtype=tf.float32)\n",
    "\n",
    "#shape of embd_text: [N,S,embd_dim]\n",
    "embd_text_t = tf.transpose(embd_text,[1,0,2]) \n",
    "#current shape of embd_text: [S,N,embd_dim]\n",
    "\n",
    "def cond(i, hidden, cell, hidden_forward):\n",
    "    return i < S\n",
    "\n",
    "def body(i, hidden, cell, hidden_forward):\n",
    "    x = embd_text_t[i]\n",
    "    \n",
    "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"forward_encoder\")\n",
    "    hidden_forward = hidden_forward.write(i, hidden)\n",
    "\n",
    "    return i+1, hidden, cell, hidden_forward\n",
    "\n",
    "_, _, _, hidden_forward = tf.while_loop(cond, body, [i, hidden, cell, hidden_forward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=S-1\n",
    "hidden=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "cell=tf.zeros([N, hidden_size], dtype=tf.float32)\n",
    "hidden_backward=tf.TensorArray(size=S, dtype=tf.float32)\n",
    "\n",
    "def cond(i, hidden, cell, hidden_backward):\n",
    "    return i >= 0\n",
    "\n",
    "def body(i, hidden, cell, hidden_backward):\n",
    "    x = embd_text_t[i]\n",
    "    hidden,cell = LSTM(x,hidden,cell,embd_dim,hidden_size,scope=\"backward_encoder\")\n",
    "    hidden_backward = hidden_backward.write(i, hidden)\n",
    "\n",
    "    return i-1, hidden, cell, hidden_backward\n",
    "\n",
    "_, _, _, hidden_backward = tf.while_loop(cond, body, [i, hidden, cell, hidden_backward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Forward and Backward Encoder Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_forward = hidden_forward.stack()\n",
    "hidden_backward = hidden_backward.stack()\n",
    "\n",
    "encoder_states = tf.concat([hidden_forward,hidden_backward],axis=-1)\n",
    "encoder_states = tf.transpose(encoder_states,[1,0,2])\n",
    "\n",
    "encoder_states = tf.layers.dropout(encoder_states,rate=0.3,training=tf_train)\n",
    "\n",
    "final_encoded_state = tf.layers.dropout(tf.concat([hidden_forward[-1],hidden_backward[-1]],axis=-1),rate=0.3,training=tf_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of attention scoring function\n",
    "\n",
    "Given a sequence of encoder states ($H_s$) and the decoder hidden state ($H_t$) of current timestep $t$, the equation for computing attention score is:\n",
    "\n",
    "$$Score = (H_s.W_a).H_t^T $$\n",
    "\n",
    "($W_a$ = trainable parameters)\n",
    "\n",
    "(https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_score(encoder_states,decoder_hidden_state,scope=\"attention_score\"):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        Wa = tf.get_variable(\"Wa\", shape=[2*hidden_size,2*hidden_size],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "    encoder_states = tf.reshape(encoder_states,[N*S,2*hidden_size])\n",
    "    \n",
    "    encoder_states = tf.reshape(tf.matmul(encoder_states,Wa),[N,S,2*hidden_size])\n",
    "    decoder_hidden_state = tf.reshape(decoder_hidden_state,[N,2*hidden_size,1])\n",
    "    \n",
    "    return tf.reshape(tf.matmul(encoder_states,decoder_hidden_state),[N,S])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Attention Function\n",
    "\n",
    "Based on: https://nlp.stanford.edu/pubs/emnlp15_attn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align(encoder_states, decoder_hidden_state,scope=\"attention\"):\n",
    "    \n",
    "    with tf.variable_scope(scope,reuse=tf.AUTO_REUSE):\n",
    "        Wp = tf.get_variable(\"Wp\", shape=[2*hidden_size,125],\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=True,\n",
    "                                    initializer=tf.glorot_uniform_initializer())\n",
    "        \n",
    "        Vp = tf.get_variable(\"Vp\", shape=[125,1],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "    positions = tf.cast(S-window_len,dtype=tf.float32) # Maximum valid attention window starting position\n",
    "    \n",
    "    # Predict attention window starting position \n",
    "    ps = positions*tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(decoder_hidden_state,Wp)),Vp))\n",
    "    # ps = (soft-)predicted starting position of attention window\n",
    "    pt = ps+D # pt = center of attention window where the whole window length is 2*D+1\n",
    "    pt = tf.reshape(pt,[N])\n",
    "    \n",
    "    i = 0\n",
    "    gaussian_position_based_scores = tf.TensorArray(size=S,dtype=tf.float32)\n",
    "    sigma = tf.constant(D/2,dtype=tf.float32)\n",
    "    \n",
    "    def cond(i,gaussian_position_based_scores):\n",
    "        \n",
    "        return i < S\n",
    "                      \n",
    "    def body(i,gaussian_position_based_scores):\n",
    "        \n",
    "        score = tf.exp(-((tf.square(tf.cast(i,tf.float32)-pt))/(2*tf.square(sigma)))) \n",
    "        # (equation (10) in https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)\n",
    "        gaussian_position_based_scores = gaussian_position_based_scores.write(i,score)\n",
    "            \n",
    "        return i+1,gaussian_position_based_scores\n",
    "                      \n",
    "    i,gaussian_position_based_scores = tf.while_loop(cond,body,[i,gaussian_position_based_scores])\n",
    "    \n",
    "    gaussian_position_based_scores = gaussian_position_based_scores.stack()\n",
    "    gaussian_position_based_scores = tf.transpose(gaussian_position_based_scores,[1,0])\n",
    "    gaussian_position_based_scores = tf.reshape(gaussian_position_based_scores,[N,S])\n",
    "    \n",
    "    scores = attention_score(encoder_states,decoder_hidden_state)*gaussian_position_based_scores\n",
    "    scores = tf.nn.softmax(scores,axis=-1)\n",
    "    \n",
    "    return tf.reshape(scores,[N,S,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Decoder With Local Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
    "    SOS = tf.get_variable(\"sos\", shape=[1,embd_dim],\n",
    "                                dtype=tf.float32,\n",
    "                                trainable=True,\n",
    "                                initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "    # SOS represents starting marker \n",
    "    # It tells the decoder that it is about to decode the first word of the output\n",
    "    # I have set SOS as a trainable parameter\n",
    "    \n",
    "    Wc = tf.get_variable(\"Wc\", shape=[4*hidden_size,embd_dim],\n",
    "                            dtype=tf.float32,\n",
    "                            trainable=True,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "    \n",
    "\n",
    "\n",
    "SOS = tf.tile(SOS,[N,1]) #now SOS shape: [N,embd_dim]\n",
    "inp = SOS\n",
    "hidden=final_encoded_state\n",
    "cell=tf.zeros([N, 2*hidden_size], dtype=tf.float32)\n",
    "decoder_outputs=tf.TensorArray(size=max_summary_len, dtype=tf.float32)\n",
    "outputs=tf.TensorArray(size=max_summary_len, dtype=tf.int32)\n",
    "\n",
    "for i in range(max_summary_len):\n",
    "    \n",
    "    inp = tf.layers.dropout(inp,rate=0.3,training=tf_train)\n",
    "    \n",
    "    attention_scores = align(encoder_states,hidden)\n",
    "    encoder_context_vector = tf.reduce_sum(encoder_states*attention_scores,axis=1)\n",
    "    \n",
    "    hidden,cell = LSTM(inp,hidden,cell,embd_dim,2*hidden_size,scope=\"decoder\")\n",
    "    \n",
    "    hidden_ = tf.layers.dropout(hidden,rate=0.3,training=tf_train)\n",
    "    \n",
    "    concated = tf.concat([hidden_,encoder_context_vector],axis=-1)\n",
    "    \n",
    "    linear_out = tf.nn.tanh(tf.matmul(concated,Wc))\n",
    "    decoder_output = tf.matmul(linear_out,tf.transpose(tf_embd,[1,0])) \n",
    "    # produce unnormalized probability distribution over vocabulary\n",
    "    \n",
    "    \n",
    "    decoder_outputs = decoder_outputs.write(i,decoder_output)\n",
    "    \n",
    "    # Pick out most probable vocab indices based on the unnormalized probability distribution\n",
    "    \n",
    "    next_word_vec = tf.cast(tf.argmax(decoder_output,1),tf.int32)\n",
    "\n",
    "    next_word_vec = tf.reshape(next_word_vec, [N])\n",
    "\n",
    "    outputs = outputs.write(i,next_word_vec)\n",
    "\n",
    "    next_word = tf.nn.embedding_lookup(tf_embd, next_word_vec)\n",
    "    inp = tf.reshape(next_word, [N, embd_dim])\n",
    "    \n",
    "    \n",
    "decoder_outputs = decoder_outputs.stack()\n",
    "outputs = outputs.stack()\n",
    "\n",
    "decoder_outputs = tf.transpose(decoder_outputs,[1,0,2])\n",
    "outputs = tf.transpose(outputs,[1,0])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Cross Entropy Cost Function and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_trainables = [var for var in tf.trainable_variables() if\n",
    "                       not(\"Bias\" in var.name or \"bias\" in var.name\n",
    "                           or \"noreg\" in var.name)]\n",
    "\n",
    "regularization = tf.reduce_sum([tf.nn.l2_loss(var) for var\n",
    "                                in filtered_trainables])\n",
    "\n",
    "with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    epsilon = tf.constant(1e-9, tf.float32)\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=tf_summary, logits=decoder_outputs)\n",
    "\n",
    "    pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
    "                                maxlen=max_summary_len,\n",
    "                                dtype=tf.float32)\n",
    "\n",
    "    masked_cross_entropy = cross_entropy*pad_mask\n",
    "\n",
    "    cost = tf.reduce_mean(masked_cross_entropy) + \\\n",
    "        l2*regularization\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(masked_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing predicted sequence with labels\n",
    "comparison = tf.cast(tf.equal(outputs, tf_summary),\n",
    "                     tf.float32)\n",
    "\n",
    "# Masking to ignore the effect of pads while calculating accuracy\n",
    "pad_mask = tf.sequence_mask(tf_true_summary_len,\n",
    "                            maxlen=max_summary_len,\n",
    "                            dtype=tf.bool)\n",
    "\n",
    "masked_comparison = tf.boolean_mask(comparison, pad_mask)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = tf.reduce_mean(masked_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/sbu_nlp2019/anaconda3/envs/nlp/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "optimizer = tf.contrib.opt.NadamOptimizer(\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "gvs = optimizer.compute_gradients(cost, var_list=all_vars)\n",
    "\n",
    "capped_gvs = [(tf.clip_by_norm(grad, 5), var) for grad, var in gvs] # Gradient Clipping\n",
    "\n",
    "train_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Load checkpoint? y/n:  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "STARTING TRAINING\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "with tf.Session() as sess:  # Start Tensorflow Session\n",
    "    display_step = 100\n",
    "    patience = 5\n",
    "\n",
    "    load = input(\"\\nLoad checkpoint? y/n: \")\n",
    "    print(\"\")\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    if load.lower() == 'y':\n",
    "\n",
    "        print('Loading pre-trained weights for the model...')\n",
    "\n",
    "        saver.restore(sess, 'Model_Backup/Seq2seq_summarization.ckpt')\n",
    "        sess.run(tf.global_variables())\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "        with open('Model_Backup/Seq2seq_summarization.pkl', 'rb') as fp:\n",
    "            train_data = pickle.load(fp)\n",
    "\n",
    "        covered_epochs = train_data['covered_epochs']\n",
    "        best_loss = train_data['best_loss']\n",
    "        impatience = 0\n",
    "        \n",
    "        print('\\nRESTORATION COMPLETE\\n')\n",
    "\n",
    "    else:\n",
    "        best_loss = 2**30\n",
    "        impatience = 0\n",
    "        covered_epochs = 0\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        sess.run(tf.tables_initializer())\n",
    "\n",
    "    epoch=0\n",
    "    while (epoch+covered_epochs)<epochs:\n",
    "        \n",
    "        print(\"\\n\\nSTARTING TRAINING\\n\\n\")\n",
    "        \n",
    "        batches_indices = [i for i in range(0, len(train_batches_text))]\n",
    "        random.shuffle(batches_indices)\n",
    "\n",
    "        total_train_acc = 0\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i in range(0, len(train_batches_text)):\n",
    "            \n",
    "            j = int(batches_indices[i])\n",
    "\n",
    "            cost,prediction,\\\n",
    "                acc, _ = sess.run([cross_entropy,\n",
    "                                   outputs,\n",
    "                                   accuracy,\n",
    "                                   train_op],\n",
    "                                  feed_dict={tf_text: train_batches_text[j],\n",
    "                                             tf_embd: embd,\n",
    "                                             tf_summary: train_batches_summary[j],\n",
    "                                             tf_true_summary_len: train_batches_true_summary_len[j],\n",
    "                                             tf_train: True})\n",
    "            \n",
    "            total_train_acc += acc\n",
    "            total_train_loss += cost\n",
    "\n",
    "            if i % display_step == 0:\n",
    "                print(\"Iter \"+str(i)+\", Cost= \" +\n",
    "                      \"{:.3f}\".format(cost)+\", Acc = \" +\n",
    "                      \"{:.2f}%\".format(acc*100))\n",
    "            \n",
    "            if i % 500 == 0:\n",
    "                \n",
    "                idx = random.randint(0,len(train_batches_text[j])-1)\n",
    "                \n",
    "                \n",
    "                \n",
    "                text = \" \".join([idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_text[j][idx]])\n",
    "                predicted_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in prediction[idx]]\n",
    "                actual_summary = [idx2vocab.get(vec,\"<UNK>\") for vec in train_batches_summary[j][idx]]\n",
    "                \n",
    "                print(\"\\nSample Text\\n\")\n",
    "                print(text)\n",
    "                print(\"\\nSample Predicted Summary\\n\")\n",
    "                for word in predicted_summary:\n",
    "                    if word == '<EOS>':\n",
    "                        break\n",
    "                    else:\n",
    "                        print(word,end=\" \")\n",
    "                print(\"\\n\\nSample Actual Summary\\n\")\n",
    "                for word in actual_summary:\n",
    "                    if word == '<EOS>':\n",
    "                        break\n",
    "                    else:\n",
    "                        print(word,end=\" \")\n",
    "                print(\"\\n\\n\")\n",
    "                \n",
    "        print(\"\\n\\nSTARTING VALIDATION\\n\\n\")\n",
    "                \n",
    "        total_val_loss=0\n",
    "        total_val_acc=0\n",
    "                \n",
    "        for i in range(0, len(val_batches_text)):\n",
    "            \n",
    "            if i%100==0:\n",
    "                print(\"Validating data # {}\".format(i))\n",
    "\n",
    "            cost, prediction,\\\n",
    "                acc = sess.run([cross_entropy,\n",
    "                                outputs,\n",
    "                                accuracy],\n",
    "                                  feed_dict={tf_text: val_batches_text[i],\n",
    "                                             tf_embd: embd,\n",
    "                                             tf_summary: val_batches_summary[i],\n",
    "                                             tf_true_summary_len: val_batches_true_summary_len[i],\n",
    "                                             tf_train: False})\n",
    "            \n",
    "            total_val_loss += cost\n",
    "            total_val_acc += acc\n",
    "            \n",
    "        avg_val_loss = total_val_loss/len(val_batches_text)\n",
    "        \n",
    "        print(\"\\n\\nEpoch: {}\\n\\n\".format(epoch+covered_epochs))\n",
    "        print(\"Average Training Loss: {:.3f}\".format(total_train_loss/len(train_batches_text)))\n",
    "        print(\"Average Training Accuracy: {:.2f}\".format(100*total_train_acc/len(train_batches_text)))\n",
    "        print(\"Average Validation Loss: {:.3f}\".format(avg_val_loss))\n",
    "        print(\"Average Validation Accuracy: {:.2f}\".format(100*total_val_acc/len(val_batches_text)))\n",
    "              \n",
    "        if (avg_val_loss < best_loss):\n",
    "            best_loss = avg_val_loss\n",
    "            save_data={'best_loss':best_loss,'covered_epochs':covered_epochs+epoch+1}\n",
    "            impatience=0\n",
    "            with open('Model_Backup/Seq2seq_summarization.pkl', 'wb') as fp:\n",
    "                pickle.dump(save_data, fp)\n",
    "            saver.save(sess, 'Model_Backup/Seq2seq_summarization.ckpt')\n",
    "            print(\"\\nModel saved\\n\")\n",
    "              \n",
    "        else:\n",
    "            impatience+=1\n",
    "              \n",
    "        if impatience > patience:\n",
    "              break\n",
    "              \n",
    "              \n",
    "        epoch+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Works\n",
    "\n",
    "* Beam Search\n",
    "* Pointer Mechanisms\n",
    "* BLEU\\ROUGE evaluation\n",
    "* Implement Testing\n",
    "* Complete Training and Optimize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
