{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.50d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadGloVe(filename)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "word_vec_dim = len(embedding[0])\n",
    "#Pre-trained GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_nearest_neighbour(x):\n",
    "    #returns array in embedding that's most similar (in terms of cosine similarity) to x\n",
    "        \n",
    "    xdoty = np.multiply(embedding,x)\n",
    "    xdoty = np.sum(xdoty,1)\n",
    "    xlen = np.square(x)\n",
    "    xlen = np.sum(xlen,0)\n",
    "    xlen = np.sqrt(xlen)\n",
    "    ylen = np.square(embedding)\n",
    "    ylen = np.sum(ylen,1)\n",
    "    ylen = np.sqrt(ylen)\n",
    "    xlenylen = np.multiply(xlen,ylen)\n",
    "    cosine_similarities = np.divide(xdoty,xlenylen)\n",
    "\n",
    "    return embedding[np.argmax(cosine_similarities)]\n",
    "\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('unk')]\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    for x in xrange(0, len(embedding)):\n",
    "        if np.array_equal(embedding[x],np.asarray(vec)):\n",
    "            return vocab[x]\n",
    "    return vec2word(np_nearest_neighbour(np.asarray(vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('vec_summaries', 'rb') as fp:\n",
    "    vec_summaries = pickle.load(fp)\n",
    "\n",
    "with open ('vec_texts', 'rb') as fp:\n",
    "    vec_texts = pickle.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('vocab_limit', 'rb') as fp:\n",
    "    vocab_limit = pickle.load(fp)\n",
    "\n",
    "with open ('embd_limit', 'rb') as fp:\n",
    "    embd_limit = pickle.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_limit.append('<SOS>')\n",
    "\n",
    "SOS_prob_dist = np.zeros((len(vocab_limit)),dtype=np.float32)\n",
    "SOS_prob_dist[vocab_limit.index('<SOS>')]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of dataset with summary length beyond 7: 16.146% \n",
      "Percentage of dataset with text length less that window size: 2.258% \n",
      "Percentage of dataset with text length more than 80: 40.412% \n"
     ]
    }
   ],
   "source": [
    "#DIAGNOSIS\n",
    "\n",
    "count = 0\n",
    "\n",
    "LEN = 7\n",
    "\n",
    "for summary in vec_summaries:\n",
    "    if len(summary)-1>LEN:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with summary length beyond \"+str(LEN)+\": \"+str((count/len(vec_summaries))*100)+\"% \"\n",
    "\n",
    "count = 0\n",
    "\n",
    "D = 10 \n",
    "\n",
    "window_size = 2*D+1\n",
    "\n",
    "for text in vec_texts:\n",
    "    if len(text)<window_size+1:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with text length less that window size: \"+str((count/len(vec_texts))*100)+\"% \"\n",
    "\n",
    "count = 0\n",
    "\n",
    "LEN = 80\n",
    "\n",
    "for text in vec_texts:\n",
    "    if len(text)>LEN:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with text length more than \"+str(LEN)+\": \"+str((count/len(vec_texts))*100)+\"% \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SUMMARY_LEN = 7\n",
    "MAX_TEXT_LEN = 80\n",
    "\n",
    "#D is a major hyperparameters. Windows size for local attention will be 2*D+1\n",
    "D = 10\n",
    "\n",
    "window_size = 2*D+1\n",
    "\n",
    "#REMOVE DATA WHOSE SUMMARIES ARE TOO BIG\n",
    "#OR WHOSE TEXT LENGTH IS TOO BIG\n",
    "#OR WHOSE TEXT LENGTH IS SMALLED THAN WINDOW SIZE\n",
    "\n",
    "vec_summaries_reduced = []\n",
    "vec_texts_reduced = []\n",
    "\n",
    "i = 0\n",
    "for summary in vec_summaries:\n",
    "    if len(summary)-1<=MAX_SUMMARY_LEN and len(vec_texts[i])>=window_size and len(vec_texts[i])<=MAX_TEXT_LEN:\n",
    "        vec_summaries_reduced.append(summary)\n",
    "        vec_texts_reduced.append(vec_texts[i])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int((.7)*len(vec_summaries_reduced))\n",
    "\n",
    "train_texts = vec_texts_reduced[0:train_len]\n",
    "train_summaries = vec_summaries_reduced[0:train_len]\n",
    "\n",
    "val_len = int((.15)*len(vec_summaries_reduced))\n",
    "\n",
    "val_texts = vec_texts_reduced[train_len:train_len+val_len]\n",
    "val_summaries = vec_summaries_reduced[train_len:train_len+val_len]\n",
    "\n",
    "test_texts = vec_texts_reduced[train_len+val_len:len(vec_summaries_reduced)]\n",
    "test_summaries = vec_summaries_reduced[train_len+val_len:len(vec_summaries_reduced)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18293\n"
     ]
    }
   ],
   "source": [
    "print train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_out(output_text):\n",
    "    output_len = len(output_text)\n",
    "    transformed_output = np.zeros([output_len],dtype=np.int32)\n",
    "    for i in xrange(0,output_len):\n",
    "        transformed_output[i] = vocab_limit.index(vec2word(output_text[i]))\n",
    "    return transformed_output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some MORE hyperparameters and other stuffs\n",
    "\n",
    "hidden_size = 250\n",
    "learning_rate = 0.001\n",
    "K = 5\n",
    "vocab_len = len(vocab_limit)\n",
    "training_iters = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#placeholders\n",
    "tf_text = tf.placeholder(tf.float32, [None,word_vec_dim])\n",
    "tf_seq_len = tf.placeholder(tf.int32)\n",
    "tf_summary = tf.placeholder(tf.int32,[None])\n",
    "tf_output_len = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_encoder(inp,hidden,cell,\n",
    "                    wf,uf,bf,\n",
    "                    wi,ui,bi,\n",
    "                    wo,uo,bo,\n",
    "                    wc,uc,bc,\n",
    "                    Wattention,seq_len,inp_dim):\n",
    "\n",
    "    Wattention = tf.nn.softmax(Wattention,0)\n",
    "    hidden_forward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    i=0\n",
    "    j=K\n",
    "    \n",
    "    def cond(i,j,hidden,cell,hidden_forward,hidden_residuals):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,j,hidden,cell,hidden_forward,hidden_residuals):\n",
    "        \n",
    "        x = tf.reshape(inp[i],[1,inp_dim])\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n",
    "        RRA = tf.reshape(RRA,[1,hidden_size])\n",
    "        \n",
    "        # LSTM with RRA\n",
    "        fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n",
    "        ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n",
    "        og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n",
    "        cell = tf.multiply(fg,cell) + tf.multiply(ig,tf.sigmoid( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n",
    "        hidden = tf.multiply(og,tf.tanh(cell+RRA))\n",
    "        \n",
    "        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n",
    "                                   lambda: hidden_residuals,\n",
    "                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n",
    "\n",
    "        hidden_forward = hidden_forward.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "        \n",
    "        return i+1,j+1,hidden,cell,hidden_forward,hidden_residuals\n",
    "    \n",
    "    _,_,_,_,hidden_forward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,cell,hidden_forward,hidden_residuals])\n",
    "    \n",
    "    hidden_residuals.close().mark_used()\n",
    "    \n",
    "    return hidden_forward.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_encoder(inp,hidden,cell,\n",
    "                     wf,uf,bf,\n",
    "                     wi,ui,bi,\n",
    "                     wo,uo,bo,\n",
    "                     wc,uc,bc,\n",
    "                     Wattention,seq_len,inp_dim):\n",
    "    \n",
    "    Wattention = tf.nn.softmax(Wattention,0)\n",
    "    hidden_backward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    i=seq_len-1\n",
    "    j=K\n",
    "    \n",
    "    def cond(i,j,hidden,cell,hidden_backward,hidden_residuals):\n",
    "        return i > -1\n",
    "    \n",
    "    def body(i,j,hidden,cell,hidden_backward,hidden_residuals):\n",
    "        \n",
    "        x = tf.reshape(inp[i],[1,inp_dim])\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n",
    "        RRA = tf.reshape(RRA,[1,hidden_size])\n",
    "        \n",
    "        # LSTM with RRA\n",
    "        fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n",
    "        ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n",
    "        og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n",
    "        cell = tf.multiply(fg,cell) + tf.multiply(ig,tf.sigmoid( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n",
    "        hidden = tf.multiply(og,tf.tanh(cell+RRA))\n",
    "\n",
    "        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n",
    "                                   lambda: hidden_residuals,\n",
    "                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n",
    "        \n",
    "        hidden_backward = hidden_backward.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "        \n",
    "        return i-1,j+1,hidden,cell,hidden_backward,hidden_residuals\n",
    "    \n",
    "    _,_,_,_,hidden_backward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,cell,hidden_backward,hidden_residuals])\n",
    "\n",
    "    hidden_residuals.close().mark_used()\n",
    "    \n",
    "    return hidden_backward.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x,hidden,cell,\n",
    "            wf,uf,bf,\n",
    "            wi,ui,bi,\n",
    "            wo,uo,bo,\n",
    "            wc,uc,bc,RRA):\n",
    "    \n",
    "    # LSTM with RRA\n",
    "    fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n",
    "    ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n",
    "    og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n",
    "    cell_next = tf.multiply(fg,cell) + tf.multiply(ig,tf.sigmoid( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n",
    "    hidden_next = tf.multiply(og,tf.tanh(cell+RRA))\n",
    "    \n",
    "    return hidden_next,cell_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(hs,ht,Wa,seq_len):\n",
    "    return tf.reshape(tf.matmul(tf.matmul(hs,Wa),tf.transpose(ht)),[seq_len])\n",
    "\n",
    "def align(hs,ht,Wp,Vp,Wa,tf_seq_len):\n",
    "   \n",
    "    pd = tf.TensorArray(size=(2*D+1),dtype=tf.float32)\n",
    "    \n",
    "    positions = tf.cast(tf_seq_len-1-2*D,dtype=tf.float32)\n",
    "    \n",
    "    sigmoid_multiplier = tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(ht,Wp)),Vp))\n",
    "    sigmoid_multiplier = tf.reshape(sigmoid_multiplier,[])\n",
    "    \n",
    "    pt_float = positions*sigmoid_multiplier\n",
    "    \n",
    "    pt = tf.cast(pt_float,tf.int32)\n",
    "    pt = pt+D #center to window\n",
    "    \n",
    "    sigma = tf.constant(D/2,dtype=tf.float32)\n",
    "    \n",
    "    i = 0\n",
    "    pos = pt - D\n",
    "    \n",
    "    def cond(i,pos,pd):\n",
    "        \n",
    "        return i < (2*D+1)\n",
    "                      \n",
    "    def body(i,pos,pd):\n",
    "        \n",
    "        comp_1 = tf.cast(tf.square(pos-pt),tf.float32)\n",
    "        comp_2 = tf.cast(2*tf.square(sigma),tf.float32)\n",
    "            \n",
    "        pd = pd.write(i,tf.exp(-(comp_1/comp_2)))\n",
    "            \n",
    "        return i+1,pos+1,pd\n",
    "                      \n",
    "    i,pos,pd = tf.while_loop(cond,body,[i,pos,pd])\n",
    "    \n",
    "    local_hs = hs[(pt-D):(pt+D+1)]\n",
    "    \n",
    "    normalized_scores = tf.nn.softmax(score(local_hs,ht,Wa,2*D+1))\n",
    "    \n",
    "    pd=pd.stack()\n",
    "    \n",
    "    G = tf.multiply(normalized_scores,pd)\n",
    "    G = tf.reshape(G,[2*D+1,1])\n",
    "    \n",
    "    return G,pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(tf_text,tf_seq_len,tf_output_len):\n",
    "    \n",
    "    #PARAMETERS\n",
    "    \n",
    "    #1.1 FORWARD ENCODER PARAMETERS\n",
    "    \n",
    "    initial_hidden_f = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    cell_f = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    wf_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uf_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bf_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wi_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    ui_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bi_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wo_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uo_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bo_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wc_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uc_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bc_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    Wattention_f = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "                               \n",
    "    #1.2 BACKWARD ENCODER PARAMETERS\n",
    "    \n",
    "    initial_hidden_b = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    cell_b = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    wf_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uf_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bf_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wi_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    ui_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bi_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wo_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uo_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bo_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wc_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uc_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bc_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    Wattention_b = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "    \n",
    "    #2 ATTENTION PARAMETERS\n",
    "    \n",
    "    Wp = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,50],stddev=0.01))\n",
    "    Vp = tf.Variable(tf.truncated_normal(shape=[50,1],stddev=0.01))\n",
    "    Wa = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,2*hidden_size],stddev=0.01))\n",
    "    Wc = tf.Variable(tf.truncated_normal(shape=[4*hidden_size,2*hidden_size],stddev=0.01))\n",
    "    \n",
    "    #3 DECODER PARAMETERS\n",
    "    \n",
    "    Ws = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,vocab_len],stddev=0.01))\n",
    "    \n",
    "    cell_d = tf.zeros([1,2*hidden_size],dtype=tf.float32)\n",
    "    wf_d = tf.Variable(tf.truncated_normal(shape=[vocab_len,2*hidden_size],stddev=0.01))\n",
    "    uf_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bf_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    wi_d = tf.Variable(tf.truncated_normal(shape=[vocab_len,2*hidden_size],stddev=0.01))\n",
    "    ui_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bi_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    wo_d = tf.Variable(tf.truncated_normal(shape=[vocab_len,2*hidden_size],stddev=0.01))\n",
    "    uo_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bo_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    wc_d = tf.Variable(tf.truncated_normal(shape=[vocab_len,2*hidden_size],stddev=0.01))\n",
    "    uc_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bc_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals_d = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals_d = hidden_residuals_d.unstack(tf.zeros([K,2*hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    Wattention_d = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "    \n",
    "    output = tf.TensorArray(size=tf_output_len,dtype=tf.float32)\n",
    "                               \n",
    "    #BI-DIRECTIONAL LSTM\n",
    "                               \n",
    "    hidden_forward = forward_encoder(tf_text,\n",
    "                                     initial_hidden_f,cell_f,\n",
    "                                     wf_f,uf_f,bf_f,\n",
    "                                     wi_f,ui_f,bi_f,\n",
    "                                     wo_f,uo_f,bo_f,\n",
    "                                     wc_f,uc_f,bc_f,\n",
    "                                     Wattention_f,\n",
    "                                     tf_seq_len,\n",
    "                                     word_vec_dim)\n",
    "    \n",
    "    hidden_backward = backward_encoder(tf_text,\n",
    "                                     initial_hidden_b,cell_b,\n",
    "                                     wf_b,uf_b,bf_b,\n",
    "                                     wi_b,ui_b,bi_b,\n",
    "                                     wo_b,uo_b,bo_b,\n",
    "                                     wc_b,uc_b,bc_b,\n",
    "                                     Wattention_b,\n",
    "                                     tf_seq_len,\n",
    "                                     word_vec_dim)\n",
    "    \n",
    "    encoded_hidden = tf.concat([hidden_forward,hidden_backward],1)\n",
    "    \n",
    "    #ATTENTION MECHANISM AND DECODER\n",
    "    \n",
    "    decoded_hidden = encoded_hidden[0]\n",
    "    decoded_hidden = tf.reshape(decoded_hidden,[1,2*hidden_size])\n",
    "    Wattention_d_normalized = tf.nn.softmax(Wattention_d)\n",
    "    \n",
    "    y = tf.convert_to_tensor(SOS_prob_dist) #inital output <SOS>\n",
    "    y = tf.reshape(y,[1,vocab_len])\n",
    "    \n",
    "    j=K\n",
    "    \n",
    "    hidden_residuals_stack = hidden_residuals_d.stack()\n",
    "    \n",
    "    RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention_d_normalized),0)\n",
    "    RRA = tf.reshape(RRA,[1,2*hidden_size])\n",
    "    \n",
    "    decoded_hidden_next,cell_d = decoder(y,decoded_hidden,cell_d,\n",
    "                                  wf_d,uf_d,bf_d,\n",
    "                                  wi_d,ui_d,bf_d,\n",
    "                                  wo_d,uo_d,bf_d,\n",
    "                                  wc_d,uc_d,bc_d,\n",
    "                                  RRA)\n",
    "    decoded_hidden = decoded_hidden_next\n",
    "    \n",
    "    hidden_residuals_d = hidden_residuals_d.write(j,tf.reshape(decoded_hidden,[2*hidden_size]))\n",
    "    \n",
    "    j=j+1\n",
    "                           \n",
    "    i=0\n",
    "    \n",
    "    def attention_decoder_cond(i,j,decoded_hidden,cell_d,hidden_residuals_d,output):\n",
    "        return i < tf_output_len\n",
    "    \n",
    "    def attention_decoder_body(i,j,decoded_hidden,cell_d,hidden_residuals_d,output):\n",
    "        \n",
    "        #LOCAL ATTENTION\n",
    "        \n",
    "        G,pt = align(encoded_hidden,decoded_hidden,Wp,Vp,Wa,tf_seq_len)\n",
    "        local_encoded_hidden = encoded_hidden[pt-D:pt+D+1]\n",
    "        weighted_encoded_hidden = tf.multiply(local_encoded_hidden,G)\n",
    "        context_vector = tf.reduce_sum(weighted_encoded_hidden,0)\n",
    "        context_vector = tf.reshape(context_vector,[1,2*hidden_size])\n",
    "        \n",
    "        attended_hidden = tf.tanh(tf.matmul(tf.concat([context_vector,decoded_hidden],1),Wc))\n",
    "        \n",
    "        #DECODER\n",
    "        \n",
    "        y = tf.matmul(attended_hidden,Ws)\n",
    "        \n",
    "        output = output.write(i,tf.reshape(y,[vocab_len]))\n",
    "        \n",
    "        y = tf.nn.softmax(y)\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals_d.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention_d_normalized),0)\n",
    "        RRA = tf.reshape(RRA,[1,2*hidden_size])\n",
    "        \n",
    "        decoded_hidden_next,cell_d = decoder(y,decoded_hidden,cell_d,\n",
    "                                  wf_d,uf_d,bf_d,\n",
    "                                  wi_d,ui_d,bf_d,\n",
    "                                  wo_d,uo_d,bf_d,\n",
    "                                  wc_d,uc_d,bc_d,\n",
    "                                  RRA)\n",
    "        \n",
    "        decoded_hidden = decoded_hidden_next\n",
    "        \n",
    "        hidden_residuals_d = tf.cond(tf.equal(j,tf_output_len-1+K+1), #(+1 for <SOS>)\n",
    "                                   lambda: hidden_residuals_d,\n",
    "                                   lambda: hidden_residuals_d.write(j,tf.reshape(decoded_hidden,[2*hidden_size])))\n",
    "        \n",
    "        return i+1,j+1,decoded_hidden,cell_d,hidden_residuals_d,output\n",
    "    \n",
    "    i,j,decoded_hidden,cell_d,hidden_residuals_d,output = tf.while_loop(attention_decoder_cond,\n",
    "                                            attention_decoder_body,\n",
    "                                            [i,j,decoded_hidden,cell_d,hidden_residuals_d,output])\n",
    "    hidden_residuals_d.close().mark_used()\n",
    "    \n",
    "    output = output.stack()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tf_text,tf_seq_len,tf_output_len)\n",
    "\n",
    "#OPTIMIZER\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=tf_summary))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#PREDICTION\n",
    "\n",
    "pred = tf.TensorArray(size=tf_output_len,dtype=tf.int32)\n",
    "\n",
    "i=0\n",
    "\n",
    "def cond_pred(i,pred):\n",
    "    return i<tf_output_len\n",
    "def body_pred(i,pred):\n",
    "    pred = pred.write(i,tf.cast(tf.argmax(output[i]),tf.int32))\n",
    "    return i+1,pred\n",
    "\n",
    "i,pred = tf.while_loop(cond_pred,body_pred,[i,pred]) \n",
    "\n",
    "prediction = pred.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 0\n",
      "Training input sequence length: 51\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "i have bought several of the vitality canned dog food products and have found them all to be of good quality. the product looks more like a stew than a processed meat and it smells better. my labrador is finicky and she appreciates this product better than most.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "shortages estimating estimating arrange\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "good quality dog food\n",
      "\n",
      "loss=10.3797\n",
      "\n",
      "Iteration: 1\n",
      "Training input sequence length: 37\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "product arrived labeled as jumbo salted peanuts ... the peanuts were actually small sized unsalted. not sure if this was an error or if the vendor intended to represent the product as `` jumbo ''.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "dog food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "not as advertised\n",
      "\n",
      "loss=10.3795\n",
      "\n",
      "Iteration: 2\n",
      "Training input sequence length: 46\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "if you are looking for the secret ingredient in robitussin i believe i have found it. i got this in addition to the root beer extract i ordered( which was good) and made some cherry soda. the flavor is very medicinal.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "quality food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "cough medicine\n",
      "\n",
      "loss=10.3667\n",
      "\n",
      "Iteration: 3\n",
      "Training input sequence length: 32\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "great taffy at a great price. there was a wide assortment of yummy taffy. delivery was very quick. if your a taffy lover, this is a deal.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "quality food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great taffy\n",
      "\n",
      "loss=10.3399\n",
      "\n",
      "Iteration: 4\n",
      "Training input sequence length: 30\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "this taffy is so good. it is very soft and chewy. the flavors are amazing. i would definitely recommend you buying it. very satisfying!!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "quality food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "wonderful, tasty taffy\n",
      "\n",
      "loss=10.2566\n",
      "\n",
      "Iteration: 5\n",
      "Training input sequence length: 29\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "right now i 'm mostly just sprouting this so my cats can eat the grass. they love it. i rotate it around with wheatgrass and rye too\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "as advertised\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "yay barley\n",
      "\n",
      "loss=10.3595\n",
      "\n",
      "Iteration: 6\n",
      "Training input sequence length: 29\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "this is a very healthy dog food. good for their digestion. also good for small puppies. my dog eats her required amount at every feeding.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "as advertised advertised\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "healthy dog food\n",
      "\n",
      "loss=8.85491\n",
      "\n",
      "Iteration: 7\n",
      "Training input sequence length: 24\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "the strawberry twizzlers are my guilty pleasure- yummy. six pounds will be around for a while with my son and i.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "cough advertised advertised food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "strawberry twizzlers- yummy\n",
      "\n",
      "loss=10.3986\n",
      "\n",
      "Iteration: 8\n",
      "Training input sequence length: 45\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "i love eating them and they are good for watching tv and looking at movies! it is not too sweet. i like to transfer them to a zip lock baggie so they stay fresh so i can take my time eating them.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "dog food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "poor taste\n",
      "\n",
      "loss=10.3397\n",
      "\n",
      "Iteration: 9\n",
      "Training input sequence length: 28\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i am very satisfied with my unk purchase. i shared these with others and we have all enjoyed them. i will definitely be ordering more.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "dog food food\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from __future__ import print_function\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 0   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_acc=0\n",
    "    display_step = 1\n",
    "    \n",
    "    while step < training_iters:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "           \n",
    "        for i in xrange(0,train_len):\n",
    "            \n",
    "            train_out = transform_out(train_summaries[i][0:len(train_summaries[i])-1])\n",
    "            \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nIteration: \"+str(i))\n",
    "                print(\"Training input sequence length: \"+str(len(train_texts[i])))\n",
    "                print(\"Training target outputs sequence length: \"+str(len(train_out)))\n",
    "            \n",
    "                print(\"\\nTEXT:\")\n",
    "                flag = 0\n",
    "                for vec in train_texts[i]:\n",
    "                    if vec2word(vec) in string.punctuation or flag==0:\n",
    "                        print(str(vec2word(vec)),end='')\n",
    "                    else:\n",
    "                        print((\" \"+str(vec2word(vec))),end='')\n",
    "                    flag=1\n",
    "\n",
    "                print(\"\\n\")\n",
    "\n",
    "\n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,pred = sess.run([optimizer,cost,prediction],feed_dict={tf_text: train_texts[i], \n",
    "                                                    tf_seq_len: len(train_texts[i]), \n",
    "                                                    tf_summary: train_out,\n",
    "                                                    tf_output_len: len(train_out)})\n",
    "            \n",
    "         \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nPREDICTED SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for index in pred:\n",
    "                    #if int(index)!=vocab_limit.index('eos'):\n",
    "                    if vocab_limit[int(index)] in string.punctuation or flag==0:\n",
    "                        print(str(vocab_limit[int(index)]),end='')\n",
    "                    else:\n",
    "                        print(\" \"+str(vocab_limit[int(index)]),end='')\n",
    "                    flag=1\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                print(\"ACTUAL SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for vec in train_summaries[i]:\n",
    "                    if vec2word(vec)!='eos':\n",
    "                        if vec2word(vec) in string.punctuation or flag==0:\n",
    "                            print(str(vec2word(vec)),end='')\n",
    "                        else:\n",
    "                            print((\" \"+str(vec2word(vec))),end='')\n",
    "                    flag=1\n",
    "\n",
    "                print(\"\\n\")\n",
    "                print(\"loss=\"+str(loss))\n",
    "            \n",
    "        step=step+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
