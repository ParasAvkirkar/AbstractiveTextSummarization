{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Summarization\n",
    "\n",
    "Loading pre-trained GloVe embeddings.\n",
    "Source of Data: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Another interesting embedding to look into:\n",
    "https://github.com/commonsense/conceptnet-numberbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.50d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadGloVe(filename)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "word_vec_dim = len(embedding[0])\n",
    "#Pre-trained GloVe embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will define functions for converting words to its vector representations and vice versa. \n",
    "\n",
    "### word2vec: \n",
    "\n",
    "Converts words to its vector representations.\n",
    "If the word is not present in the vocabulary, and thus if it doesn't have any vector representation,\n",
    "the word will be considered as 'unk' (denotes unknown) and the vector representation of unk will be\n",
    "returned instead. \n",
    "\n",
    "### np_nearest_neighbour:\n",
    "\n",
    "Returns the word vector in the vocabularity that is most similar\n",
    "to word vector given as an argument. The similarity is evaluated based on the formula of cosine\n",
    "similarity. \n",
    "\n",
    "### vec2word: \n",
    "\n",
    "Converts vectors to words. If the vector representation is unknown, and no corresponding word\n",
    "is known, then it returns the word representation of a known vector representation which is most similar \n",
    "to the vector given as argument (the np_nearest_neighbour() function is used for that).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_nearest_neighbour(x):\n",
    "    #returns array in embedding that's most similar (in terms of cosine similarity) to x\n",
    "        \n",
    "    xdoty = np.multiply(embedding,x)\n",
    "    xdoty = np.sum(xdoty,1)\n",
    "    xlen = np.square(x)\n",
    "    xlen = np.sum(xlen,0)\n",
    "    xlen = np.sqrt(xlen)\n",
    "    ylen = np.square(embedding)\n",
    "    ylen = np.sum(ylen,1)\n",
    "    ylen = np.sqrt(ylen)\n",
    "    xlenylen = np.multiply(xlen,ylen)\n",
    "    cosine_similarities = np.divide(xdoty,xlenylen)\n",
    "\n",
    "    return embedding[np.argmax(cosine_similarities)]\n",
    "\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('unk')]\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    for x in xrange(0, len(embedding)):\n",
    "        if np.array_equal(embedding[x],np.asarray(vec)):\n",
    "            return vocab[x]\n",
    "    return vec2word(np_nearest_neighbour(np.asarray(vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre-processed dataset\n",
    "\n",
    "The Data is preprocessed in [Data_Pre-processing.ipynb](https://github.com/JRC1995/Abstractive-Summarization/blob/master/Data%20Pre-processing.ipynb)\n",
    "\n",
    "Dataset source: https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('vec_summaries', 'rb') as fp:\n",
    "    vec_summaries = pickle.load(fp)\n",
    "\n",
    "with open ('vec_texts', 'rb') as fp:\n",
    "    vec_texts = pickle.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am Loading vocab_limit and embd_limit (though I may not ever use embd_limit).\n",
    "Vocab_limit contains only vocabularies that are present in the dataset and \n",
    "some special words representing markers 'eos', '<PAD>' etc.\n",
    "\n",
    "The network should output the probability distribution over the words in \n",
    "vocab_limit. So using limited vocabulary (vocab_limit) will mean requiring\n",
    "less parameters for calculating the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('vocab_limit', 'rb') as fp:\n",
    "    vocab_limit = pickle.load(fp)\n",
    "\n",
    "with open ('embd_limit', 'rb') as fp:\n",
    "    embd_limit = pickle.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a one hot encoded vector to represent <SOS> which will represent the starting token for the decoder or the initial decoded input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_limit.append('<SOS>')\n",
    "\n",
    "SOS_prob_dist = np.zeros((len(vocab_limit)),dtype=np.float32)\n",
    "SOS_prob_dist[vocab_limit.index('<SOS>')]=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVING DATA WITH SUMMARIES WHICH ARE TOO LONG\n",
    "\n",
    "I will not be training the model in batches. I will train the model one sample at a time, because my laptop\n",
    "will probably not be able to handle batch training (the kernel crashes now and then even with SGD ).\n",
    "\n",
    "However, if I was training in batches I had to choose a fixed maximum length for output.\n",
    "Each target output is marked with the word 'eos' at the end. After that each target output can be padded with\n",
    "'<PAD>' to fit the maximum output length. The network can be taught to produce an output in the form\n",
    "\"word1 word2 eos <PAD> <PAD>\". The batch training can be handled better if all target outputs are transformed\n",
    "to a fixed length. \n",
    "\n",
    "But, the fixed length should be less than or equal to the length of the longest target output so as to\n",
    "not discard any word from any target-output sample.\n",
    "\n",
    "But there may be a few very long target outputs\\summaries (say, 50+) whereas most summaries are near about\n",
    "length 10. So to fix the length, lots of padding has to be done to most of the summaries just because there\n",
    "are a few long summaries. \n",
    "\n",
    "Better to just remove the data whose summaries are bigger than a specified threshold (MAX_SUMMARY_LEN).\n",
    "In this cell I will diagnose how many percentage of data will be removed for a given threshold length,\n",
    "and in the next cell I will remove them.\n",
    "\n",
    "Note: I am comparing len(summary_vec)-1, instead of len(summary_vec). The reason is that I am ignoring \n",
    "the last word vector which is the representation of the 'eos' marker. I will explain why later on this\n",
    "notebook. \n",
    "\n",
    "### REMOVING DATA WITH TEXTS WHOSE LENGTH IS SMALLER THAN THE WINDOW SIZE\n",
    "\n",
    "In this model I will try to implement <b>local attention</b> with standard encoder-decoder architecture.\n",
    "\n",
    "Where global attention looks at all the hidden states of the encoder to determine where to attend to,\n",
    "local attention looks only at the hidden states under the range pt-D to pt+D where D is empirically selected\n",
    "and pt is a position determined by the program.\n",
    "The range of pt-D to pt+D can be said to be the window where attention takes place.  Pt is the center of the\n",
    "window.\n",
    "\n",
    "I am treating D as a hyperparameter. The window size will be (pt-D)-(pt+D)+1 = 2D+1.\n",
    "\n",
    "Now, obviously, the window needs to be smaller than or equal to the no. of the encoded hidden states themselves.\n",
    "We will encode one hidden state for each words in the input text, so size of the hidden states will be equivalent\n",
    "to the size of the input text.\n",
    "\n",
    "So we must choose D such that 2D+1 is not bigger than the length of any text in the dataset.\n",
    "\n",
    "To ensure that, I will first diagnose how many data will be removed for a given D, and in the next cell,\n",
    "I will remove all input texts whose length is less than 2D+1.\n",
    "\n",
    "### REMOVING DATA WITH TEXTS(REVIEWS) WHICH ARE TOO LONG\n",
    "\n",
    "The RNN encoders will encode one word at a time. No. of words in the text data or in other words,\n",
    "the length of the text size will also be the no. of timesteps for the encoder RNN. To make the training less intensive \n",
    "(so that it doesn't burden my laptop too much), I will be removing\n",
    "all data with whose review size exceeds a given threshold (MAX_TEXT_LEN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of dataset with summary length beyond 7: 16.146% \n",
      "Percentage of dataset with text length less that window size: 2.258% \n",
      "Percentage of dataset with text length more than 80: 40.412% \n"
     ]
    }
   ],
   "source": [
    "#DIAGNOSIS\n",
    "\n",
    "count = 0\n",
    "\n",
    "LEN = 7\n",
    "\n",
    "for summary in vec_summaries:\n",
    "    if len(summary)-1>LEN:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with summary length beyond \"+str(LEN)+\": \"+str((count/len(vec_summaries))*100)+\"% \"\n",
    "\n",
    "count = 0\n",
    "\n",
    "D = 10 \n",
    "\n",
    "window_size = 2*D+1\n",
    "\n",
    "for text in vec_texts:\n",
    "    if len(text)<window_size+1:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with text length less that window size: \"+str((count/len(vec_texts))*100)+\"% \"\n",
    "\n",
    "count = 0\n",
    "\n",
    "LEN = 80\n",
    "\n",
    "for text in vec_texts:\n",
    "    if len(text)>LEN:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with text length more than \"+str(LEN)+\": \"+str((count/len(vec_texts))*100)+\"% \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will start the aformentioned removal process.\n",
    "vec_summary_reduced and vec_texts_reduced will contain the remaining data after the removal.\n",
    "\n",
    "<b>Note: an important hyperparameter D is initialized here.</b>\n",
    "\n",
    "D determines the window size of local attention as mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SUMMARY_LEN = 7\n",
    "MAX_TEXT_LEN = 80\n",
    "\n",
    "#D is a major hyperparameters. Windows size for local attention will be 2*D+1\n",
    "D = 10\n",
    "\n",
    "window_size = 2*D+1\n",
    "\n",
    "#REMOVE DATA WHOSE SUMMARIES ARE TOO BIG\n",
    "#OR WHOSE TEXT LENGTH IS TOO BIG\n",
    "#OR WHOSE TEXT LENGTH IS SMALLED THAN WINDOW SIZE\n",
    "\n",
    "vec_summaries_reduced = []\n",
    "vec_texts_reduced = []\n",
    "\n",
    "i = 0\n",
    "for summary in vec_summaries:\n",
    "    if len(summary)-1<=MAX_SUMMARY_LEN and len(vec_texts[i])>=window_size and len(vec_texts[i])<=MAX_TEXT_LEN:\n",
    "        vec_summaries_reduced.append(summary)\n",
    "        vec_texts_reduced.append(vec_texts[i])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will start the aformentioned removal process.\n",
    "vec_summary_reduced and vec_texts_reduced will contain the remaining data after the removal.\n",
    "\n",
    "<b>Note: an important hyperparameter D is initialized here.</b>\n",
    "\n",
    "D determines the window size of local attention as mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int((.7)*len(vec_summaries_reduced))\n",
    "\n",
    "train_texts = vec_texts_reduced[0:train_len]\n",
    "train_summaries = vec_summaries_reduced[0:train_len]\n",
    "\n",
    "val_len = int((.15)*len(vec_summaries_reduced))\n",
    "\n",
    "val_texts = vec_texts_reduced[train_len:train_len+val_len]\n",
    "val_summaries = vec_summaries_reduced[train_len:train_len+val_len]\n",
    "\n",
    "test_texts = vec_texts_reduced[train_len+val_len:len(vec_summaries_reduced)]\n",
    "test_summaries = vec_summaries_reduced[train_len+val_len:len(vec_summaries_reduced)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18293\n"
     ]
    }
   ],
   "source": [
    "print train_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function transform_out() will convert the target output sample so that \n",
    "it can be in a format which can be used by tensorflow's \n",
    "sparse_softmax_cross_entropy_with_logits() for loss calculation.\n",
    "\n",
    "Think of one hot encoding. This transformation is kind of like that.\n",
    "All the words in the vocab_limit are like classes in this context.\n",
    "\n",
    "However, instead of being precisely one hot encoded the output will be transformed\n",
    "such that it will contain the list of indexes which would have been 'one' if it was one hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_out(output_text):\n",
    "    output_len = len(output_text)\n",
    "    transformed_output = np.zeros([output_len],dtype=np.int32)\n",
    "    for i in xrange(0,output_len):\n",
    "        transformed_output[i] = vocab_limit.index(vec2word(output_text[i]))\n",
    "    return transformed_output   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Here I am simply setting up some of the rest of the hyperparameters.\n",
    "K, here, is a special hyperparameter. It denotes the no. of previous hidden states\n",
    "to consider for residual connections. More on that later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some MORE hyperparameters and other stuffs\n",
    "\n",
    "hidden_size = 250\n",
    "learning_rate = 0.001\n",
    "K = 5\n",
    "vocab_len = len(vocab_limit)\n",
    "training_iters = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up tensorflow placeholders.\n",
    "The purpose of the placeholders are pretty much self explanatory from the name.\n",
    "\n",
    "Note: tf_seq_len, and tf_output_len aren't really necessary. They can be derived \n",
    "from tf_text and tf_summary respectively, but I ended up making them anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#placeholders\n",
    "tf_text = tf.placeholder(tf.float32, [None,word_vec_dim])\n",
    "tf_seq_len = tf.placeholder(tf.int32)\n",
    "tf_summary = tf.placeholder(tf.int32,[None])\n",
    "tf_output_len = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORWARD AND BACKWARD LSTM WITH RRA\n",
    "\n",
    "I will be using the encoder-decoder architecture.\n",
    "For the encoder I will be using a bi-directional LSTM.\n",
    "Below is the function of the forward encoder (the LSTM in the forward direction\n",
    "that starts from the first word and encodes a word in the context of previous words),\n",
    "and then for the backward encoder (the LSTM in the backward direction\n",
    "that starts from the last word and encodes a word in the context of later words)\n",
    "\n",
    "The RNN used here, is a standard LSTM with RRA ([Residual Recurrent Attention](https://arxiv.org/abs/1709.03714))\n",
    "\n",
    "Remember, the hyperparameter K?\n",
    "\n",
    "The model will compute the weighted sum (weighted based on some trainable parameters\n",
    "in the attention weight matrix) of the PREVIOUS K hidden states - the weighted sum\n",
    "is denoted as RRA in this function.\n",
    "\n",
    "hidden_residuals will contain the last K hidden states.\n",
    "\n",
    "The RRA will influence the Hidden State calculation in LSTM.\n",
    "\n",
    "(The attention weight matrix is to be normalized by dividing each elements by the sum of all \n",
    "the elements as said in the paper. But, here, I am normalizing it by softmax)\n",
    "\n",
    "The purpose for this is to created connections between hidden states of different timesteps,\n",
    "to establish long term dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_encoder(inp,hidden,cell,\n",
    "                    wf,uf,bf,\n",
    "                    wi,ui,bi,\n",
    "                    wo,uo,bo,\n",
    "                    wc,uc,bc,\n",
    "                    Wattention,seq_len,inp_dim):\n",
    "\n",
    "    Wattention = tf.nn.softmax(Wattention,0)\n",
    "    hidden_forward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    i=0\n",
    "    j=K\n",
    "    \n",
    "    def cond(i,j,hidden,cell,hidden_forward,hidden_residuals):\n",
    "        return i < seq_len\n",
    "    \n",
    "    def body(i,j,hidden,cell,hidden_forward,hidden_residuals):\n",
    "        \n",
    "        x = tf.reshape(inp[i],[1,inp_dim])\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n",
    "        RRA = tf.reshape(RRA,[1,hidden_size])\n",
    "        \n",
    "        # LSTM with RRA\n",
    "        fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n",
    "        ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n",
    "        og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n",
    "        cell = tf.multiply(fg,cell) + tf.multiply(ig,tf.sigmoid( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n",
    "        hidden = tf.multiply(og,tf.tanh(cell+RRA))\n",
    "        \n",
    "        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n",
    "                                   lambda: hidden_residuals,\n",
    "                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n",
    "\n",
    "        hidden_forward = hidden_forward.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "        \n",
    "        return i+1,j+1,hidden,cell,hidden_forward,hidden_residuals\n",
    "    \n",
    "    _,_,_,_,hidden_forward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,cell,hidden_forward,hidden_residuals])\n",
    "    \n",
    "    hidden_residuals.close().mark_used()\n",
    "    \n",
    "    return hidden_forward.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_encoder(inp,hidden,cell,\n",
    "                     wf,uf,bf,\n",
    "                     wi,ui,bi,\n",
    "                     wo,uo,bo,\n",
    "                     wc,uc,bc,\n",
    "                     Wattention,seq_len,inp_dim):\n",
    "    \n",
    "    Wattention = tf.nn.softmax(Wattention,0)\n",
    "    hidden_backward = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals = hidden_residuals.unstack(tf.zeros([K,hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    i=seq_len-1\n",
    "    j=K\n",
    "    \n",
    "    def cond(i,j,hidden,cell,hidden_backward,hidden_residuals):\n",
    "        return i > -1\n",
    "    \n",
    "    def body(i,j,hidden,cell,hidden_backward,hidden_residuals):\n",
    "        \n",
    "        x = tf.reshape(inp[i],[1,inp_dim])\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention),0)\n",
    "        RRA = tf.reshape(RRA,[1,hidden_size])\n",
    "        \n",
    "        # LSTM with RRA\n",
    "        fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n",
    "        ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n",
    "        og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n",
    "        cell = tf.multiply(fg,cell) + tf.multiply(ig,tf.sigmoid( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n",
    "        hidden = tf.multiply(og,tf.tanh(cell+RRA))\n",
    "\n",
    "        hidden_residuals = tf.cond(tf.equal(j,seq_len-1+K),\n",
    "                                   lambda: hidden_residuals,\n",
    "                                   lambda: hidden_residuals.write(j,tf.reshape(hidden,[hidden_size])))\n",
    "        \n",
    "        hidden_backward = hidden_backward.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "        \n",
    "        return i-1,j+1,hidden,cell,hidden_backward,hidden_residuals\n",
    "    \n",
    "    _,_,_,_,hidden_backward,hidden_residuals = tf.while_loop(cond,body,[i,j,hidden,cell,hidden_backward,hidden_residuals])\n",
    "\n",
    "    hidden_residuals.close().mark_used()\n",
    "    \n",
    "    return hidden_backward.stack()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder similarly uses LSTM with RRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x,hidden,cell,\n",
    "            wf,uf,bf,\n",
    "            wi,ui,bi,\n",
    "            wo,uo,bo,\n",
    "            wc,uc,bc,RRA):\n",
    "    \n",
    "    # LSTM with RRA\n",
    "    fg = tf.sigmoid( tf.matmul(x,wf) + tf.matmul(hidden,uf) + bf)\n",
    "    ig = tf.sigmoid( tf.matmul(x,wi) + tf.matmul(hidden,ui) + bi)\n",
    "    og = tf.sigmoid( tf.matmul(x,wo) + tf.matmul(hidden,uo) + bo)\n",
    "    cell_next = tf.multiply(fg,cell) + tf.multiply(ig,tf.sigmoid( tf.matmul(x,wc) + tf.matmul(hidden,uc) + bc))\n",
    "    hidden_next = tf.multiply(og,tf.tanh(cell+RRA))\n",
    "    \n",
    "    return hidden_next,cell_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOCAL ATTENTION:\n",
    "\n",
    "The cell below includes some major functions for the attention mechanism.\n",
    "\n",
    "The attention mechanism is usually implemented to compute an attention score \n",
    "for each of the encoded hidden state in the context of a particular\n",
    "decoder hidden state in each timestep - all to determine which encoded hidden\n",
    "states to attend to for a particular decoder hidden state context.\n",
    "\n",
    "More specifically, I am here implementing local attention as opposed to global attention.\n",
    "\n",
    "I already mentioned local attention before. Local attention mechanism involves focusing on\n",
    "a subset of encoded hidden states, whereas a gloabl attention mechanism invovles focusing on all\n",
    "the encoded hidden states.\n",
    "\n",
    "This is the paper on which this implementation is based on:\n",
    "https://nlp.stanford.edu/pubs/emnlp15_attn.pdf\n",
    "    \n",
    "Following the formulas presented in the paper, first, I am computing\n",
    "the position pt (the center of the window of attention).\n",
    "\n",
    "pt is simply a position in the sequence.\n",
    "For a given pt, the model will only consider the hidden state starting from the position\n",
    "pt-D to the hidden state at the position pt+D. \n",
    "\n",
    "To say a hidden state is at position p, I mean to say that the hidden state is the encoded\n",
    "representation of a word at position p in the sequence.\n",
    "\n",
    "The paper formulates the equation for calculating pt like this:\n",
    "pt = sequence_length x sigmoid(..some linear algebras and activations...)\n",
    "\n",
    "But, I didn't used the sequence_length of the whole text which is tf_seq_len but 'positions' which\n",
    "is = tf_seq_len-1-2D\n",
    "\n",
    "if pt = tf_seq_len x sigmoid(tensor)\n",
    "\n",
    "Then pt will be in the range 0 to tf_seq_len\n",
    "\n",
    "But, we can't have that. There is no tf_seq_len position. Since the length is tf_seq_len,\n",
    "the available positions are 0 to (tf_seq_len-1). Which is why I subtracted 1 from it.\n",
    "\n",
    "Next, we must have the value of pt to be such that it represents the CENTER of the window.\n",
    "If pt is too close to 0, pt-D will be negative - a non-existent position.\n",
    "If pt is too close to tf_seq_len, pt+D will be a non-existent position.\n",
    "\n",
    "So pt can't occupy the first D positions (0 to D-1) and it can't occupy the last D positions\n",
    "((tf_seq_len-D) to (tf_seq_len-1)) in order to keep pt-D and pt+D as legal positions.\n",
    "So a total 2D positions should be restricted to pt.\n",
    "\n",
    "Which is why I further subtracted 2D from tf_seq_len.\n",
    "\n",
    "Still, after calculating pt = positions x sigmoid(tensor)\n",
    "where positions = tf_seq_len-(2D+1), \n",
    "pt will merely range between 0 to tf_seq_len-(2D+1)\n",
    "\n",
    "We can't still accept pt to be 0 since pt-D will be negative. But the length of the range \n",
    "of integer positions pt can occupy is now perfect.\n",
    "\n",
    "So at this point, we can simply center pt at the window by adding a D.\n",
    "\n",
    "After that, pt will range from D to (tf_seq_len-1)-D\n",
    "\n",
    "Now, it can be checked that pt+D, or pt-D will never become negative or exceed\n",
    "the total sequence length.\n",
    "\n",
    "After calculating pt, we can use the formulas presented in the paper to calculate\n",
    "the G score which signifies the weight (or attention) that should be given to a hidden state.\n",
    "\n",
    "G scores is calculated for each of hidden states in the local window. This is equivalent to\n",
    "a(s) used in the paper.\n",
    "\n",
    "The function returns the G scores and the position pt, so that the model can create the \n",
    "context vector. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(hs,ht,Wa,seq_len):\n",
    "    return tf.reshape(tf.matmul(tf.matmul(hs,Wa),tf.transpose(ht)),[seq_len])\n",
    "\n",
    "def align(hs,ht,Wp,Vp,Wa,tf_seq_len):\n",
    "   \n",
    "    pd = tf.TensorArray(size=(2*D+1),dtype=tf.float32)\n",
    "    \n",
    "    positions = tf.cast(tf_seq_len-1-2*D,dtype=tf.float32)\n",
    "    \n",
    "    sigmoid_multiplier = tf.nn.sigmoid(tf.matmul(tf.tanh(tf.matmul(ht,Wp)),Vp))\n",
    "    sigmoid_multiplier = tf.reshape(sigmoid_multiplier,[])\n",
    "    \n",
    "    pt_float = positions*sigmoid_multiplier\n",
    "    \n",
    "    pt = tf.cast(pt_float,tf.int32)\n",
    "    pt = pt+D #center to window\n",
    "    \n",
    "    sigma = tf.constant(D/2,dtype=tf.float32)\n",
    "    \n",
    "    i = 0\n",
    "    pos = pt - D\n",
    "    \n",
    "    def cond(i,pos,pd):\n",
    "        \n",
    "        return i < (2*D+1)\n",
    "                      \n",
    "    def body(i,pos,pd):\n",
    "        \n",
    "        comp_1 = tf.cast(tf.square(pos-pt),tf.float32)\n",
    "        comp_2 = tf.cast(2*tf.square(sigma),tf.float32)\n",
    "            \n",
    "        pd = pd.write(i,tf.exp(-(comp_1/comp_2)))\n",
    "            \n",
    "        return i+1,pos+1,pd\n",
    "                      \n",
    "    i,pos,pd = tf.while_loop(cond,body,[i,pos,pd])\n",
    "    \n",
    "    local_hs = hs[(pt-D):(pt+D+1)]\n",
    "    \n",
    "    normalized_scores = tf.nn.softmax(score(local_hs,ht,Wa,2*D+1))\n",
    "    \n",
    "    pd=pd.stack()\n",
    "    \n",
    "    G = tf.multiply(normalized_scores,pd)\n",
    "    G = tf.reshape(G,[2*D+1,1])\n",
    "    \n",
    "    return G,pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL DEFINITION\n",
    "\n",
    "First is the <b>bi-directional encoder</b>.\n",
    "\n",
    "h_forward is the tensorarray of all the hidden states from the \n",
    "forward encoder whereas h_backward is the tensorarray of all the hidden states\n",
    "from the backward encoder.\n",
    "\n",
    "The final list of encoder hidden states are usually calculated by combining \n",
    "the equivalents of h_forward and h_backward by some means.\n",
    "\n",
    "There are many means of combining them, like: concatenation, summation, average etc.\n",
    "    \n",
    "I will be using concatenation.\n",
    "\n",
    "hidden_encoder is the final list of encoded hidden state\n",
    "\n",
    "The first decoder input is the probability distribution with 1 at the index of <SOS>,\n",
    "in other words, one hot encoded representation of <SOS> - which signifies the start of\n",
    "decoding.\n",
    "\n",
    "I am using the first encoded_hidden_state \n",
    "as the initial decoder state. The first encoded_hidden_state may have the least \n",
    "past context (none actually) but, it will have the most future context.\n",
    "\n",
    "The next decoder hidden state is generated from the initial decoder input and the initial decoder state.\n",
    "Next, I start a loop which iterates for output_len times. \n",
    "\n",
    "Next the <b>attention function</b> is called, to compute the G score by scoring the encoder hidden states\n",
    "in term of current decoder hidden step.\n",
    "\n",
    "The context vector is created by the weight (weighted in terms of G scores) summation\n",
    "of hidden states in the local attention window.\n",
    "\n",
    "I used the formulas mentioned here: https://nlp.stanford.edu/pubs/emnlp15_attn.pdf\n",
    "\n",
    "to calculate the first actual (not the <SOS> one) y (output) from the context vector and decoder hidden state.\n",
    "\n",
    "Note: y is of the same size as the no. of vocabs in vocab_limit. Y is supposed to be \n",
    "a probability distribution. The value of index i of Y denotes the probability for Y \n",
    "to be the word that is located in the index i of vacab_limit.\n",
    "\n",
    "('beam search' is another approach to look into, for not predicting simply the next word,\n",
    "but the next k words.)\n",
    "\n",
    "This y will be the input for the <b>decoder LSTM</b>. In the context of this y and \n",
    "the current decoder hidden state, the RNN produces the next decoder hidden state. And, the loop\n",
    "continues for 'output_len' no. of iterations. \n",
    "\n",
    "Since I will be training sample to sample, I can dynamically send the output length \n",
    "of the current sample, and the decoder loops for the given 'output length' times.\n",
    "\n",
    "NOTE: I am saving y without softmax in the tensorarray output. Why? Because\n",
    "I will be using tensorflow cost functions that requires the logits to be without\n",
    "softmax (the function will internally apply Softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(tf_text,tf_seq_len,tf_output_len):\n",
    "    \n",
    "    #PARAMETERS\n",
    "    \n",
    "    #1.1 FORWARD ENCODER PARAMETERS\n",
    "    \n",
    "    initial_hidden_f = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    cell_f = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    wf_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uf_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bf_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wi_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    ui_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bi_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wo_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uo_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bo_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wc_f = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uc_f = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bc_f = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    Wattention_f = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "                               \n",
    "    #1.2 BACKWARD ENCODER PARAMETERS\n",
    "    \n",
    "    initial_hidden_b = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    cell_b = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    wf_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uf_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bf_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wi_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    ui_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bi_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wo_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uo_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bo_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    wc_b = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    uc_b = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    bc_b = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    Wattention_b = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "    \n",
    "    #2 ATTENTION PARAMETERS\n",
    "    \n",
    "    Wp = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,50],stddev=0.01))\n",
    "    Vp = tf.Variable(tf.truncated_normal(shape=[50,1],stddev=0.01))\n",
    "    Wa = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,2*hidden_size],stddev=0.01))\n",
    "    Wc = tf.Variable(tf.truncated_normal(shape=[4*hidden_size,2*hidden_size],stddev=0.01))\n",
    "    \n",
    "    #3 DECODER PARAMETERS\n",
    "    \n",
    "    Ws = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,vocab_len],stddev=0.01))\n",
    "    \n",
    "    cell_d = tf.zeros([1,2*hidden_size],dtype=tf.float32)\n",
    "    wf_d = tf.Variable(tf.truncated_normal(shape=[vocab_len,2*hidden_size],stddev=0.01))\n",
    "    uf_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bf_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    wi_d = tf.Variable(tf.truncated_normal(shape=[vocab_len,2*hidden_size],stddev=0.01))\n",
    "    ui_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bi_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    wo_d = tf.Variable(tf.truncated_normal(shape=[vocab_len,2*hidden_size],stddev=0.01))\n",
    "    uo_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bo_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    wc_d = tf.Variable(tf.truncated_normal(shape=[vocab_len,2*hidden_size],stddev=0.01))\n",
    "    uc_d = tf.Variable(np.eye(2*hidden_size),dtype=tf.float32)\n",
    "    bc_d = tf.Variable(tf.zeros([1,2*hidden_size]),dtype=tf.float32)\n",
    "    \n",
    "    hidden_residuals_d = tf.TensorArray(size=K,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_residuals_d = hidden_residuals_d.unstack(tf.zeros([K,2*hidden_size],dtype=tf.float32))\n",
    "    \n",
    "    Wattention_d = tf.Variable(tf.zeros([K,1]),dtype=tf.float32)\n",
    "    \n",
    "    output = tf.TensorArray(size=tf_output_len,dtype=tf.float32)\n",
    "                               \n",
    "    #BI-DIRECTIONAL LSTM\n",
    "                               \n",
    "    hidden_forward = forward_encoder(tf_text,\n",
    "                                     initial_hidden_f,cell_f,\n",
    "                                     wf_f,uf_f,bf_f,\n",
    "                                     wi_f,ui_f,bi_f,\n",
    "                                     wo_f,uo_f,bo_f,\n",
    "                                     wc_f,uc_f,bc_f,\n",
    "                                     Wattention_f,\n",
    "                                     tf_seq_len,\n",
    "                                     word_vec_dim)\n",
    "    \n",
    "    hidden_backward = backward_encoder(tf_text,\n",
    "                                     initial_hidden_b,cell_b,\n",
    "                                     wf_b,uf_b,bf_b,\n",
    "                                     wi_b,ui_b,bi_b,\n",
    "                                     wo_b,uo_b,bo_b,\n",
    "                                     wc_b,uc_b,bc_b,\n",
    "                                     Wattention_b,\n",
    "                                     tf_seq_len,\n",
    "                                     word_vec_dim)\n",
    "    \n",
    "    encoded_hidden = tf.concat([hidden_forward,hidden_backward],1)\n",
    "    \n",
    "    #ATTENTION MECHANISM AND DECODER\n",
    "    \n",
    "    decoded_hidden = encoded_hidden[0]\n",
    "    decoded_hidden = tf.reshape(decoded_hidden,[1,2*hidden_size])\n",
    "    Wattention_d_normalized = tf.nn.softmax(Wattention_d)\n",
    "    \n",
    "    y = tf.convert_to_tensor(SOS_prob_dist) #inital output <SOS>\n",
    "    y = tf.reshape(y,[1,vocab_len])\n",
    "    \n",
    "    j=K\n",
    "    \n",
    "    hidden_residuals_stack = hidden_residuals_d.stack()\n",
    "    \n",
    "    RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention_d_normalized),0)\n",
    "    RRA = tf.reshape(RRA,[1,2*hidden_size])\n",
    "    \n",
    "    decoded_hidden_next,cell_d = decoder(y,decoded_hidden,cell_d,\n",
    "                                  wf_d,uf_d,bf_d,\n",
    "                                  wi_d,ui_d,bf_d,\n",
    "                                  wo_d,uo_d,bf_d,\n",
    "                                  wc_d,uc_d,bc_d,\n",
    "                                  RRA)\n",
    "    decoded_hidden = decoded_hidden_next\n",
    "    \n",
    "    hidden_residuals_d = hidden_residuals_d.write(j,tf.reshape(decoded_hidden,[2*hidden_size]))\n",
    "    \n",
    "    j=j+1\n",
    "                           \n",
    "    i=0\n",
    "    \n",
    "    def attention_decoder_cond(i,j,decoded_hidden,cell_d,hidden_residuals_d,output):\n",
    "        return i < tf_output_len\n",
    "    \n",
    "    def attention_decoder_body(i,j,decoded_hidden,cell_d,hidden_residuals_d,output):\n",
    "        \n",
    "        #LOCAL ATTENTION\n",
    "        \n",
    "        G,pt = align(encoded_hidden,decoded_hidden,Wp,Vp,Wa,tf_seq_len)\n",
    "        local_encoded_hidden = encoded_hidden[pt-D:pt+D+1]\n",
    "        weighted_encoded_hidden = tf.multiply(local_encoded_hidden,G)\n",
    "        context_vector = tf.reduce_sum(weighted_encoded_hidden,0)\n",
    "        context_vector = tf.reshape(context_vector,[1,2*hidden_size])\n",
    "        \n",
    "        attended_hidden = tf.tanh(tf.matmul(tf.concat([context_vector,decoded_hidden],1),Wc))\n",
    "        \n",
    "        #DECODER\n",
    "        \n",
    "        y = tf.matmul(attended_hidden,Ws)\n",
    "        \n",
    "        output = output.write(i,tf.reshape(y,[vocab_len]))\n",
    "        \n",
    "        y = tf.nn.softmax(y)\n",
    "        \n",
    "        hidden_residuals_stack = hidden_residuals_d.stack()\n",
    "        \n",
    "        RRA = tf.reduce_sum(tf.multiply(hidden_residuals_stack[j-K:j],Wattention_d_normalized),0)\n",
    "        RRA = tf.reshape(RRA,[1,2*hidden_size])\n",
    "        \n",
    "        decoded_hidden_next,cell_d = decoder(y,decoded_hidden,cell_d,\n",
    "                                  wf_d,uf_d,bf_d,\n",
    "                                  wi_d,ui_d,bf_d,\n",
    "                                  wo_d,uo_d,bf_d,\n",
    "                                  wc_d,uc_d,bc_d,\n",
    "                                  RRA)\n",
    "        \n",
    "        decoded_hidden = decoded_hidden_next\n",
    "        \n",
    "        hidden_residuals_d = tf.cond(tf.equal(j,tf_output_len-1+K+1), #(+1 for <SOS>)\n",
    "                                   lambda: hidden_residuals_d,\n",
    "                                   lambda: hidden_residuals_d.write(j,tf.reshape(decoded_hidden,[2*hidden_size])))\n",
    "        \n",
    "        return i+1,j+1,decoded_hidden,cell_d,hidden_residuals_d,output\n",
    "    \n",
    "    i,j,decoded_hidden,cell_d,hidden_residuals_d,output = tf.while_loop(attention_decoder_cond,\n",
    "                                            attention_decoder_body,\n",
    "                                            [i,j,decoded_hidden,cell_d,hidden_residuals_d,output])\n",
    "    hidden_residuals_d.close().mark_used()\n",
    "    \n",
    "    output = output.stack()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model function is initiated here. The output is\n",
    "computed. Cost function and optimizer are defined.\n",
    "I am creating a prediction tensorarray which will \n",
    "store the index of maximum element of \n",
    "the output probability distributions.\n",
    "From that index I can find the word in vocab_limit\n",
    "which is represented by it. So the final visible\n",
    "predictions will be the words that the model decides to\n",
    "be most probable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tf_text,tf_seq_len,tf_output_len)\n",
    "\n",
    "#OPTIMIZER\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=tf_summary))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#PREDICTION\n",
    "\n",
    "pred = tf.TensorArray(size=tf_output_len,dtype=tf.int32)\n",
    "\n",
    "i=0\n",
    "\n",
    "def cond_pred(i,pred):\n",
    "    return i<tf_output_len\n",
    "def body_pred(i,pred):\n",
    "    pred = pred.write(i,tf.cast(tf.argmax(output[i]),tf.int32))\n",
    "    return i+1,pred\n",
    "\n",
    "i,pred = tf.while_loop(cond_pred,body_pred,[i,pred]) \n",
    "\n",
    "prediction = pred.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING\n",
    "\n",
    "Finally, this is where training takes place.\n",
    "It's all pretty self explanatory, but one thing to note is that\n",
    "I am sending \"train_summaries[i][0:len(train_summaries[i])-1]\"\n",
    "to the transform_out() function. That is, I am ignoring the last\n",
    "word from summary. The last word marks the end of the summary.\n",
    "It's 'eos'. \n",
    "\n",
    "I trained it before without dynamically feeding the output_len.\n",
    "Ideally the network should determine the output_len by itself.\n",
    "\n",
    "Which is why I defined (in past) a MAX_LEN, and transformed target outputs in\n",
    "the form \"word1 word2 word3....eos <PAD> <PAD>....until max_length\"\n",
    "I created the model output in the same way.\n",
    "\n",
    "The model would ideally learn in which context and where to put eos.\n",
    "And then the only the portion before eos can be shown to the user.\n",
    "\n",
    "After training, the model can even be modified to run until,\n",
    "the previous output y denotes an eos. \n",
    "\n",
    "That way, we can have variable length output, with the length decided\n",
    "by the model itself, not the user.\n",
    "\n",
    "But all the padding and eos, makes the model to come in contact with \n",
    "pads and eos in most of the target output. The model learns to consider eos and \n",
    "pad to be important. Trying to fit to the data, the early model starts to\n",
    "spam eos and pad in its predicted output.\n",
    "\n",
    "That necessarily isn't a problem. The model may learn to fare better\n",
    "later on, but I planned only to check a couple of early iterations, \n",
    "and looking at predictions consisting of only eos and pads\n",
    "isn't too interesting. I wanted to check what kind of words (other than\n",
    "eos and pads) the model learns to produce in the early iterations. \n",
    "\n",
    "Which is why I am doing what I am doing. Ideally, my past implemention\n",
    "waould be better. \n",
    "\n",
    "As I said before, I will run it for only a few early iterations.\n",
    "So, it's not likely to see any great predicted summaries here.\n",
    "As can be seen, the summaries seem more influenced by previous \n",
    "output sample than the input context in these early iterations.\n",
    "\n",
    "Some of the texts contains undesirable words like br tags and so\n",
    "on. So better preprocessing and tokenization may be desirable.\n",
    "\n",
    "With more layer depth, larger hidden size, mini-batch training,\n",
    "and other changes, this model may have potential, or may not.\n",
    "\n",
    "The same arcitechture should be usable for training on translation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 0\n",
      "Training input sequence length: 51\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "i have bought several of the vitality canned dog food products and have found them all to be of good quality. the product looks more like a stew than a processed meat and it smells better. my labrador is finicky and she appreciates this product better than most.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "15-20 15-20 15-20 effectiveness\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "good quality dog food\n",
      "\n",
      "loss=10.3909\n",
      "\n",
      "Iteration: 1\n",
      "Training input sequence length: 37\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "product arrived labeled as jumbo salted peanuts ... the peanuts were actually small sized unsalted. not sure if this was an error or if the vendor intended to represent the product as `` jumbo ''.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "quality food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "not as advertised\n",
      "\n",
      "loss=10.4148\n",
      "\n",
      "Iteration: 2\n",
      "Training input sequence length: 46\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "if you are looking for the secret ingredient in robitussin i believe i have found it. i got this in addition to the root beer extract i ordered( which was good) and made some cherry soda. the flavor is very medicinal.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "quality food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "cough medicine\n",
      "\n",
      "loss=10.385\n",
      "\n",
      "Iteration: 3\n",
      "Training input sequence length: 32\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "great taffy at a great price. there was a wide assortment of yummy taffy. delivery was very quick. if your a taffy lover, this is a deal.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "quality food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great taffy\n",
      "\n",
      "loss=10.3916\n",
      "\n",
      "Iteration: 4\n",
      "Training input sequence length: 30\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "this taffy is so good. it is very soft and chewy. the flavors are amazing. i would definitely recommend you buying it. very satisfying!!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "quality food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "wonderful, tasty taffy\n",
      "\n",
      "loss=10.2868\n",
      "\n",
      "Iteration: 5\n",
      "Training input sequence length: 29\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "right now i 'm mostly just sprouting this so my cats can eat the grass. they love it. i rotate it around with wheatgrass and rye too\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "not food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "yay barley\n",
      "\n",
      "loss=10.3993\n",
      "\n",
      "Iteration: 6\n",
      "Training input sequence length: 29\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "this is a very healthy dog food. good for their digestion. also good for small puppies. my dog eats her required amount at every feeding.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "not food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "healthy dog food\n",
      "\n",
      "loss=9.35136\n",
      "\n",
      "Iteration: 7\n",
      "Training input sequence length: 24\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "the strawberry twizzlers are my guilty pleasure- yummy. six pounds will be around for a while with my son and i.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "not food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "strawberry twizzlers- yummy\n",
      "\n",
      "loss=10.4702\n",
      "\n",
      "Iteration: 8\n",
      "Training input sequence length: 45\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "i love eating them and they are good for watching tv and looking at movies! it is not too sweet. i like to transfer them to a zip lock baggie so they stay fresh so i can take my time eating them.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "not food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "poor taste\n",
      "\n",
      "loss=10.4183\n",
      "\n",
      "Iteration: 9\n",
      "Training input sequence length: 28\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i am very satisfied with my unk purchase. i shared these with others and we have all enjoyed them. i will definitely be ordering more.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "not food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "love it!\n",
      "\n",
      "loss=10.3971\n",
      "\n",
      "Iteration: 10\n",
      "Training input sequence length: 31\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "candy was delivered very fast and was purchased at a reasonable price. i was home bound and unable to get to a store so this was perfect for me.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "home delivered unk\n",
      "\n",
      "loss=10.3215\n",
      "\n",
      "Iteration: 11\n",
      "Training input sequence length: 52\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "my husband is a twizzlers addict. we 've bought these many times from amazon because we 're government employees living overseas and ca n't get them in the country we are assigned to. they 've always been fresh and tasty, packed well and arrive in a timely manner.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "always fresh\n",
      "\n",
      "loss=10.3019\n",
      "\n",
      "Iteration: 12\n",
      "Training input sequence length: 68\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "i bought these for my husband who is currently overseas. he loves these, and apparently his staff likes them unk< br/> there are generous amounts of twizzlers in each 16-ounce bag, and this was well worth the price.< a unk '' http: unk ''> twizzlers, strawberry, 16-ounce bags( pack of 6)< unk>\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "loss=9.13059\n",
      "\n",
      "Iteration: 13\n",
      "Training input sequence length: 31\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i can remember buying this candy as a kid and the quality has n't dropped in all these years. still a superb product you wo n't be disappointed with.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "delicious product!\n",
      "\n",
      "loss=9.87088\n",
      "\n",
      "Iteration: 14\n",
      "Training input sequence length: 21\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "i love this candy. after weight watchers i had to cut back but still have a craving for it.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "loss=8.08563\n",
      "\n",
      "Iteration: 15\n",
      "Training input sequence length: 72\n",
      "Training target outputs sequence length: 7\n",
      "\n",
      "TEXT:\n",
      "i have lived out of the us for over 7 yrs now, and i so miss my twizzlers!! when i go back to visit or someone visits me, i always stock up. all i can say is yum!< br/> sell these in mexico and you will have a faithful buyer, more often than i 'm able to buy them right now.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food food food food food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "please sell these in mexico!!\n",
      "\n",
      "loss=9.52525\n",
      "\n",
      "Iteration: 16\n",
      "Training input sequence length: 36\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "product received is as unk< br/>< br/>< a unk '' http: unk ''> twizzlers, strawberry, 16-ounce bags( pack of 6)< unk>\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "twizzlers- strawberry\n",
      "\n",
      "loss=6.50716\n",
      "\n",
      "Iteration: 17\n",
      "Training input sequence length: 43\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "i was so glad amazon carried these batteries. i have a hard time finding them elsewhere because they are such a unique size. i need them for my garage door unk< br/> great deal for the price.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food food food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great bargain for the price\n",
      "\n",
      "loss=9.7148\n",
      "\n",
      "Iteration: 18\n",
      "Training input sequence length: 26\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "this offer is a great price and a great taste, thanks amazon for selling this unk< br/>< br/> unk\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food food food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "this is my taste ...\n",
      "\n",
      "loss=9.77397\n",
      "\n",
      "Iteration: 19\n",
      "Training input sequence length: 60\n",
      "Training target outputs sequence length: 7\n",
      "\n",
      "TEXT:\n",
      "for those of us with celiac disease this product is a lifesaver and what could be better than getting it at almost half the price of the grocery or health food store! i love mccann 's instant oatmeal- all flavors!!!< br/>< br/> thanks,< br/> abby\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food food food food food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "love gluten free oatmeal!!!\n",
      "\n",
      "loss=7.71986\n",
      "\n",
      "Iteration: 20\n",
      "Training input sequence length: 59\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "what else do you need to know? oatmeal, instant( make it with a half cup of low-fat milk and add raisins; nuke for 90 seconds). more expensive than kroger store brand oatmeal and maybe a little tastier or better texture or something. it 's still just oatmeal. mmm, convenient!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "it 's oatmeal\n",
      "\n",
      "loss=9.44353\n",
      "\n",
      "Iteration: 21\n",
      "Training input sequence length: 79\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "i ordered this for my wife as it was unk by our daughter. she has this almost every morning and likes all flavors. she 's happy, i 'm happy!!!< br/>< a unk '' http: unk ''> mccann 's instant irish oatmeal, variety pack of regular, apples& cinnamon, and maple& brown sugar, 10-count boxes( pack of 6)< unk>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "wife 's favorite breakfast\n",
      "\n",
      "loss=11.2363\n",
      "\n",
      "Iteration: 22\n",
      "Training input sequence length: 38\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "i have mccann 's oatmeal every morning and by ordering it from amazon i am able to save almost$ 3.00 per unk< br/> it is a great product. tastes great and very healthy\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "unk\n",
      "\n",
      "loss=5.83876\n",
      "\n",
      "Iteration: 23\n",
      "Training input sequence length: 41\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "mccann 's oatmeal is a good quality choice. our favorite is the apples and cinnamon, but we find that none of these are overly sugary. for a good hot breakfast in 2 minutes, this is excellent.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great great great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "good hot breakfast\n",
      "\n",
      "loss=9.0744\n",
      "\n",
      "Iteration: 24\n",
      "Training input sequence length: 55\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "we really like the mccann 's steel cut oats but find we do n't cook it up too unk< br/> this tastes much better to me than the grocery store brands and is just as unk< br/> anything that keeps me eating oatmeal regularly is a good thing.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers twizzlers!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great taste and convenience\n",
      "\n",
      "loss=7.54807\n",
      "\n",
      "Iteration: 25\n",
      "Training input sequence length: 46\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "this seems a little more wholesome than some of the supermarket brands, but it is somewhat mushy and does n't have quite as much flavor either. it did n't pass muster with my kids, so i probably wo n't buy it again.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers twizzlers\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "hearty oatmeal\n",
      "\n",
      "loss=10.0591\n",
      "\n",
      "Iteration: 26\n",
      "Training input sequence length: 52\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "good oatmeal. i like the apple cinnamon the best. though i would n't follow the directions on the package since it always comes out too soupy for my taste. that could just be me since i like my oatmeal really thick to add some milk on top of.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "good\n",
      "\n",
      "loss=4.35645\n",
      "\n",
      "Iteration: 27\n",
      "Training input sequence length: 25\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "the flavors are good. however, i do not see any unk between this and unk oats brand- they are both mushy.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "mushy\n",
      "\n",
      "loss=11.9467\n",
      "\n",
      "Iteration: 28\n",
      "Training input sequence length: 41\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "this is the same stuff you can buy at the big box stores. there is nothing healthy about it. it is just carbs and sugars. save your money and get something that at least has some taste.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "same stuff\n",
      "\n",
      "loss=12.031\n",
      "\n",
      "Iteration: 29\n",
      "Training input sequence length: 25\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "this oatmeal is not good. its mushy, soft, i do n't like it. quaker oats is the way to go.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "do n't like it\n",
      "\n",
      "loss=9.48624\n",
      "\n",
      "Iteration: 30\n",
      "Training input sequence length: 37\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "we 're used to spicy foods down here in south texas and these are not at all spicy. doubt very much habanero is used at all. could take it up a notch or two.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "not ass kickin\n",
      "\n",
      "loss=9.74505\n",
      "\n",
      "Iteration: 31\n",
      "Training input sequence length: 80\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "i roast at home with a unk popcorn popper( but i do it outside, of course). these beans( coffee bean direct green mexican altura) seem to be well-suited for this method. the first and second cracks are distinct, and i 've roasted the beans from medium to slightly dark with great results every time. the aroma is strong and persistent. the taste is smooth, velvety, yet lively.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "roasts up a smooth brew\n",
      "\n",
      "loss=11.588\n",
      "\n",
      "Iteration: 32\n",
      "Training input sequence length: 69\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "we roast these in a large cast iron pan on the grill( about 1/3 of the bag at a time). the smell is wonderful and the roasted beans taste delicious too. more importantly, the coffee is smooth; no bitter aftertaste. on numerous occasions, we 've had to send the roasted beans home with friends because they like it so much.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "our guests love it!\n",
      "\n",
      "loss=6.4716\n",
      "\n",
      "Iteration: 33\n",
      "Training input sequence length: 38\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "deal was awesome! arrived before halloween as indicated and was enough to satisfy trick or treaters. i love the quality of this product and it was much less expensive than the local store 's candy.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "awesome deal!\n",
      "\n",
      "loss=8.3475\n",
      "\n",
      "Iteration: 34\n",
      "Training input sequence length: 40\n",
      "Training target outputs sequence length: 6\n",
      "\n",
      "TEXT:\n",
      "it is chocolate, what can i say. great variety of everything our family loves. with a family of six it goes fast here. perfect variety. kit kat, unk, take five and more.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!!!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "how can you go wrong!\n",
      "\n",
      "loss=9.67854\n",
      "\n",
      "Iteration: 35\n",
      "Training input sequence length: 26\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "halloween is over but, i sent a bag to my daughters class for her share. the chocolate was fresh and enjoyed by many.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great deal.\n",
      "\n",
      "loss=8.14521\n",
      "\n",
      "Iteration: 36\n",
      "Training input sequence length: 38\n",
      "Training target outputs sequence length: 6\n",
      "\n",
      "TEXT:\n",
      "watch your prices with this. while the assortment was good, and i did get this on a gold box purchase, the price for this was< br/>$ 3-4 less at target.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!!!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "better price for this at target\n",
      "\n",
      "loss=8.73441\n",
      "\n",
      "Iteration: 37\n",
      "Training input sequence length: 33\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "this bag of candy online is pretty expensive, it should be cheaper in order to compete with grocery stores, other than that, its a good combination of my favorite candy\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "pretty expensive\n",
      "\n",
      "loss=10.8112\n",
      "\n",
      "Iteration: 38\n",
      "Training input sequence length: 64\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "this product serves me well as a source of electrolytes during and after a long run or bike unk< br/> i have tried all of the flavors but really do like the grapefruit flavor ... no unk and i actually like the slight unk< br/> i use other hammer products and really like their whole product line.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "good!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great source of electrolytes\n",
      "\n",
      "loss=9.28261\n",
      "\n",
      "Iteration: 39\n",
      "Training input sequence length: 36\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "this stuff really works for preventing cramping during the middle to latter stages of your rides. pop 1 into each water bottle and you 're set. flavor is fine and goes down easy.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "good!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great for preventing cramps\n",
      "\n",
      "loss=7.69772\n",
      "\n",
      "Iteration: 40\n",
      "Training input sequence length: 23\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "no tea flavor at all. just whole brunch of unk flavors. it is not returnable. i wasted unk bucks.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "good!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "no tea flavor\n",
      "\n",
      "loss=10.7192\n",
      "\n",
      "Iteration: 41\n",
      "Training input sequence length: 67\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "these taste really good. i have been purchasing a different brand and these are very similar in taste and texture. i agree with the other reviewer regarding ordering in the summer. there is no insulating packaging with ice packs so they will melt in warm weather like all chocolate food items. order in cold weather and buy enough to last!!!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "good!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "taste great\n",
      "\n",
      "loss=4.2054\n",
      "\n",
      "Iteration: 42\n",
      "Training input sequence length: 28\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "the taste was great, but the berries had melted. may order again in winter. if you order in cold weather you should enjoy flavor.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great!!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "order only in cold weather\n",
      "\n",
      "loss=9.69668\n",
      "\n",
      "Iteration: 43\n",
      "Training input sequence length: 39\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i know i can not make tea this good. granted, i am not from the south but i know i have never enjoyed tea that was this sweet without being too sweet. it tastes crisp.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "this is the best\n",
      "\n",
      "loss=6.75685\n",
      "\n",
      "Iteration: 44\n",
      "Training input sequence length: 41\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "this peppermint stick is delicious and fun to eat. my dad got me one for christmas because he remembered me having a similar one when i was a little girl. i 'm 30 now and i love it!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "delicious!\n",
      "\n",
      "loss=4.01642\n",
      "\n",
      "Iteration: 45\n",
      "Training input sequence length: 29\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "great gift for all ages! i purchased these giant canes before and the recipients loved them so much, they kept them and would not eat them.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great\n",
      "\n",
      "loss=1.70116\n",
      "\n",
      "Iteration: 46\n",
      "Training input sequence length: 77\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "awesome dog food. however, when given to my `` boston '', who has severe reactions to some food ingredients; his itching increased to violent jumping out of bed at night, scratching. as soon as i changed to a different formula, the scratching stopped. so glad natural balance has other choices. i guess you have to try each, until you find what 's best for your pet.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great great great great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "increased my dogs itching\n",
      "\n",
      "loss=10.4727\n",
      "\n",
      "Iteration: 47\n",
      "Training input sequence length: 56\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "we have three dogs and all of them love this food! we bought it specifically for one of our dogs who has food allergies and it works great for him, no more hot spots or tummy unk< br/> i love that it ships right to our door with free shipping.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great great great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great food!\n",
      "\n",
      "loss=3.04694\n",
      "\n",
      "Iteration: 48\n",
      "Training input sequence length: 42\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "my unk mix has ibs. our vet recommended a limited ingredient food. this has really helped her symptoms and she likes it. i will always buy it from amazon ... it 's$ 10 cheaper and free shipping!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great great great great great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great for stomach problems!\n",
      "\n",
      "loss=6.43315\n",
      "\n",
      "Iteration: 49\n",
      "Training input sequence length: 58\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "great food! i love the idea of one food for all ages& breeds. t 's a real convenience as well as a really good product. my 3 dogs eat less, have almost no gas, their poop is regular and a perfect consistency, what else can a mom ask for!!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great food\n",
      "\n",
      "loss=3.02226\n",
      "\n",
      "Iteration: 50\n",
      "Training input sequence length: 24\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "this is great dog food, my dog has severs allergies and this brand is the only one that we can feed him.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great dog food\n",
      "\n",
      "loss=3.86777\n",
      "\n",
      "Iteration: 51\n",
      "Training input sequence length: 43\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "this food is great- all ages dogs. i have a 3 year old and a puppy. they are both so soft and hardly ever get sick. the food is good especially when you have amazon prime shipping:)\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great great great!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "mmmmm mmmmm good.\n",
      "\n",
      "loss=9.50443\n",
      "\n",
      "Iteration: 52\n",
      "Training input sequence length: 28\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "this is the same food we get at pet store. but it 's delivered to my door! and for the same price or slightly less.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "so convenient\n",
      "\n",
      "loss=13.1772\n",
      "\n",
      "Iteration: 53\n",
      "Training input sequence length: 67\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "i 've been very pleased with the natural balance dog food. our dogs have had issues with other dog foods in the past and i had someone recommend natural balance grain free since it is possible they were allergic to grains. since switching i have n't had any issues. it is also helpful that have have different kibble size for unk sized dogs.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great great great great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "good healthy dog food\n",
      "\n",
      "loss=4.78182\n",
      "\n",
      "Iteration: 54\n",
      "Training input sequence length: 43\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "i fed this to my golden retriever and he hated it. he would n't eat it, and when he did, it gave him terrible diarrhea. we will not be buying this again. it 's also super expensive.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "bad\n",
      "\n",
      "loss=14.4075\n",
      "\n",
      "Iteration: 55\n",
      "Training input sequence length: 24\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "arrived slightly thawed. my parents would n't accept it. however, the company was very helpful and issued a full refund.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great support\n",
      "\n",
      "loss=6.12445\n",
      "\n",
      "Iteration: 56\n",
      "Training input sequence length: 56\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "the crust on these tarts are perfect. my husband loves these, but i 'm not so crazy about them. they are just too unk for my taste. i 'll eat the crust and hubby takes my filling. my kids think they 're great, so maybe it 's just me.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "tart!\n",
      "\n",
      "loss=7.88888\n",
      "\n",
      "Iteration: 57\n",
      "Training input sequence length: 39\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "these are absolutely unk! my husband and i both love them, however, as another customer put it, they are expensive to ship! the cost of shipping is more than the tartlets themselves are!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great food!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "omaha apple tartlets\n",
      "\n",
      "loss=12.924\n",
      "\n",
      "Iteration: 58\n",
      "Training input sequence length: 37\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "what a nice alternative to an apple pie. love the fact there was no slicing and dicing. easy to prepare. i also loved the fact that you can make them fresh whenever needed.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "loved these tartlets\n",
      "\n",
      "loss=10.2792\n",
      "\n",
      "Iteration: 59\n",
      "Training input sequence length: 58\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "i like creme brulee. i loved that these were so easy. just sprinkle on the sugar that came with and broil. they look amazing and taste great. my guess thought i really went out of the way for them when really it took all of 5 minutes. i will be ordering more!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "the best\n",
      "\n",
      "loss=5.38269\n",
      "\n",
      "Iteration: 60\n",
      "Training input sequence length: 63\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "i love asparagus. up until very recently, i had never had pickled asparagus. oh my goodness, when a friend introduced me to this exact brand, i could n't believe how great stuff tasted. i loved it so much i bought the six pack. i 've got 2 jars left. gon na need more!!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "asparagus bliss\n",
      "\n",
      "loss=11.9187\n",
      "\n",
      "Iteration: 61\n",
      "Training input sequence length: 33\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "i was unk in the flavor and texture of this mix. i usually like most of the low carb things i have tried, but was unk in this specific one.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great food food food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "low carb angel food puffs\n",
      "\n",
      "loss=9.81204\n",
      "\n",
      "Iteration: 62\n",
      "Training input sequence length: 60\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "i have been drinking this tea for a long time now. i used to have to purchase it at a doctor 's office because it was n't available elsewhere. i 'm so glad that i can buy it now from amazon.com. i drink this tea throughout the day like other folks drink coffee. wonderful taste.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "delicious tea\n",
      "\n",
      "loss=4.55072\n",
      "\n",
      "Iteration: 63\n",
      "Training input sequence length: 65\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "i love, love this green tea. it is very hard to find in our area and some places on the internet charge a big price and i usually do n't get as many boxes as i did with this merchant. i will definitely order from this seller again!! thanks!! i depend on my green tea fix everyday!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "tea review\n",
      "\n",
      "loss=8.26901\n",
      "\n",
      "Iteration: 64\n",
      "Training input sequence length: 26\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "i love this tea. it helps curb my eating during the day. my mom and i have given it all friends to try.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food.\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "wonderful tea\n",
      "\n",
      "loss=5.67268\n",
      "\n",
      "Iteration: 65\n",
      "Training input sequence length: 47\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "i 'm italian and i lived in italy for years. i used to buy these cookies for my everyday breakfast with an italian espresso. i could n't find them anywhere here in the bay area, so it 's great to have them again.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great cookies\n",
      "\n",
      "loss=7.8061\n",
      "\n",
      "Iteration: 66\n",
      "Training input sequence length: 79\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i have done a lot of research to find the best food for my cat, and this is an excellent food. that is also according to my holistic veterinarian. they put probiotics on the kibble as the last step, which is very important to me. the best thing is that my cat loved it immediately and i had to stop mixing it with the old food because she only would eat holistic select.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "delicious tea tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great food.\n",
      "\n",
      "loss=3.78115\n",
      "\n",
      "Iteration: 67\n",
      "Training input sequence length: 65\n",
      "Training target outputs sequence length: 7\n",
      "\n",
      "TEXT:\n",
      "one of my cats is allergic to fish and beef. this formula is one of the few she can eat, and it has much better ingredients than the prescription diets available at the vet. both of my kitties are very active, have soft shiny fur, and neither are fat. dry food reduces tartar buildup on teeth, also.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food tea tea tea tea tea tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "wonderful food- perfect for allergic kitties\n",
      "\n",
      "loss=8.03118\n",
      "\n",
      "Iteration: 68\n",
      "Training input sequence length: 51\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "our cats thrive extremely well on this dry cat food. they definitely have much less hair ball throw ups and their fur is great. they are fit and not over weight. this vendor ships extremely fast. is one of the top amazon suppliers in our book!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food tea tea tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "holistic select cat food\n",
      "\n",
      "loss=9.35138\n",
      "\n",
      "Iteration: 69\n",
      "Training input sequence length: 45\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i 've been eating ramen noodles since i was a little kid, and i 've never found a better flavor than hot& spicy chicken! it is n't hot at all to a unk like me, but it sure is good!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food tea tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "my favorite ramen\n",
      "\n",
      "loss=7.70701\n",
      "\n",
      "Iteration: 70\n",
      "Training input sequence length: 54\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i love spicy ramen, but for whatever reasons this thing burns my stomach badly and the burning sensation does n't go away for like 3 hours! not sure if that is healthy or not .... and you can buy this at walmart for$ 0.28, way cheaper than amazon.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food tea tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "it burns!\n",
      "\n",
      "loss=7.53228\n",
      "\n",
      "Iteration: 71\n",
      "Training input sequence length: 42\n",
      "Training target outputs sequence length: 6\n",
      "\n",
      "TEXT:\n",
      "always being a fan of ramen as a quick and easy meal, finding it on amazon for a decent price and having it delivered to your door by the case is an amazing situation for anyone to find themselves in.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food tea tea tea tea tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "amazing to the last bite.\n",
      "\n",
      "loss=9.26448\n",
      "\n",
      "Iteration: 72\n",
      "Training input sequence length: 56\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i must be a bit of a wuss, because this soup tastes to me how i imagine fire might taste. typically i like spicy food if it has a good flavor. i do n't find this to be the case with this soup. any flavor is killed off by the burn.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food tea tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "not for me\n",
      "\n",
      "loss=7.93951\n",
      "\n",
      "Iteration: 73\n",
      "Training input sequence length: 50\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i really loved the spicy flavor these had. i found myself liking the broth more than the noodles which is usually the opposite. if you are n't used to the heat this might bother you and if you like hot hot foods this might not be enough.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food tea tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great spicy flavor\n",
      "\n",
      "loss=6.68682\n",
      "\n",
      "Iteration: 74\n",
      "Training input sequence length: 78\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "got these on sale for roughly 25 cents per cup, which is half the price of my local grocery stores, plus they rarely stock the spicy flavors. these things are a great snack for my office where time is constantly crunched and sometimes you ca n't escape for a real meal. this is one of my favorite flavors of instant lunch and will be back to buy every time it goes on sale.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food food tea tea tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great value and convenient ramen\n",
      "\n",
      "loss=7.53814\n",
      "\n",
      "Iteration: 75\n",
      "Training input sequence length: 22\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "i have bought allot of different flavors and this happens to be one of my favorites and will be getting more soon\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great flavor\n",
      "\n",
      "loss=3.15631\n",
      "\n",
      "Iteration: 76\n",
      "Training input sequence length: 74\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "the best investment i 've ever made for ginger. it 's unbelievable! it 's fibrous like the real ginger, has that spicy kick to it, but it 's perfect with the sugar- calms it down. it 's very worth the$ 40 for unk of it! i 'll be getting more soon- i use these as a topper for my ginger cupcakes and cookies:)\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "food tea tea tea tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "tastes awesome& looks beautiful\n",
      "\n",
      "loss=11.1367\n",
      "\n",
      "Iteration: 77\n",
      "Training input sequence length: 33\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "delicious. i can not get australian ginger where i live. this compares favorably to australian ginger i 've purchased in other cities. now i can enjoy it without traveling.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "happy face\n",
      "\n",
      "loss=13.2036\n",
      "\n",
      "Iteration: 78\n",
      "Training input sequence length: 30\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "i keep trying other brands .... cheaper brands. stupid me! this ginger is soooo worth the money. tender, moist and never a let down.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great tea tea tea\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "simply the best!\n",
      "\n",
      "loss=6.33492\n",
      "\n",
      "Iteration: 79\n",
      "Training input sequence length: 52\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "i bought this for our office to give people something sweet to snack on. because it 's bite size it 's easier for people to grab a couple a pieces rather than an entire licorice stick. my only complaint is that one of the bags broke open in shipping.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great flavor\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "nice snack\n",
      "\n",
      "loss=13.0985\n",
      "\n",
      "Iteration: 80\n",
      "Training input sequence length: 59\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "twizzlers brand licorice is much better than that other well known unk< br/> if you can get these for$ 2 to$ 2.50 a package with free unk it 's a good unk< br/> the black and cherry have good taste; but the strawberry taste was too delicate and barely there\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great flavor\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "good licorice\n",
      "\n",
      "loss=8.12259\n",
      "\n",
      "Iteration: 81\n",
      "Training input sequence length: 36\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "this is one of the best salsas that i have found in a long time but stay away from the variety pack. the other two that come with it are not worth your money.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great flavor flavor flavor flavor\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "love the salsa!!\n",
      "\n",
      "loss=6.54998\n",
      "\n",
      "Iteration: 82\n",
      "Training input sequence length: 44\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "these remind me of dog treats i made once using pumpkin and cinnamon. they 're kind of bland and not my favorite back to nature product. but my unk really loves them so that 's where the three stars come from.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great flavor\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "unk ...\n",
      "\n",
      "loss=5.97179\n",
      "\n",
      "Iteration: 83\n",
      "Training input sequence length: 39\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "this is the best cornmeal. i made regular cornbread and hot water cornbread with this meal and both were outstanding. also fried some oysters with this meal, it gave them a great texture and unk.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great flavor\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "awesome cornmeal\n",
      "\n",
      "loss=8.57024\n",
      "\n",
      "Iteration: 84\n",
      "Training input sequence length: 64\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a fabulous marinade! i love to use it for chicken, either baked in the oven or on the grill. this has enough flavor& flair, i 've even used it for dinner parties, only to receive rave reviews from my guests!! definitely worth the price! super cheap and super easy! love it!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great flavor flavor\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great marinade!\n",
      "\n",
      "loss=5.96674\n",
      "\n",
      "Iteration: 85\n",
      "Training input sequence length: 29\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "works with chicken fish beef or pork. fast easy and makes it taste excellent. plus buying in bulk is more than 50% off from box stores\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great flavor\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "awesome stuff\n",
      "\n",
      "loss=5.27093\n",
      "\n",
      "Iteration: 86\n",
      "Training input sequence length: 25\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "got this for my brother who is on jorge cruise diet and decided to try one for myself. it actually tastes pretty good.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great flavor\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "tastes good\n",
      "\n",
      "loss=8.59205\n",
      "\n",
      "Iteration: 87\n",
      "Training input sequence length: 54\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "these singles sell for$ 2.50-$ 3.36 at the store for 1 box of 24 singles. i 'm not sure why amazon is selling it for$ 9.99 for a box of 24 singles. hazelnut coffee creamer is my favorite, but truly this is not a good buy.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great flavor flavor\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "rip off price\n",
      "\n",
      "loss=10.4594\n",
      "\n",
      "Iteration: 88\n",
      "Training input sequence length: 66\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "awesome!!! such a yummy flavor i got it as a healthy alternative to the desserts we normally eat and i am so glad that i did there are so many things you can do with jello desserts and still have them taste good and be good for you. i will"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d577097b223c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mvec2word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec2word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-66857750bc0f>\u001b[0m in \u001b[0;36mvec2word\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvec2word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# converts a given vector representation into the represented word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvec2word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_nearest_neighbour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36marray_equal\u001b[0;34m(a1, a2)\u001b[0m\n\u001b[1;32m   2602\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import string\n",
    "from __future__ import print_function\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 0   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_acc=0\n",
    "    display_step = 1\n",
    "    \n",
    "    while step < training_iters:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "           \n",
    "        for i in xrange(0,train_len):\n",
    "            \n",
    "            train_out = transform_out(train_summaries[i][0:len(train_summaries[i])-1])\n",
    "            \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nIteration: \"+str(i))\n",
    "                print(\"Training input sequence length: \"+str(len(train_texts[i])))\n",
    "                print(\"Training target outputs sequence length: \"+str(len(train_out)))\n",
    "            \n",
    "                print(\"\\nTEXT:\")\n",
    "                flag = 0\n",
    "                for vec in train_texts[i]:\n",
    "                    if vec2word(vec) in string.punctuation or flag==0:\n",
    "                        print(str(vec2word(vec)),end='')\n",
    "                    else:\n",
    "                        print((\" \"+str(vec2word(vec))),end='')\n",
    "                    flag=1\n",
    "\n",
    "                print(\"\\n\")\n",
    "\n",
    "\n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,pred = sess.run([optimizer,cost,prediction],feed_dict={tf_text: train_texts[i], \n",
    "                                                    tf_seq_len: len(train_texts[i]), \n",
    "                                                    tf_summary: train_out,\n",
    "                                                    tf_output_len: len(train_out)})\n",
    "            \n",
    "         \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nPREDICTED SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for index in pred:\n",
    "                    #if int(index)!=vocab_limit.index('eos'):\n",
    "                    if vocab_limit[int(index)] in string.punctuation or flag==0:\n",
    "                        print(str(vocab_limit[int(index)]),end='')\n",
    "                    else:\n",
    "                        print(\" \"+str(vocab_limit[int(index)]),end='')\n",
    "                    flag=1\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                print(\"ACTUAL SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for vec in train_summaries[i]:\n",
    "                    if vec2word(vec)!='eos':\n",
    "                        if vec2word(vec) in string.punctuation or flag==0:\n",
    "                            print(str(vec2word(vec)),end='')\n",
    "                        else:\n",
    "                            print((\" \"+str(vec2word(vec))),end='')\n",
    "                    flag=1\n",
    "\n",
    "                print(\"\\n\")\n",
    "                print(\"loss=\"+str(loss))\n",
    "            \n",
    "        step=step+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
